{"chunk_id": "DOC02:20251014:2-7:0000:0a5d5333", "doc_id": "DOC02", "version": "20251014", "chunk_index": 0, "block_start_index": 2, "block_end_index": 7, "text": "A Data Scientist turns raw data into reliable decisions, products, and measurable business impact. The mission is to reduce uncertainty with evidence, build models that generalize, and create feedback loops so the organization learns faster than competitors.\n\n- Turn data into decisions: hypotheses → experiments/models → actions → measured impact\n\n- Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting\n\n- Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools\n\n- Build durable assets: high-quality datasets, features, models, and documentation\n\n- Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Mission &amp; value proposition\n\nA Data Scientist turns raw data into reliable decisions, products, and measurable business impact. The mission is to reduce uncertainty with evidence, build models that generalize, and create feedback loops so the organization learns faster than competitors.\n\n- Turn data into decisions: hypotheses → experiments/models → actions → measured impact\n\n- Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting\n\n- Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools\n\n- Build durable assets: high-quality datasets, features, models, and documentation\n\n- Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases", "token_count": 126, "embedding_token_count": 144, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Mission &amp; value proposition"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Mission &amp; value proposition", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Mission &amp; value proposition", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.928934Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}}
{"chunk_id": "DOC02:20251014:9-9:0001:8a852c86", "doc_id": "DOC02", "version": "20251014", "chunk_index": 1, "block_start_index": 9, "block_end_index": 9, "text": "impact. The mission is to reduce uncertainty with evidence, build models that generalize, and create feedback loops so the organization learns faster than competitors. - Turn data into decisions: hypotheses → experiments/models → actions → measured impact - Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases\n\nData Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Boundaries vs. Data Analyst / ML Engineer / Data Engineer\n\nimpact. The mission is to reduce uncertainty with evidence, build models that generalize, and create feedback loops so the organization learns faster than competitors. - Turn data into decisions: hypotheses → experiments/models → actions → measured impact - Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases\n\nData Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor.", "token_count": 132, "embedding_token_count": 155, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Boundaries vs. Data Analyst / ML Engineer / Data Engineer"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Boundaries vs. Data Analyst / ML Engineer / Data Engineer", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Boundaries vs. Data Analyst / ML Engineer / Data Engineer", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.929116Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:11-13:0002:c2d61605", "doc_id": "DOC02", "version": "20251014", "chunk_index": 2, "block_start_index": 11, "block_end_index": 13, "text": "loops so the organization learns faster than competitors. - Turn data into decisions: hypotheses → experiments/models → actions → measured impact - Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases Data Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor.\n\n- o Focus: descriptive/diagnostic analytics, dashboards, KPI health, ad-hoc queries\n\n- o Primary artifacts: reports, visualizations, metric definitions\n\n- o DS difference: heavier inference/prediction, experimentation, and model building", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Data Analyst\n\nloops so the organization learns faster than competitors. - Turn data into decisions: hypotheses → experiments/models → actions → measured impact - Increase revenue/retention and reduce cost/risk via better targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases Data Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor.\n\n- o Focus: descriptive/diagnostic analytics, dashboards, KPI health, ad-hoc queries\n\n- o Primary artifacts: reports, visualizations, metric definitions\n\n- o DS difference: heavier inference/prediction, experimentation, and model building", "token_count": 157, "embedding_token_count": 172, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Data Analyst"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Data Analyst", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Data Analyst", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.929307Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:15-21:0003:f25fd258", "doc_id": "DOC02", "version": "20251014", "chunk_index": 3, "block_start_index": 15, "block_end_index": 21, "text": "better targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases Data Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor. - o Focus: descriptive/diagnostic analytics, dashboards, KPI health, ad-hoc queries - o Primary artifacts: reports, visualizations, metric definitions - o DS difference: heavier inference/prediction, experimentation, and model building\n\n- o Focus: productionizing models, serving/latency, pipelines, infra, scalability\n\n- o Primary artifacts: services, feature stores, CI/CD, monitoring systems\n\n- o DS difference: chooses model/class of solutions, validates assumptions, owns evaluation\n\n- Data Engineer\n\n- o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance\n\n- o Primary artifacts: schemas, transformations, data contracts, reliability SLAs\n\n- o DS difference: consumes/defines data requirements, creates features, validates suitability", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · ML Engineer\n\nbetter targeting, automation, and forecasting - Shorten decision cycles with reproducible analyses, dashboards, and self-serve tools - Build durable assets: high-quality datasets, features, models, and documentation - Institutionalize learning: A/B tests, post-mortems, model monitoring, knowledge bases Data Science overlaps adjacent roles but has distinct ownership over problem framing, modeling, and causal/decision rigor. - o Focus: descriptive/diagnostic analytics, dashboards, KPI health, ad-hoc queries - o Primary artifacts: reports, visualizations, metric definitions - o DS difference: heavier inference/prediction, experimentation, and model building\n\n- o Focus: productionizing models, serving/latency, pipelines, infra, scalability\n\n- o Primary artifacts: services, feature stores, CI/CD, monitoring systems\n\n- o DS difference: chooses model/class of solutions, validates assumptions, owns evaluation\n\n- Data Engineer\n\n- o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance\n\n- o Primary artifacts: schemas, transformations, data contracts, reliability SLAs\n\n- o DS difference: consumes/defines data requirements, creates features, validates suitability", "token_count": 221, "embedding_token_count": 236, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· ML Engineer"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · ML Engineer", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· ML Engineer", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.929516Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:23-25:0004:cef0dad6", "doc_id": "DOC02", "version": "20251014", "chunk_index": 4, "block_start_index": 23, "block_end_index": 25, "text": "artifacts: reports, visualizations, metric definitions - o DS difference: heavier inference/prediction, experimentation, and model building - o Focus: productionizing models, serving/latency, pipelines, infra, scalability - o Primary artifacts: services, feature stores, CI/CD, monitoring systems - o DS difference: chooses model/class of solutions, validates assumptions, owns evaluation - Data Engineer - o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance - o Primary artifacts: schemas, transformations, data contracts, reliability SLAs - o DS difference: consumes/defines data requirements, creates features, validates suitability\n\n- o DS frames the question, designs evaluation, builds/validates models; MLE ships them reliably\n\n- o DS specifies data needs and quality gates; DE ensures trustworthy, timely data access\n\n- o DS partners with DA to scale insights and codify metrics into decision tools", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Practical boundary rules\n\nartifacts: reports, visualizations, metric definitions - o DS difference: heavier inference/prediction, experimentation, and model building - o Focus: productionizing models, serving/latency, pipelines, infra, scalability - o Primary artifacts: services, feature stores, CI/CD, monitoring systems - o DS difference: chooses model/class of solutions, validates assumptions, owns evaluation - Data Engineer - o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance - o Primary artifacts: schemas, transformations, data contracts, reliability SLAs - o DS difference: consumes/defines data requirements, creates features, validates suitability\n\n- o DS frames the question, designs evaluation, builds/validates models; MLE ships them reliably\n\n- o DS specifies data needs and quality gates; DE ensures trustworthy, timely data access\n\n- o DS partners with DA to scale insights and codify metrics into decision tools", "token_count": 174, "embedding_token_count": 190, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Practical boundary rules"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Practical boundary rules", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Practical boundary rules", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.929685Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:27-43:0005:40df5148", "doc_id": "DOC02", "version": "20251014", "chunk_index": 5, "block_start_index": 27, "block_end_index": 43, "text": "owns evaluation - Data Engineer - o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance - o Primary artifacts: schemas, transformations, data contracts, reliability SLAs - o DS difference: consumes/defines data requirements, creates features, validates suitability - o DS frames the question, designs evaluation, builds/validates models; MLE ships them reliably - o DS specifies data needs and quality gates; DE ensures trustworthy, timely data access - o DS partners with DA to scale insights and codify metrics into decision tools\n\nData Scientists select the right problem framing before selecting methods-often blending types.\n\n- Prediction (What will happen?)\n\n- o Examples: churn, demand, CTR, LTV, fraud probability\n\n- o Methods: supervised learning (trees/boosting, GLMs, DL)\n\n- o Deliverables: calibrated scores, rankers, thresholds, ROC/PR curves\n\n- Inference/Explanation (Why/what drives it?)\n\n- o Examples: drivers of conversion, treatment effects, elasticity\n\n- o Methods: causal inference, regression with interpretability, SHAP, experiments\n\n- o Deliverables: effect sizes, confidence intervals, decision guidance\n\n- Optimization (What should we do?)\n\n- o Examples: budget allocation, pricing, routing, recommendation policies\n\n- o Methods: bandits, RL, linear/convex programming, stochastic optimization\n\n- o Deliverables: policies, constraints, scenario analyses, payoff curves\n\n- Insight/Discovery (What patterns exist?)\n\n- o Examples: segmentation, anomaly detection, topic discovery\n\n- o Methods: clustering, embeddings, dimensionality reduction, unsupervised DL\n\n- o Deliverables: segments/personas, hypotheses, opportunity maps", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Problem types (prediction, inference, optimization, insight)\n\nowns evaluation - Data Engineer - o Focus: data ingestion, ETL/ELT, warehousing, quality, lineage, governance - o Primary artifacts: schemas, transformations, data contracts, reliability SLAs - o DS difference: consumes/defines data requirements, creates features, validates suitability - o DS frames the question, designs evaluation, builds/validates models; MLE ships them reliably - o DS specifies data needs and quality gates; DE ensures trustworthy, timely data access - o DS partners with DA to scale insights and codify metrics into decision tools\n\nData Scientists select the right problem framing before selecting methods-often blending types.\n\n- Prediction (What will happen?)\n\n- o Examples: churn, demand, CTR, LTV, fraud probability\n\n- o Methods: supervised learning (trees/boosting, GLMs, DL)\n\n- o Deliverables: calibrated scores, rankers, thresholds, ROC/PR curves\n\n- Inference/Explanation (Why/what drives it?)\n\n- o Examples: drivers of conversion, treatment effects, elasticity\n\n- o Methods: causal inference, regression with interpretability, SHAP, experiments\n\n- o Deliverables: effect sizes, confidence intervals, decision guidance\n\n- Optimization (What should we do?)\n\n- o Examples: budget allocation, pricing, routing, recommendation policies\n\n- o Methods: bandits, RL, linear/convex programming, stochastic optimization\n\n- o Deliverables: policies, constraints, scenario analyses, payoff curves\n\n- Insight/Discovery (What patterns exist?)\n\n- o Examples: segmentation, anomaly detection, topic discovery\n\n- o Methods: clustering, embeddings, dimensionality reduction, unsupervised DL\n\n- o Deliverables: segments/personas, hypotheses, opportunity maps", "token_count": 317, "embedding_token_count": 340, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Problem types (prediction, inference, optimization, insight)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Problem types (prediction, inference, optimization, insight)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Problem types (prediction, inference, optimization, insight)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.929949Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:45-57:0006:49b04462", "doc_id": "DOC02", "version": "20251014", "chunk_index": 6, "block_start_index": 45, "block_end_index": 57, "text": "o Methods: causal inference, regression with interpretability, SHAP, experiments - o Deliverables: effect sizes, confidence intervals, decision guidance - Optimization (What should we do?) - o Examples: budget allocation, pricing, routing, recommendation policies - o Methods: bandits, RL, linear/convex programming, stochastic optimization - o Deliverables: policies, constraints, scenario analyses, payoff curves - Insight/Discovery (What patterns exist?) - o Examples: segmentation, anomaly detection, topic discovery - o Methods: clustering, embeddings, dimensionality reduction, unsupervised DL - o Deliverables: segments/personas, hypotheses, opportunity maps\n\nThe same skills express differently across business contexts; expectations and KPIs change accordingly.\n\n- Product\n\n- o Work: ranking/reco, search quality, personalization, safety\n\n- o KPIs: engagement, retention, revenue per user, latency, fairness\n\n- Growth/Marketing\n\n- o Work: targeting/propensity, LTV forecasting, media mix/model lift\n\n- o KPIs: CAC/LTV ratio, incremental lift, ROI, attribution confidence\n\n- Operations\n\n- o Work: demand/supply forecasting, workforce planning, quality/defects, logistics\n\n- o KPIs: SLA adherence, cost/unit, utilization, forecast accuracy (MAPE/RMSE)\n\n- Research/Advanced Methods\n\n- o Work: novel modeling, causal frameworks, GenAI/LLM evaluation, experimentation science\n\n- o KPIs: published/internal tech notes, method adoption, upstream improvements", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Contexts (product, growth/marketing, operations, research)\n\no Methods: causal inference, regression with interpretability, SHAP, experiments - o Deliverables: effect sizes, confidence intervals, decision guidance - Optimization (What should we do?) - o Examples: budget allocation, pricing, routing, recommendation policies - o Methods: bandits, RL, linear/convex programming, stochastic optimization - o Deliverables: policies, constraints, scenario analyses, payoff curves - Insight/Discovery (What patterns exist?) - o Examples: segmentation, anomaly detection, topic discovery - o Methods: clustering, embeddings, dimensionality reduction, unsupervised DL - o Deliverables: segments/personas, hypotheses, opportunity maps\n\nThe same skills express differently across business contexts; expectations and KPIs change accordingly.\n\n- Product\n\n- o Work: ranking/reco, search quality, personalization, safety\n\n- o KPIs: engagement, retention, revenue per user, latency, fairness\n\n- Growth/Marketing\n\n- o Work: targeting/propensity, LTV forecasting, media mix/model lift\n\n- o KPIs: CAC/LTV ratio, incremental lift, ROI, attribution confidence\n\n- Operations\n\n- o Work: demand/supply forecasting, workforce planning, quality/defects, logistics\n\n- o KPIs: SLA adherence, cost/unit, utilization, forecast accuracy (MAPE/RMSE)\n\n- Research/Advanced Methods\n\n- o Work: novel modeling, causal frameworks, GenAI/LLM evaluation, experimentation science\n\n- o KPIs: published/internal tech notes, method adoption, upstream improvements", "token_count": 279, "embedding_token_count": 303, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Contexts (product, growth/marketing, operations, research)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Contexts (product, growth/marketing, operations, research)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Contexts (product, growth/marketing, operations, research)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.930184Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:59-69:0007:0ad2eab5", "doc_id": "DOC02", "version": "20251014", "chunk_index": 7, "block_start_index": 59, "block_end_index": 69, "text": "search quality, personalization, safety - o KPIs: engagement, retention, revenue per user, latency, fairness - Growth/Marketing - o Work: targeting/propensity, LTV forecasting, media mix/model lift - o KPIs: CAC/LTV ratio, incremental lift, ROI, attribution confidence - Operations - o Work: demand/supply forecasting, workforce planning, quality/defects, logistics - o KPIs: SLA adherence, cost/unit, utilization, forecast accuracy (MAPE/RMSE) - Research/Advanced Methods - o Work: novel modeling, causal frameworks, GenAI/LLM evaluation, experimentation science - o KPIs: published/internal tech notes, method adoption, upstream improvements\n\nSuccess is defined by shipped solutions that are correct, reliable, and move the target metric-safely and repeatably.\n\n- Business impact : clear north-star (e.g., retention +x%), guardrails (e.g., fairness, latency)\n\n- Model quality : offline metrics (AUC/F1/RMSE), calibration, stability across cohorts\n\n- Causality &amp; experimentation : valid uplift, power/MDE met, no p-hacking\n\n- Operationalization : SLAs (latency/uptime), monitoring (drift, data quality), alerting\n\n- Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost\n\n- Governance : privacy/compliance, reproducibility, versioning, rollback plans\n\n- Ownership boundaries\n\n- o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations\n\n- o With MLE: serving pattern choice, deployment pathway, monitoring specs\n\n- o With PM/Leads: success metrics, launch criteria, iteration cadence", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Success criteria &amp; ownership (from metric movement to shipped models)\n\nsearch quality, personalization, safety - o KPIs: engagement, retention, revenue per user, latency, fairness - Growth/Marketing - o Work: targeting/propensity, LTV forecasting, media mix/model lift - o KPIs: CAC/LTV ratio, incremental lift, ROI, attribution confidence - Operations - o Work: demand/supply forecasting, workforce planning, quality/defects, logistics - o KPIs: SLA adherence, cost/unit, utilization, forecast accuracy (MAPE/RMSE) - Research/Advanced Methods - o Work: novel modeling, causal frameworks, GenAI/LLM evaluation, experimentation science - o KPIs: published/internal tech notes, method adoption, upstream improvements\n\nSuccess is defined by shipped solutions that are correct, reliable, and move the target metric-safely and repeatably.\n\n- Business impact : clear north-star (e.g., retention +x%), guardrails (e.g., fairness, latency)\n\n- Model quality : offline metrics (AUC/F1/RMSE), calibration, stability across cohorts\n\n- Causality &amp; experimentation : valid uplift, power/MDE met, no p-hacking\n\n- Operationalization : SLAs (latency/uptime), monitoring (drift, data quality), alerting\n\n- Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost\n\n- Governance : privacy/compliance, reproducibility, versioning, rollback plans\n\n- Ownership boundaries\n\n- o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations\n\n- o With MLE: serving pattern choice, deployment pathway, monitoring specs\n\n- o With PM/Leads: success metrics, launch criteria, iteration cadence", "token_count": 327, "embedding_token_count": 353, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Success criteria &amp; ownership (from metric movement to shipped models)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Success criteria &amp; ownership (from metric movement to shipped models)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Success criteria &amp; ownership (from metric movement to shipped models)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.930464Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:71-71:0008:cbe749f7", "doc_id": "DOC02", "version": "20251014", "chunk_index": 8, "block_start_index": 71, "block_end_index": 71, "text": "experimentation : valid uplift, power/MDE met, no p-hacking - Operationalization : SLAs (latency/uptime), monitoring (drift, data quality), alerting - Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost - Governance : privacy/compliance, reproducibility, versioning, rollback plans - Ownership boundaries - o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations - o With MLE: serving pattern choice, deployment pathway, monitoring specs - o With PM/Leads: success metrics, launch criteria, iteration cadence\n\nEffective DS work is cross-functional; tailor artifacts and communication to each audience.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Key stakeholders (PM, Eng, Ops, Leadership)\n\nexperimentation : valid uplift, power/MDE met, no p-hacking - Operationalization : SLAs (latency/uptime), monitoring (drift, data quality), alerting - Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost - Governance : privacy/compliance, reproducibility, versioning, rollback plans - Ownership boundaries - o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations - o With MLE: serving pattern choice, deployment pathway, monitoring specs - o With PM/Leads: success metrics, launch criteria, iteration cadence\n\nEffective DS work is cross-functional; tailor artifacts and communication to each audience.", "token_count": 133, "embedding_token_count": 156, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Key stakeholders (PM, Eng, Ops, Leadership)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Key stakeholders (PM, Eng, Ops, Leadership)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Key stakeholders (PM, Eng, Ops, Leadership)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.930602Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:73-89:0009:5366f8a4", "doc_id": "DOC02", "version": "20251014", "chunk_index": 9, "block_start_index": 73, "block_end_index": 89, "text": "(latency/uptime), monitoring (drift, data quality), alerting - Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost - Governance : privacy/compliance, reproducibility, versioning, rollback plans - Ownership boundaries - o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations - o With MLE: serving pattern choice, deployment pathway, monitoring specs - o With PM/Leads: success metrics, launch criteria, iteration cadence Effective DS work is cross-functional; tailor artifacts and communication to each audience.\n\n- o Cares about: business outcome, prioritization, trade-offs, timelines\n\n- o How to work: align on problem/metrics, design experiments, define launch gates\n\n- Engineers (ML/Backend/Data)\n\n- o Cares about: feasibility, interfaces, reliability, cost, performance\n\n- o How to work: write clear specs (schemas, contracts), tests, and monitoring requirements\n\n- Operations / GTM\n\n- o Cares about: workflows, capacity, playbooks, exception handling\n\n- o How to work: deliver actionable outputs (queues, thresholds), SOPs, training\n\n- Leadership (Functional/Exec)\n\n- o Cares about: ROI, risk, strategy, resourcing, compliance\n\n- o How to work: concise narratives with alternatives, scenario impact, evidence strength\n\n- Design/UX &amp; Analytics\n\n- o Cares about: interpretability, user experience, metric clarity\n\n- o How to work: co-design explainable surfaces, consistent metric definitions\n\n- Legal/Privacy/Security\n\n- o Cares about: data use, consent, auditability, bias/safety\n\n- o How to work: early reviews, data minimization plans, model cards, audit trails", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Product Manager (PM)\n\n(latency/uptime), monitoring (drift, data quality), alerting - Adoption : stakeholders use the output in decisions; documentation lowers onboarding cost - Governance : privacy/compliance, reproducibility, versioning, rollback plans - Ownership boundaries - o DS owns: problem framing, data &amp; feature requirements, modeling, evaluation plan, experiment design, decision recommendations - o With MLE: serving pattern choice, deployment pathway, monitoring specs - o With PM/Leads: success metrics, launch criteria, iteration cadence Effective DS work is cross-functional; tailor artifacts and communication to each audience.\n\n- o Cares about: business outcome, prioritization, trade-offs, timelines\n\n- o How to work: align on problem/metrics, design experiments, define launch gates\n\n- Engineers (ML/Backend/Data)\n\n- o Cares about: feasibility, interfaces, reliability, cost, performance\n\n- o How to work: write clear specs (schemas, contracts), tests, and monitoring requirements\n\n- Operations / GTM\n\n- o Cares about: workflows, capacity, playbooks, exception handling\n\n- o How to work: deliver actionable outputs (queues, thresholds), SOPs, training\n\n- Leadership (Functional/Exec)\n\n- o Cares about: ROI, risk, strategy, resourcing, compliance\n\n- o How to work: concise narratives with alternatives, scenario impact, evidence strength\n\n- Design/UX &amp; Analytics\n\n- o Cares about: interpretability, user experience, metric clarity\n\n- o How to work: co-design explainable surfaces, consistent metric definitions\n\n- Legal/Privacy/Security\n\n- o Cares about: data use, consent, auditability, bias/safety\n\n- o How to work: early reviews, data minimization plans, model cards, audit trails", "token_count": 337, "embedding_token_count": 355, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Product Manager (PM)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Product Manager (PM)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Product Manager (PM)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.930887Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:92-109:0010:217b6431", "doc_id": "DOC02", "version": "20251014", "chunk_index": 10, "block_start_index": 92, "block_end_index": 109, "text": "actionable outputs (queues, thresholds), SOPs, training - Leadership (Functional/Exec) - o Cares about: ROI, risk, strategy, resourcing, compliance - o How to work: concise narratives with alternatives, scenario impact, evidence strength - Design/UX &amp; Analytics - o Cares about: interpretability, user experience, metric clarity - o How to work: co-design explainable surfaces, consistent metric definitions - Legal/Privacy/Security - o Cares about: data use, consent, auditability, bias/safety - o How to work: early reviews, data minimization plans, model cards, audit trails\n\nNatural Language Processing turns unstructured text into structured signals and decisions. Modern NLP blends classic supervised tasks (classification/NER) with retrieval-augmented generation (RAG) and LLM-powered apps. What you do (in depth): Build datasets from logs/docs/chats, define label taxonomies, choose model families (linear → transformers), evaluate with task-appropriate metrics, and ship APIs or in-product features with monitoring for drift, toxicity, and hallucinations.\n\n- Core tasks\n\n- o Text classification (intent, sentiment, topic) → Accuracy/F1, calibration\n\n- o NER (entities, PII) → Precision/Recall/F1 per class, span F1\n\n- o QA &amp; RAG → Exact Match/F1; retrieval: Recall@k, nDCG; generation: factuality checks\n\n- o Summarization/rewrites → ROUGE/BLEU + human eval (clarity, faithfulness)\n\n- Data &amp; features\n\n- o Label schema design, inter-annotator agreement, active learning loops\n\n- o Tokenization pitfalls (multi-lingual, emoji, code), domain lexicons\n\n- Modeling approaches\n\n- o Baselines (logreg + TF-IDF), small transformers (DistilBERT), domain-tuned LMs\n\n- o RAG: chunking, embeddings, vector stores, reranking; prompt templates\n\n- Production concerns\n\n- o Guardrails: profanity/PII filters, allowed topics, citation requirements\n\n- o Latency vs cost (prompt size, temperature, context window)\n\n- o Hallucination mitigation: retrieval grounding, constrained decoding, answerability detectors\n\n- Anti-patterns\n\n- o Training on test leakage (FAQ overlap), vague labels, no human eval for generative tasks", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > NLP (classification, NER, RAG, LLM apps)\n\nactionable outputs (queues, thresholds), SOPs, training - Leadership (Functional/Exec) - o Cares about: ROI, risk, strategy, resourcing, compliance - o How to work: concise narratives with alternatives, scenario impact, evidence strength - Design/UX &amp; Analytics - o Cares about: interpretability, user experience, metric clarity - o How to work: co-design explainable surfaces, consistent metric definitions - Legal/Privacy/Security - o Cares about: data use, consent, auditability, bias/safety - o How to work: early reviews, data minimization plans, model cards, audit trails\n\nNatural Language Processing turns unstructured text into structured signals and decisions. Modern NLP blends classic supervised tasks (classification/NER) with retrieval-augmented generation (RAG) and LLM-powered apps. What you do (in depth): Build datasets from logs/docs/chats, define label taxonomies, choose model families (linear → transformers), evaluate with task-appropriate metrics, and ship APIs or in-product features with monitoring for drift, toxicity, and hallucinations.\n\n- Core tasks\n\n- o Text classification (intent, sentiment, topic) → Accuracy/F1, calibration\n\n- o NER (entities, PII) → Precision/Recall/F1 per class, span F1\n\n- o QA &amp; RAG → Exact Match/F1; retrieval: Recall@k, nDCG; generation: factuality checks\n\n- o Summarization/rewrites → ROUGE/BLEU + human eval (clarity, faithfulness)\n\n- Data &amp; features\n\n- o Label schema design, inter-annotator agreement, active learning loops\n\n- o Tokenization pitfalls (multi-lingual, emoji, code), domain lexicons\n\n- Modeling approaches\n\n- o Baselines (logreg + TF-IDF), small transformers (DistilBERT), domain-tuned LMs\n\n- o RAG: chunking, embeddings, vector stores, reranking; prompt templates\n\n- Production concerns\n\n- o Guardrails: profanity/PII filters, allowed topics, citation requirements\n\n- o Latency vs cost (prompt size, temperature, context window)\n\n- o Hallucination mitigation: retrieval grounding, constrained decoding, answerability detectors\n\n- Anti-patterns\n\n- o Training on test leakage (FAQ overlap), vague labels, no human eval for generative tasks", "token_count": 438, "embedding_token_count": 461, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "NLP (classification, NER, RAG, LLM apps)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > NLP (classification, NER, RAG, LLM apps)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|NLP (classification, NER, RAG, LLM apps)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.931300Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:111-126:0011:2aeea3c8", "doc_id": "DOC02", "version": "20251014", "chunk_index": 11, "block_start_index": 111, "block_end_index": 126, "text": "pitfalls (multi-lingual, emoji, code), domain lexicons - Modeling approaches - o Baselines (logreg + TF-IDF), small transformers (DistilBERT), domain-tuned LMs - o RAG: chunking, embeddings, vector stores, reranking; prompt templates - Production concerns - o Guardrails: profanity/PII filters, allowed topics, citation requirements - o Latency vs cost (prompt size, temperature, context window) - o Hallucination mitigation: retrieval grounding, constrained decoding, answerability detectors - Anti-patterns - o Training on test leakage (FAQ overlap), vague labels, no human eval for generative tasks\n\nCV converts pixels into objects, masks, and text-used in quality control, document processing, safety, and AR. What you do (in depth): Collect labeled images/video, manage class imbalance, pick task-specific models (detection vs segmentation vs OCR), and deploy on cloud or edge with careful latency/throughput trade-offs.\n\n- Core tasks\n\n- o Object detection → mAP, latency (ms), FPS constraints\n\n- o Semantic/instance segmentation → IoU/mIoU, boundary quality\n\n- o OCR &amp; document AI → word/line CER/WER, entity extraction accuracy\n\n- Data &amp; augmentation\n\n- o Multi-angle, lighting/weather, synthetic data, mosaic/cutmix, copy-paste\n\n- o Dataset versioning, per-class metrics, hard-negative mining\n\n- Modeling approaches\n\n- o Detection: YOLO/RetinaNet/DETR families; Segmentation: U-Net/Mask R-CNN/SegFormer\n\n- o OCR pipelines: text detection (DB/EAST) + recognition (CRNN/Transformers)\n\n- Production concerns\n\n- o Edge deployment (TensorRT, ONNX, quantization), batching, NMS tuning\n\n- o Monitoring: drift via color histograms, embedding shift, class frequency changes\n\n- Anti-patterns\n\n- o Training only on 'hero' angles, ignoring rare classes, not annotating occlusions/partial objects", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Computer Vision (detection, segmentation, OCR)\n\npitfalls (multi-lingual, emoji, code), domain lexicons - Modeling approaches - o Baselines (logreg + TF-IDF), small transformers (DistilBERT), domain-tuned LMs - o RAG: chunking, embeddings, vector stores, reranking; prompt templates - Production concerns - o Guardrails: profanity/PII filters, allowed topics, citation requirements - o Latency vs cost (prompt size, temperature, context window) - o Hallucination mitigation: retrieval grounding, constrained decoding, answerability detectors - Anti-patterns - o Training on test leakage (FAQ overlap), vague labels, no human eval for generative tasks\n\nCV converts pixels into objects, masks, and text-used in quality control, document processing, safety, and AR. What you do (in depth): Collect labeled images/video, manage class imbalance, pick task-specific models (detection vs segmentation vs OCR), and deploy on cloud or edge with careful latency/throughput trade-offs.\n\n- Core tasks\n\n- o Object detection → mAP, latency (ms), FPS constraints\n\n- o Semantic/instance segmentation → IoU/mIoU, boundary quality\n\n- o OCR &amp; document AI → word/line CER/WER, entity extraction accuracy\n\n- Data &amp; augmentation\n\n- o Multi-angle, lighting/weather, synthetic data, mosaic/cutmix, copy-paste\n\n- o Dataset versioning, per-class metrics, hard-negative mining\n\n- Modeling approaches\n\n- o Detection: YOLO/RetinaNet/DETR families; Segmentation: U-Net/Mask R-CNN/SegFormer\n\n- o OCR pipelines: text detection (DB/EAST) + recognition (CRNN/Transformers)\n\n- Production concerns\n\n- o Edge deployment (TensorRT, ONNX, quantization), batching, NMS tuning\n\n- o Monitoring: drift via color histograms, embedding shift, class frequency changes\n\n- Anti-patterns\n\n- o Training only on 'hero' angles, ignoring rare classes, not annotating occlusions/partial objects", "token_count": 382, "embedding_token_count": 403, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Computer Vision (detection, segmentation, OCR)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Computer Vision (detection, segmentation, OCR)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Computer Vision (detection, segmentation, OCR)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.931648Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:128-145:0012:808edd31", "doc_id": "DOC02", "version": "20251014", "chunk_index": 12, "block_start_index": 128, "block_end_index": 145, "text": "&amp; augmentation - o Multi-angle, lighting/weather, synthetic data, mosaic/cutmix, copy-paste - o Dataset versioning, per-class metrics, hard-negative mining - Modeling approaches - o Detection: YOLO/RetinaNet/DETR families; Segmentation: U-Net/Mask R-CNN/SegFormer - o OCR pipelines: text detection (DB/EAST) + recognition (CRNN/Transformers) - Production concerns - o Edge deployment (TensorRT, ONNX, quantization), batching, NMS tuning - o Monitoring: drift via color histograms, embedding shift, class frequency changes - Anti-patterns - o Training only on 'hero' angles, ignoring rare classes, not annotating occlusions/partial objects\n\nRecsys matches users to items (content, products, ads). It's a two-tower world: fast retrieval then precise ranking, with exploration to avoid stagnation.\n\nWhat you do (in depth): Design objectives (engagement vs revenue vs diversity), build features\n\n(user/item/context), architect retrieval + ranking stacks, and validate online with A/B tests and guardrails.\n\n- Problem framing\n\n- o Retrieval (candidate generation) → Recall@k; ANN/vector search\n\n- o Ranking (score ordering) → nDCG/MAP/AUC; calibration for click-through\n\n- o Policies (bandits) for explore/exploit; slate/diversity constraints\n\n- Features &amp; signals\n\n- o Short-/long-term behavior, content embeddings, popularity priors, recency decay\n\n- o Bias handling (position bias, selection bias), counterfactual logging\n\n- Modeling approaches\n\n- o Two-tower retrieval (user/item embeddings), GBDT/Deep models for ranking (XGBoost/DeepFM/Transformer)\n\n- o Contextual bandits/Thompson sampling for exploration\n\n- Online evaluation\n\n- o Guardrails (session length, dwell time, negative feedback), win-rate by cohort\n\n- o Interference controls (holdouts, ghost experiments, CUPED for variance reduction)\n\n- Anti-patterns\n\n- o Optimizing only CTR (clickbait), ignoring saturation/novelty, neglecting cold-start strategies", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Recommender Systems (ranking, retrieval, bandits)\n\n&amp; augmentation - o Multi-angle, lighting/weather, synthetic data, mosaic/cutmix, copy-paste - o Dataset versioning, per-class metrics, hard-negative mining - Modeling approaches - o Detection: YOLO/RetinaNet/DETR families; Segmentation: U-Net/Mask R-CNN/SegFormer - o OCR pipelines: text detection (DB/EAST) + recognition (CRNN/Transformers) - Production concerns - o Edge deployment (TensorRT, ONNX, quantization), batching, NMS tuning - o Monitoring: drift via color histograms, embedding shift, class frequency changes - Anti-patterns - o Training only on 'hero' angles, ignoring rare classes, not annotating occlusions/partial objects\n\nRecsys matches users to items (content, products, ads). It's a two-tower world: fast retrieval then precise ranking, with exploration to avoid stagnation.\n\nWhat you do (in depth): Design objectives (engagement vs revenue vs diversity), build features\n\n(user/item/context), architect retrieval + ranking stacks, and validate online with A/B tests and guardrails.\n\n- Problem framing\n\n- o Retrieval (candidate generation) → Recall@k; ANN/vector search\n\n- o Ranking (score ordering) → nDCG/MAP/AUC; calibration for click-through\n\n- o Policies (bandits) for explore/exploit; slate/diversity constraints\n\n- Features &amp; signals\n\n- o Short-/long-term behavior, content embeddings, popularity priors, recency decay\n\n- o Bias handling (position bias, selection bias), counterfactual logging\n\n- Modeling approaches\n\n- o Two-tower retrieval (user/item embeddings), GBDT/Deep models for ranking (XGBoost/DeepFM/Transformer)\n\n- o Contextual bandits/Thompson sampling for exploration\n\n- Online evaluation\n\n- o Guardrails (session length, dwell time, negative feedback), win-rate by cohort\n\n- o Interference controls (holdouts, ghost experiments, CUPED for variance reduction)\n\n- Anti-patterns\n\n- o Optimizing only CTR (clickbait), ignoring saturation/novelty, neglecting cold-start strategies", "token_count": 409, "embedding_token_count": 430, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Recommender Systems (ranking, retrieval, bandits)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Recommender Systems (ranking, retrieval, bandits)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Recommender Systems (ranking, retrieval, bandits)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.932011Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:147-162:0013:07ab3f55", "doc_id": "DOC02", "version": "20251014", "chunk_index": 13, "block_start_index": 147, "block_end_index": 162, "text": "- o Short-/long-term behavior, content embeddings, popularity priors, recency decay - o Bias handling (position bias, selection bias), counterfactual logging - Modeling approaches - o Two-tower retrieval (user/item embeddings), GBDT/Deep models for ranking (XGBoost/DeepFM/Transformer) - o Contextual bandits/Thompson sampling for exploration - Online evaluation - o Guardrails (session length, dwell time, negative feedback), win-rate by cohort - o Interference controls (holdouts, ghost experiments, CUPED for variance reduction) - Anti-patterns - o Optimizing only CTR (clickbait), ignoring saturation/novelty, neglecting cold-start strategies\n\nCausal DS asks 'what caused what?' not just 'what correlates.' It powers confident decision-making and trustworthy KPI movement.\n\nWhat you do (in depth): Design experiments, compute power/MDE, run clean randomization, analyze effects with variance reduction (CUPED), and where RCTs aren't possible, use observational methods with sensitivity checks.\n\n- Experiment design\n\n- o Unit of randomization (user, session, geo), power analysis, sample ratio mismatch checks\n\n- o Metric design: north-star vs guardrails, pre-registration to avoid p-hacking\n\n- Analysis &amp; variance reduction\n\n- o CUPED (pre-period covariate regression), stratification, difference-in-differences\n\n- o Sequential testing with alpha-spending; non-parametric CI when needed\n\n- Uplift modeling (heterogeneous treatment effects)\n\n- o Methods: two-model, meta-learners (T-/S-/X-/DR-learner), causal forests\n\n- o Evaluation: Qini/uplift curves, policy value estimation\n\n- Observational causal inference\n\n- o Propensity scores, matching/weighting, IVs, front-door/back-door criteria\n\n- o Sensitivity analyses (partial R², Rosenbaum bounds)\n\n- Anti-patterns\n\n- o Peeking, metric shopping, spillover/interference, noncompliance unaddressed", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Causal Inference &amp; Experimentation (A/B, CUPED, uplift)\n\n- o Short-/long-term behavior, content embeddings, popularity priors, recency decay - o Bias handling (position bias, selection bias), counterfactual logging - Modeling approaches - o Two-tower retrieval (user/item embeddings), GBDT/Deep models for ranking (XGBoost/DeepFM/Transformer) - o Contextual bandits/Thompson sampling for exploration - Online evaluation - o Guardrails (session length, dwell time, negative feedback), win-rate by cohort - o Interference controls (holdouts, ghost experiments, CUPED for variance reduction) - Anti-patterns - o Optimizing only CTR (clickbait), ignoring saturation/novelty, neglecting cold-start strategies\n\nCausal DS asks 'what caused what?' not just 'what correlates.' It powers confident decision-making and trustworthy KPI movement.\n\nWhat you do (in depth): Design experiments, compute power/MDE, run clean randomization, analyze effects with variance reduction (CUPED), and where RCTs aren't possible, use observational methods with sensitivity checks.\n\n- Experiment design\n\n- o Unit of randomization (user, session, geo), power analysis, sample ratio mismatch checks\n\n- o Metric design: north-star vs guardrails, pre-registration to avoid p-hacking\n\n- Analysis &amp; variance reduction\n\n- o CUPED (pre-period covariate regression), stratification, difference-in-differences\n\n- o Sequential testing with alpha-spending; non-parametric CI when needed\n\n- Uplift modeling (heterogeneous treatment effects)\n\n- o Methods: two-model, meta-learners (T-/S-/X-/DR-learner), causal forests\n\n- o Evaluation: Qini/uplift curves, policy value estimation\n\n- Observational causal inference\n\n- o Propensity scores, matching/weighting, IVs, front-door/back-door criteria\n\n- o Sensitivity analyses (partial R², Rosenbaum bounds)\n\n- Anti-patterns\n\n- o Peeking, metric shopping, spillover/interference, noncompliance unaddressed", "token_count": 382, "embedding_token_count": 409, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Causal Inference &amp; Experimentation (A/B, CUPED, uplift)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Causal Inference &amp; Experimentation (A/B, CUPED, uplift)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Causal Inference &amp; Experimentation (A/B, CUPED, uplift)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.932347Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:164-180:0014:b6043620", "doc_id": "DOC02", "version": "20251014", "chunk_index": 14, "block_start_index": 164, "block_end_index": 180, "text": "guardrails, pre-registration to avoid p-hacking - Analysis &amp; variance reduction - o CUPED (pre-period covariate regression), stratification, difference-in-differences - o Sequential testing with alpha-spending; non-parametric CI when needed - Uplift modeling (heterogeneous treatment effects) - o Methods: two-model, meta-learners (T-/S-/X-/DR-learner), causal forests - o Evaluation: Qini/uplift curves, policy value estimation - Observational causal inference - o Propensity scores, matching/weighting, IVs, front-door/back-door criteria - o Sensitivity analyses (partial R², Rosenbaum bounds) - Anti-patterns - o Peeking, metric shopping, spillover/interference, noncompliance unaddressed\n\nForecasting predicts future demand/signals under seasonality, promotions, and shocks. Choose the simplest model that meets accuracy and operational needs.\n\nWhat you do (in depth): Build robust pipelines with correct cross-validation, add exogenous regressors, and translate forecasts into inventory/staffing/pricing decisions.\n\n- Data characteristics\n\n- o Granularity (SKU-store-day), intermittency, hierarchy (global → regional → store), cold-start\n\n- o Feature engineering: holiday calendars, lags/rolling stats, price/promos, weather\n\n- Modeling approaches\n\n- o Classical: ARIMA/SARIMA, ETS, TBATS, Prophet; hierarchical reconciliation\n\n- o Machine learning: gradient boosting with lag features, quantile regression\n\n- o Deep: LSTM/TCN/Temporal Fusion Transformer/Informer for long context\n\n- Evaluation &amp; validation\n\n- o Rolling-origin CV; metrics: MAPE/sMAPE/WAPE, RMSE; quantile loss &amp; coverage for PIs\n\n- o Business metrics: stock-outs, overstock cost, service levels (fill rate)\n\n- Production concerns\n\n- o Retrain cadence, anomaly handling, blackout periods, backfill logic\n\n- o Scenario planning (best/base/worst), forecast explainability for planners\n\n- Anti-patterns\n\n- o Random CV splits, metric misuse on intermittent series, ignoring hierarchy leakage", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Forecasting &amp; Time-Series (classical vs deep)\n\nguardrails, pre-registration to avoid p-hacking - Analysis &amp; variance reduction - o CUPED (pre-period covariate regression), stratification, difference-in-differences - o Sequential testing with alpha-spending; non-parametric CI when needed - Uplift modeling (heterogeneous treatment effects) - o Methods: two-model, meta-learners (T-/S-/X-/DR-learner), causal forests - o Evaluation: Qini/uplift curves, policy value estimation - Observational causal inference - o Propensity scores, matching/weighting, IVs, front-door/back-door criteria - o Sensitivity analyses (partial R², Rosenbaum bounds) - Anti-patterns - o Peeking, metric shopping, spillover/interference, noncompliance unaddressed\n\nForecasting predicts future demand/signals under seasonality, promotions, and shocks. Choose the simplest model that meets accuracy and operational needs.\n\nWhat you do (in depth): Build robust pipelines with correct cross-validation, add exogenous regressors, and translate forecasts into inventory/staffing/pricing decisions.\n\n- Data characteristics\n\n- o Granularity (SKU-store-day), intermittency, hierarchy (global → regional → store), cold-start\n\n- o Feature engineering: holiday calendars, lags/rolling stats, price/promos, weather\n\n- Modeling approaches\n\n- o Classical: ARIMA/SARIMA, ETS, TBATS, Prophet; hierarchical reconciliation\n\n- o Machine learning: gradient boosting with lag features, quantile regression\n\n- o Deep: LSTM/TCN/Temporal Fusion Transformer/Informer for long context\n\n- Evaluation &amp; validation\n\n- o Rolling-origin CV; metrics: MAPE/sMAPE/WAPE, RMSE; quantile loss &amp; coverage for PIs\n\n- o Business metrics: stock-outs, overstock cost, service levels (fill rate)\n\n- Production concerns\n\n- o Retrain cadence, anomaly handling, blackout periods, backfill logic\n\n- o Scenario planning (best/base/worst), forecast explainability for planners\n\n- Anti-patterns\n\n- o Random CV splits, metric misuse on intermittent series, ignoring hierarchy leakage", "token_count": 400, "embedding_token_count": 424, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Forecasting &amp; Time-Series (classical vs deep)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Forecasting &amp; Time-Series (classical vs deep)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Forecasting &amp; Time-Series (classical vs deep)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.932712Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:182-198:0015:1e7c3d53", "doc_id": "DOC02", "version": "20251014", "chunk_index": 15, "block_start_index": 182, "block_end_index": 198, "text": "boosting with lag features, quantile regression - o Deep: LSTM/TCN/Temporal Fusion Transformer/Informer for long context - Evaluation &amp; validation - o Rolling-origin CV; metrics: MAPE/sMAPE/WAPE, RMSE; quantile loss &amp; coverage for PIs - o Business metrics: stock-outs, overstock cost, service levels (fill rate) - Production concerns - o Retrain cadence, anomaly handling, blackout periods, backfill logic - o Scenario planning (best/base/worst), forecast explainability for planners - Anti-patterns - o Random CV splits, metric misuse on intermittent series, ignoring hierarchy leakage\n\nGenAI apps synthesize, plan, and converse. Success depends on grounding, safety, and cost/latency control-not just model choice.\n\nWhat you do (in depth): Design tasks, choose between prompting, tool-use, RAG, or fine-tuning, define human+automatic evals, and implement safety/abuse controls.\n\n- Build choices\n\n- o Prompt engineering &amp; tool-use (function calling, retrieval)\n\n- o Lightweight adaptation (LoRA, adapters) vs full fine-tune; when to choose which\n\n- o RAG stacks (chunk, embed, retrieve, rerank, generate) with caching\n\n- Evaluation\n\n- o Automated: exact match, BLEU/ROUGE for structured tasks; rubric-based LLM judges\n\n- o Human: helpfulness, faithfulness, harmlessness; rubric sampling per release\n\n- Safety &amp; compliance\n\n- o Red-teaming, jailbreak resistance, toxicity/PII filters, content policies\n\n- o Data governance: consented corpora, opt-out, copyright handling\n\n- Ops &amp; economics\n\n- o Latency budgets, context window management, response caching, cost per 1k tokens\n\n- o Observability: prompt/version registry, drift and hallucination rates, incident playbooks\n\n- Anti-patterns\n\n- o No ground truth eval, over-long contexts, letting hallucinations reach users unflagged", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > GenAI / LLM Applications (prompting, fine-tuning, safety)\n\nboosting with lag features, quantile regression - o Deep: LSTM/TCN/Temporal Fusion Transformer/Informer for long context - Evaluation &amp; validation - o Rolling-origin CV; metrics: MAPE/sMAPE/WAPE, RMSE; quantile loss &amp; coverage for PIs - o Business metrics: stock-outs, overstock cost, service levels (fill rate) - Production concerns - o Retrain cadence, anomaly handling, blackout periods, backfill logic - o Scenario planning (best/base/worst), forecast explainability for planners - Anti-patterns - o Random CV splits, metric misuse on intermittent series, ignoring hierarchy leakage\n\nGenAI apps synthesize, plan, and converse. Success depends on grounding, safety, and cost/latency control-not just model choice.\n\nWhat you do (in depth): Design tasks, choose between prompting, tool-use, RAG, or fine-tuning, define human+automatic evals, and implement safety/abuse controls.\n\n- Build choices\n\n- o Prompt engineering &amp; tool-use (function calling, retrieval)\n\n- o Lightweight adaptation (LoRA, adapters) vs full fine-tune; when to choose which\n\n- o RAG stacks (chunk, embed, retrieve, rerank, generate) with caching\n\n- Evaluation\n\n- o Automated: exact match, BLEU/ROUGE for structured tasks; rubric-based LLM judges\n\n- o Human: helpfulness, faithfulness, harmlessness; rubric sampling per release\n\n- Safety &amp; compliance\n\n- o Red-teaming, jailbreak resistance, toxicity/PII filters, content policies\n\n- o Data governance: consented corpora, opt-out, copyright handling\n\n- Ops &amp; economics\n\n- o Latency budgets, context window management, response caching, cost per 1k tokens\n\n- o Observability: prompt/version registry, drift and hallucination rates, incident playbooks\n\n- Anti-patterns\n\n- o No ground truth eval, over-long contexts, letting hallucinations reach users unflagged", "token_count": 374, "embedding_token_count": 399, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "GenAI / LLM Applications (prompting, fine-tuning, safety)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > GenAI / LLM Applications (prompting, fine-tuning, safety)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|GenAI / LLM Applications (prompting, fine-tuning, safety)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.933010Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:200-215:0016:8d28b322", "doc_id": "DOC02", "version": "20251014", "chunk_index": 16, "block_start_index": 200, "block_end_index": 215, "text": "structured tasks; rubric-based LLM judges - o Human: helpfulness, faithfulness, harmlessness; rubric sampling per release - Safety &amp; compliance - o Red-teaming, jailbreak resistance, toxicity/PII filters, content policies - o Data governance: consented corpora, opt-out, copyright handling - Ops &amp; economics - o Latency budgets, context window management, response caching, cost per 1k tokens - o Observability: prompt/version registry, drift and hallucination rates, incident playbooks - Anti-patterns - o No ground truth eval, over-long contexts, letting hallucinations reach users unflagged\n\nTwo complementary tracks: Applied Research pushes methods and prototypes; Product DS ships impact to customers and metrics.\n\nWhat you do (in depth): Choose the track (or blend) that matches the problem horizon, then manage hand-offs so research becomes real product value.\n\n- Scope &amp; time horizon\n\n- o Applied Research: 6-24-month horizon, novel methods, internal/external publications\n\n- o Product DS: 2-12-week iterations, shipping features/models to users\n\n- Deliverables\n\n- o Research: tech notes, benchmarks, reference implementations, datasets/simulators\n\n- o Product: KPI movement, A/B reads, dashboards, model services with SLAs\n\n- Collaboration patterns\n\n- o Research ↔ Product: gated tech transfer, pilot experiments, de-risking studies\n\n- o With MLE/Platform: productionization plans, latency/cost modeling, monitoring specs\n\n- Success metrics\n\n- o Research: SOTA/near-SOTA on internal benchmarks, adoption by product teams\n\n- o Product: sustained metric lift, reliability, maintainability, stakeholder adoption\n\n- Anti-patterns\n\n- o Research with no path to integration; product work ignoring methodological risks", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Applied Research vs Product DS (horizons, deliverables)\n\nstructured tasks; rubric-based LLM judges - o Human: helpfulness, faithfulness, harmlessness; rubric sampling per release - Safety &amp; compliance - o Red-teaming, jailbreak resistance, toxicity/PII filters, content policies - o Data governance: consented corpora, opt-out, copyright handling - Ops &amp; economics - o Latency budgets, context window management, response caching, cost per 1k tokens - o Observability: prompt/version registry, drift and hallucination rates, incident playbooks - Anti-patterns - o No ground truth eval, over-long contexts, letting hallucinations reach users unflagged\n\nTwo complementary tracks: Applied Research pushes methods and prototypes; Product DS ships impact to customers and metrics.\n\nWhat you do (in depth): Choose the track (or blend) that matches the problem horizon, then manage hand-offs so research becomes real product value.\n\n- Scope &amp; time horizon\n\n- o Applied Research: 6-24-month horizon, novel methods, internal/external publications\n\n- o Product DS: 2-12-week iterations, shipping features/models to users\n\n- Deliverables\n\n- o Research: tech notes, benchmarks, reference implementations, datasets/simulators\n\n- o Product: KPI movement, A/B reads, dashboards, model services with SLAs\n\n- Collaboration patterns\n\n- o Research ↔ Product: gated tech transfer, pilot experiments, de-risking studies\n\n- o With MLE/Platform: productionization plans, latency/cost modeling, monitoring specs\n\n- Success metrics\n\n- o Research: SOTA/near-SOTA on internal benchmarks, adoption by product teams\n\n- o Product: sustained metric lift, reliability, maintainability, stakeholder adoption\n\n- Anti-patterns\n\n- o Research with no path to integration; product work ignoring methodological risks", "token_count": 338, "embedding_token_count": 360, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Applied Research vs Product DS (horizons, deliverables)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Applied Research vs Product DS (horizons, deliverables)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Applied Research vs Product DS (horizons, deliverables)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.933358Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:218-225:0017:603352e8", "doc_id": "DOC02", "version": "20251014", "chunk_index": 17, "block_start_index": 218, "block_end_index": 225, "text": "benchmarks, reference implementations, datasets/simulators - o Product: KPI movement, A/B reads, dashboards, model services with SLAs - Collaboration patterns - o Research ↔ Product: gated tech transfer, pilot experiments, de-risking studies - o With MLE/Platform: productionization plans, latency/cost modeling, monitoring specs - Success metrics - o Research: SOTA/near-SOTA on internal benchmarks, adoption by product teams - o Product: sustained metric lift, reliability, maintainability, stakeholder adoption - Anti-patterns - o Research with no path to integration; product work ignoring methodological risks\n\nA data scientist's rigor comes from statistical thinking: modeling uncertainty, identifying signal vs noise, and making defensible claims about cause and effect. You should be able to choose appropriate assumptions, quantify uncertainty, and explain trade-offs (bias/variance, power, sample size).\n\n- Probability &amp; distributions: conditional probability, Bayes rule; common families (Bernoulli/Binomial, Poisson, Gaussian, Log-normal); when heavy tails matter.\n\n- Estimation: MLE/MAP, bias-variance trade-off, bootstrapping/jackknife for CIs, calibration of predicted probabilities.\n\n- Hypothesis testing: null/alt, p-values, power/MDE, multiple testing control (BH/FDR), non-parametric tests (MWU/KS).\n\n- Uncertainty: confidence vs prediction intervals; propagation of error; robust stats (medians, Huber loss).\n\n- Causality basics: potential outcomes, ATE/ATE by cohort (HTE), confounding, DAGs/back-door, randomization as gold standard.\n\n- Observational methods: matching/weighting, propensity scores, DiD, IVs; sensitivity analysis to hidden bias.\n\n- Common pitfalls: p-hacking/metric shopping, Simpson's paradox, leakage, non-IID data (clusters, time dependence).", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Math &amp; Stats (probability, inference, causality basics)\n\nbenchmarks, reference implementations, datasets/simulators - o Product: KPI movement, A/B reads, dashboards, model services with SLAs - Collaboration patterns - o Research ↔ Product: gated tech transfer, pilot experiments, de-risking studies - o With MLE/Platform: productionization plans, latency/cost modeling, monitoring specs - Success metrics - o Research: SOTA/near-SOTA on internal benchmarks, adoption by product teams - o Product: sustained metric lift, reliability, maintainability, stakeholder adoption - Anti-patterns - o Research with no path to integration; product work ignoring methodological risks\n\nA data scientist's rigor comes from statistical thinking: modeling uncertainty, identifying signal vs noise, and making defensible claims about cause and effect. You should be able to choose appropriate assumptions, quantify uncertainty, and explain trade-offs (bias/variance, power, sample size).\n\n- Probability &amp; distributions: conditional probability, Bayes rule; common families (Bernoulli/Binomial, Poisson, Gaussian, Log-normal); when heavy tails matter.\n\n- Estimation: MLE/MAP, bias-variance trade-off, bootstrapping/jackknife for CIs, calibration of predicted probabilities.\n\n- Hypothesis testing: null/alt, p-values, power/MDE, multiple testing control (BH/FDR), non-parametric tests (MWU/KS).\n\n- Uncertainty: confidence vs prediction intervals; propagation of error; robust stats (medians, Huber loss).\n\n- Causality basics: potential outcomes, ATE/ATE by cohort (HTE), confounding, DAGs/back-door, randomization as gold standard.\n\n- Observational methods: matching/weighting, propensity scores, DiD, IVs; sensitivity analysis to hidden bias.\n\n- Common pitfalls: p-hacking/metric shopping, Simpson's paradox, leakage, non-IID data (clusters, time dependence).", "token_count": 365, "embedding_token_count": 390, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Math &amp; Stats (probability, inference, causality basics)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Math &amp; Stats (probability, inference, causality basics)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Math &amp; Stats (probability, inference, causality basics)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.933698Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:227-243:0018:d31e11f6", "doc_id": "DOC02", "version": "20251014", "chunk_index": 18, "block_start_index": 227, "block_end_index": 243, "text": "- Estimation: MLE/MAP, bias-variance trade-off, bootstrapping/jackknife for CIs, calibration of predicted probabilities. - Hypothesis testing: null/alt, p-values, power/MDE, multiple testing control (BH/FDR), non-parametric tests (MWU/KS). - Uncertainty: confidence vs prediction intervals; propagation of error; robust stats (medians, Huber loss). - Causality basics: potential outcomes, ATE/ATE by cohort (HTE), confounding, DAGs/back-door, randomization as gold standard. - Observational methods: matching/weighting, propensity scores, DiD, IVs; sensitivity analysis to hidden bias. - Common pitfalls: p-hacking/metric shopping, Simpson's paradox, leakage, non-IID data (clusters, time dependence).\n\nKnow when a simple model wins (interpretability, speed) and when complex ones pay off. Be fluent in failure modes, key hyperparameters, and evaluation suited to the problem.\n\n- Linear/Generalized Linear Models\n\n- o When: fast baselines, linear/semi-linear relations, explainability.\n\n- o Tools: regularization (L1/L2/elastic net), link functions (logit/poisson).\n\n- o Watch-outs: multicollinearity, feature scaling, outliers, nonlinearity.\n\n- Trees &amp; Gradient Boosting (RF, XGBoost/LightGBM, CatBoost)\n\n- o Strengths: nonlinearities, interactions, strong tabular performance.\n\n- o Keys: depth, learning\\_rate, n\\_estimators, min\\_child\\_weight; early stopping; class weights.\n\n- o Watch-outs: overfitting without early stopping; leakage in target encoding; imbalanced data handling.\n\n- Clustering &amp; Unsupervised\n\n- o K-means/K-medoids, DBSCAN/HDBSCAN, Gaussian Mixtures; dimensionality reduction (PCA, UMAP).\n\n- o Evaluate: silhouette, Davies-Bouldin, stability; always validate clusters against business utility.\n\n- o Watch-outs: scale sensitivity, arbitrary K, spurious clusters from noise.\n\n- Embeddings\n\n- o Tabular (entity embeddings), text (word/transformer embeddings), images (CNN/ViT features).\n\n- o Uses: retrieval, similarity, cold-start, feature learning; metric learning/triplet losses.\n\n- o Watch-outs: drift, alignment with downstream objective, privacy concerns with vector search.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > ML Algorithms (trees/boosting, linear models, clustering, embeddings)\n\n- Estimation: MLE/MAP, bias-variance trade-off, bootstrapping/jackknife for CIs, calibration of predicted probabilities. - Hypothesis testing: null/alt, p-values, power/MDE, multiple testing control (BH/FDR), non-parametric tests (MWU/KS). - Uncertainty: confidence vs prediction intervals; propagation of error; robust stats (medians, Huber loss). - Causality basics: potential outcomes, ATE/ATE by cohort (HTE), confounding, DAGs/back-door, randomization as gold standard. - Observational methods: matching/weighting, propensity scores, DiD, IVs; sensitivity analysis to hidden bias. - Common pitfalls: p-hacking/metric shopping, Simpson's paradox, leakage, non-IID data (clusters, time dependence).\n\nKnow when a simple model wins (interpretability, speed) and when complex ones pay off. Be fluent in failure modes, key hyperparameters, and evaluation suited to the problem.\n\n- Linear/Generalized Linear Models\n\n- o When: fast baselines, linear/semi-linear relations, explainability.\n\n- o Tools: regularization (L1/L2/elastic net), link functions (logit/poisson).\n\n- o Watch-outs: multicollinearity, feature scaling, outliers, nonlinearity.\n\n- Trees &amp; Gradient Boosting (RF, XGBoost/LightGBM, CatBoost)\n\n- o Strengths: nonlinearities, interactions, strong tabular performance.\n\n- o Keys: depth, learning\\_rate, n\\_estimators, min\\_child\\_weight; early stopping; class weights.\n\n- o Watch-outs: overfitting without early stopping; leakage in target encoding; imbalanced data handling.\n\n- Clustering &amp; Unsupervised\n\n- o K-means/K-medoids, DBSCAN/HDBSCAN, Gaussian Mixtures; dimensionality reduction (PCA, UMAP).\n\n- o Evaluate: silhouette, Davies-Bouldin, stability; always validate clusters against business utility.\n\n- o Watch-outs: scale sensitivity, arbitrary K, spurious clusters from noise.\n\n- Embeddings\n\n- o Tabular (entity embeddings), text (word/transformer embeddings), images (CNN/ViT features).\n\n- o Uses: retrieval, similarity, cold-start, feature learning; metric learning/triplet losses.\n\n- o Watch-outs: drift, alignment with downstream objective, privacy concerns with vector search.", "token_count": 460, "embedding_token_count": 486, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "ML Algorithms (trees/boosting, linear models, clustering, embeddings)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > ML Algorithms (trees/boosting, linear models, clustering, embeddings)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|ML Algorithms (trees/boosting, linear models, clustering, embeddings)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.934050Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:245-252:0019:99483be7", "doc_id": "DOC02", "version": "20251014", "chunk_index": 19, "block_start_index": 245, "block_end_index": 252, "text": "in target encoding; imbalanced data handling. - Clustering &amp; Unsupervised - o K-means/K-medoids, DBSCAN/HDBSCAN, Gaussian Mixtures; dimensionality reduction (PCA, UMAP). - o Evaluate: silhouette, Davies-Bouldin, stability; always validate clusters against business utility. - o Watch-outs: scale sensitivity, arbitrary K, spurious clusters from noise. - Embeddings - o Tabular (entity embeddings), text (word/transformer embeddings), images (CNN/ViT features). - o Uses: retrieval, similarity, cold-start, feature learning; metric learning/triplet losses. - o Watch-outs: drift, alignment with downstream objective, privacy concerns with vector search.\n\nProduction-oriented coding turns notebooks into reliable assets. Aim for readable, testable, reproducible code with clear environments and CI.\n\n- Environments &amp; packaging: pyproject.toml/Poetry or pip + requirements; pinned versions; reproducible seeds.\n\n- Structure: notebook → module pattern; CLI scripts; config via .env/YAML; logging (logging), typing (typing, pydantic).\n\n- Testing: unit tests for feature functions and metrics (pytest); data contract tests (Great Expectations); property-based tests (hypothesis).\n\n- Reproducibility: seeds, data snapshots, deterministic ops; model/feature registries (MLflow).\n\n- Performance: vectorization (NumPy/Pandas), profiling (cProfile, line\\_profiler), parallelism (joblib), JIT (Numba) when needed.\n\n- Quality: lint/format (ruff/black), pre-commit hooks, small pure functions, docstrings.\n\n- Anti-patterns: hidden state in notebooks, mixing EDA &amp; training code paths, unpinned deps.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Programming (Python/R), packaging, testing\n\nin target encoding; imbalanced data handling. - Clustering &amp; Unsupervised - o K-means/K-medoids, DBSCAN/HDBSCAN, Gaussian Mixtures; dimensionality reduction (PCA, UMAP). - o Evaluate: silhouette, Davies-Bouldin, stability; always validate clusters against business utility. - o Watch-outs: scale sensitivity, arbitrary K, spurious clusters from noise. - Embeddings - o Tabular (entity embeddings), text (word/transformer embeddings), images (CNN/ViT features). - o Uses: retrieval, similarity, cold-start, feature learning; metric learning/triplet losses. - o Watch-outs: drift, alignment with downstream objective, privacy concerns with vector search.\n\nProduction-oriented coding turns notebooks into reliable assets. Aim for readable, testable, reproducible code with clear environments and CI.\n\n- Environments &amp; packaging: pyproject.toml/Poetry or pip + requirements; pinned versions; reproducible seeds.\n\n- Structure: notebook → module pattern; CLI scripts; config via .env/YAML; logging (logging), typing (typing, pydantic).\n\n- Testing: unit tests for feature functions and metrics (pytest); data contract tests (Great Expectations); property-based tests (hypothesis).\n\n- Reproducibility: seeds, data snapshots, deterministic ops; model/feature registries (MLflow).\n\n- Performance: vectorization (NumPy/Pandas), profiling (cProfile, line\\_profiler), parallelism (joblib), JIT (Numba) when needed.\n\n- Quality: lint/format (ruff/black), pre-commit hooks, small pure functions, docstrings.\n\n- Anti-patterns: hidden state in notebooks, mixing EDA &amp; training code paths, unpinned deps.", "token_count": 343, "embedding_token_count": 365, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Programming (Python/R), packaging, testing"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Programming (Python/R), packaging, testing", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Programming (Python/R), packaging, testing", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.934610Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:254-261:0020:3090c9c0", "doc_id": "DOC02", "version": "20251014", "chunk_index": 20, "block_start_index": 254, "block_end_index": 261, "text": "Structure: notebook → module pattern; CLI scripts; config via .env/YAML; logging (logging), typing (typing, pydantic). - Testing: unit tests for feature functions and metrics (pytest); data contract tests (Great Expectations); property-based tests (hypothesis). - Reproducibility: seeds, data snapshots, deterministic ops; model/feature registries (MLflow). - Performance: vectorization (NumPy/Pandas), profiling (cProfile, line\\_profiler), parallelism (joblib), JIT (Numba) when needed. - Quality: lint/format (ruff/black), pre-commit hooks, small pure functions, docstrings. - Anti-patterns: hidden state in notebooks, mixing EDA &amp; training code paths, unpinned deps.\n\nStrong SQL prevents subtle errors and slow dashboards. You should write correct, explainable queries that scale and respect data contracts.\n\n- Joins: inner/left/right/full; null semantics; de-duplication with surrogate keys; many-to-many explode hazards.\n\n- Window functions: ROW\\_NUMBER, LAG/LEAD, cumulative sums, sessionization; partitioning by entity and ordering by time.\n\n- Time &amp; grains: defining canonical event time, late-arriving data, slowly changing dimensions (SCD), timezone consistency.\n\n- Performance: predicate pushdown, clustering/partitioning, avoiding SELECT *, approximate functions for big scans.\n\n- Reliability: CTEs for readability; idempotent incremental loads (MERGE/UPSERT); data quality checks (row counts, uniqueness, referential integrity).\n\n- Wrangling: tidy data shapes, wide ↔ long pivots, categorical encoding, missing-data strategies.\n\n- Anti-patterns: windowing without partitions, cartesian joins, computing KPIs at mixed grains.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > SQL &amp; Data Wrangling (joins, window funcs, performance)\n\nStructure: notebook → module pattern; CLI scripts; config via .env/YAML; logging (logging), typing (typing, pydantic). - Testing: unit tests for feature functions and metrics (pytest); data contract tests (Great Expectations); property-based tests (hypothesis). - Reproducibility: seeds, data snapshots, deterministic ops; model/feature registries (MLflow). - Performance: vectorization (NumPy/Pandas), profiling (cProfile, line\\_profiler), parallelism (joblib), JIT (Numba) when needed. - Quality: lint/format (ruff/black), pre-commit hooks, small pure functions, docstrings. - Anti-patterns: hidden state in notebooks, mixing EDA &amp; training code paths, unpinned deps.\n\nStrong SQL prevents subtle errors and slow dashboards. You should write correct, explainable queries that scale and respect data contracts.\n\n- Joins: inner/left/right/full; null semantics; de-duplication with surrogate keys; many-to-many explode hazards.\n\n- Window functions: ROW\\_NUMBER, LAG/LEAD, cumulative sums, sessionization; partitioning by entity and ordering by time.\n\n- Time &amp; grains: defining canonical event time, late-arriving data, slowly changing dimensions (SCD), timezone consistency.\n\n- Performance: predicate pushdown, clustering/partitioning, avoiding SELECT *, approximate functions for big scans.\n\n- Reliability: CTEs for readability; idempotent incremental loads (MERGE/UPSERT); data quality checks (row counts, uniqueness, referential integrity).\n\n- Wrangling: tidy data shapes, wide ↔ long pivots, categorical encoding, missing-data strategies.\n\n- Anti-patterns: windowing without partitions, cartesian joins, computing KPIs at mixed grains.", "token_count": 345, "embedding_token_count": 371, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "SQL &amp; Data Wrangling (joins, window funcs, performance)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > SQL &amp; Data Wrangling (joins, window funcs, performance)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|SQL &amp; Data Wrangling (joins, window funcs, performance)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.934972Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:263-271:0021:17c77260", "doc_id": "DOC02", "version": "20251014", "chunk_index": 21, "block_start_index": 263, "block_end_index": 271, "text": "sums, sessionization; partitioning by entity and ordering by time. - Time &amp; grains: defining canonical event time, late-arriving data, slowly changing dimensions (SCD), timezone consistency. - Performance: predicate pushdown, clustering/partitioning, avoiding SELECT *, approximate functions for big scans. - Reliability: CTEs for readability; idempotent incremental loads (MERGE/UPSERT); data quality checks (row counts, uniqueness, referential integrity). - Wrangling: tidy data shapes, wide ↔ long pivots, categorical encoding, missing-data strategies. - Anti-patterns: windowing without partitions, cartesian joins, computing KPIs at mixed grains.\n\nVisuals are for decisions, not decoration. Great storytelling frames the question, shows comparisons, and annotates the 'so what'.\n\n- Narrative arc: question → method (brief) → result → implication/recommendation → risk/next step.\n\n- Chart choices:\n\n- o Distribution (hist/box/violin), comparison (bar/line), relationship (scatter), part-of-whole (stacked bars), uncertainty (error bands/intervals).\n\n- Principles: preattentive cues (position first), declutter (zero baselines where needed), consistent scales/colors, highlight deltas.\n\n- Uncertainty: CIs, prediction intervals, fan charts, sample sizes; avoid hiding variance.\n\n- Dashboards vs memos: operational monitoring (freshness &amp; guardrails) vs one-time decision docs (annotated insights).\n\n- Accessibility: colorblind-safe palettes, readable labels, mobile-first layouts when needed.\n\n- Anti-patterns: dual y-axes misuse, 3D charts, truncated axes to exaggerate effects, KPI soup without context.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Visualization &amp; Storytelling (clear narratives, visuals)\n\nsums, sessionization; partitioning by entity and ordering by time. - Time &amp; grains: defining canonical event time, late-arriving data, slowly changing dimensions (SCD), timezone consistency. - Performance: predicate pushdown, clustering/partitioning, avoiding SELECT *, approximate functions for big scans. - Reliability: CTEs for readability; idempotent incremental loads (MERGE/UPSERT); data quality checks (row counts, uniqueness, referential integrity). - Wrangling: tidy data shapes, wide ↔ long pivots, categorical encoding, missing-data strategies. - Anti-patterns: windowing without partitions, cartesian joins, computing KPIs at mixed grains.\n\nVisuals are for decisions, not decoration. Great storytelling frames the question, shows comparisons, and annotates the 'so what'.\n\n- Narrative arc: question → method (brief) → result → implication/recommendation → risk/next step.\n\n- Chart choices:\n\n- o Distribution (hist/box/violin), comparison (bar/line), relationship (scatter), part-of-whole (stacked bars), uncertainty (error bands/intervals).\n\n- Principles: preattentive cues (position first), declutter (zero baselines where needed), consistent scales/colors, highlight deltas.\n\n- Uncertainty: CIs, prediction intervals, fan charts, sample sizes; avoid hiding variance.\n\n- Dashboards vs memos: operational monitoring (freshness &amp; guardrails) vs one-time decision docs (annotated insights).\n\n- Accessibility: colorblind-safe palettes, readable labels, mobile-first layouts when needed.\n\n- Anti-patterns: dual y-axes misuse, 3D charts, truncated axes to exaggerate effects, KPI soup without context.", "token_count": 330, "embedding_token_count": 353, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Visualization &amp; Storytelling (clear narratives, visuals)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Visualization &amp; Storytelling (clear narratives, visuals)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Visualization &amp; Storytelling (clear narratives, visuals)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.935365Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:273-281:0022:1d476c12", "doc_id": "DOC02", "version": "20251014", "chunk_index": 22, "block_start_index": 273, "block_end_index": 281, "text": "Distribution (hist/box/violin), comparison (bar/line), relationship (scatter), part-of-whole (stacked bars), uncertainty (error bands/intervals). - Principles: preattentive cues (position first), declutter (zero baselines where needed), consistent scales/colors, highlight deltas. - Uncertainty: CIs, prediction intervals, fan charts, sample sizes; avoid hiding variance. - Dashboards vs memos: operational monitoring (freshness &amp; guardrails) vs one-time decision docs (annotated insights). - Accessibility: colorblind-safe palettes, readable labels, mobile-first layouts when needed. - Anti-patterns: dual y-axes misuse, 3D charts, truncated axes to exaggerate effects, KPI soup without context.\n\nTechnical excellence only matters if it changes outcomes. Frame problems as decisions under constraints with explicit payoffs and risks.\n\n- Define the decision: action set, frequency, who acts; map to a north-star metric and guardrails.\n\n- Hypotheses: causal story of how your lever moves the metric; measurable assumptions.\n\n- Constraints: latency, budget, data availability/quality, regulatory/privacy limits, fairness thresholds.\n\n- ROI model: back-of-envelope value, cost to build/maintain, sensitivity to adoption and error rates.\n\n- Data readiness: sources, coverage, leakage risks, label latency, feedback loops.\n\n- Evaluation plan: offline metrics ↔ business KPI, decision thresholds (ROC → profit curve), A/B or quasiexperimental design.\n\n- Risk &amp; fallback: failure modes, rollback, shadow launches, human-in-the-loop.\n\n- Anti-patterns: starting from an algorithm, optimizing proxy metrics, ignoring operational adoption.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Business Problem Framing (hypotheses, constraints, ROI)\n\nDistribution (hist/box/violin), comparison (bar/line), relationship (scatter), part-of-whole (stacked bars), uncertainty (error bands/intervals). - Principles: preattentive cues (position first), declutter (zero baselines where needed), consistent scales/colors, highlight deltas. - Uncertainty: CIs, prediction intervals, fan charts, sample sizes; avoid hiding variance. - Dashboards vs memos: operational monitoring (freshness &amp; guardrails) vs one-time decision docs (annotated insights). - Accessibility: colorblind-safe palettes, readable labels, mobile-first layouts when needed. - Anti-patterns: dual y-axes misuse, 3D charts, truncated axes to exaggerate effects, KPI soup without context.\n\nTechnical excellence only matters if it changes outcomes. Frame problems as decisions under constraints with explicit payoffs and risks.\n\n- Define the decision: action set, frequency, who acts; map to a north-star metric and guardrails.\n\n- Hypotheses: causal story of how your lever moves the metric; measurable assumptions.\n\n- Constraints: latency, budget, data availability/quality, regulatory/privacy limits, fairness thresholds.\n\n- ROI model: back-of-envelope value, cost to build/maintain, sensitivity to adoption and error rates.\n\n- Data readiness: sources, coverage, leakage risks, label latency, feedback loops.\n\n- Evaluation plan: offline metrics ↔ business KPI, decision thresholds (ROC → profit curve), A/B or quasiexperimental design.\n\n- Risk &amp; fallback: failure modes, rollback, shadow launches, human-in-the-loop.\n\n- Anti-patterns: starting from an algorithm, optimizing proxy metrics, ignoring operational adoption.", "token_count": 339, "embedding_token_count": 361, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Business Problem Framing (hypotheses, constraints, ROI)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Business Problem Framing (hypotheses, constraints, ROI)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Business Problem Framing (hypotheses, constraints, ROI)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.935687Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:283-290:0023:b09ae39b", "doc_id": "DOC02", "version": "20251014", "chunk_index": 23, "block_start_index": 283, "block_end_index": 290, "text": "moves the metric; measurable assumptions. - Constraints: latency, budget, data availability/quality, regulatory/privacy limits, fairness thresholds. - ROI model: back-of-envelope value, cost to build/maintain, sensitivity to adoption and error rates. - Data readiness: sources, coverage, leakage risks, label latency, feedback loops. - Evaluation plan: offline metrics ↔ business KPI, decision thresholds (ROC → profit curve), A/B or quasiexperimental design. - Risk &amp; fallback: failure modes, rollback, shadow launches, human-in-the-loop. - Anti-patterns: starting from an algorithm, optimizing proxy metrics, ignoring operational adoption.\n\nInfluence comes from clarity, cadence, and trust. Tailor depth to the audience and keep a shared source of truth.\n\n- Audience mapping: execs (outcomes/risks), PMs (trade-offs/roadmap), Eng (interfaces/SLAs), Ops (playbooks), Legal (compliance).\n\n- Artifacts: 1-pager PRD/DRD, experiment pre-reg, weekly updates, launch checklist, post-mortems; living metric definitions.\n\n- Cadence: kickoffs, design reviews, decision meetings with options, read-outs with 'recommend / next steps / owner'.\n\n- Expectation setting: scope, assumptions, MDE/power limits, timelines; say 'no' with alternatives.\n\n- Collaboration: write clear specs (schemas, APIs), agree on data contracts, co-own monitoring.\n\n- Conflict handling: surface trade-offs, show scenario tables, log decisions; escalate with crisp evidence.\n\n- Anti-patterns: over-promising, burying the lede, jargon dumps, undocumented decisions.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Communication &amp; Stakeholder Management\n\nmoves the metric; measurable assumptions. - Constraints: latency, budget, data availability/quality, regulatory/privacy limits, fairness thresholds. - ROI model: back-of-envelope value, cost to build/maintain, sensitivity to adoption and error rates. - Data readiness: sources, coverage, leakage risks, label latency, feedback loops. - Evaluation plan: offline metrics ↔ business KPI, decision thresholds (ROC → profit curve), A/B or quasiexperimental design. - Risk &amp; fallback: failure modes, rollback, shadow launches, human-in-the-loop. - Anti-patterns: starting from an algorithm, optimizing proxy metrics, ignoring operational adoption.\n\nInfluence comes from clarity, cadence, and trust. Tailor depth to the audience and keep a shared source of truth.\n\n- Audience mapping: execs (outcomes/risks), PMs (trade-offs/roadmap), Eng (interfaces/SLAs), Ops (playbooks), Legal (compliance).\n\n- Artifacts: 1-pager PRD/DRD, experiment pre-reg, weekly updates, launch checklist, post-mortems; living metric definitions.\n\n- Cadence: kickoffs, design reviews, decision meetings with options, read-outs with 'recommend / next steps / owner'.\n\n- Expectation setting: scope, assumptions, MDE/power limits, timelines; say 'no' with alternatives.\n\n- Collaboration: write clear specs (schemas, APIs), agree on data contracts, co-own monitoring.\n\n- Conflict handling: surface trade-offs, show scenario tables, log decisions; escalate with crisp evidence.\n\n- Anti-patterns: over-promising, burying the lede, jargon dumps, undocumented decisions.", "token_count": 334, "embedding_token_count": 352, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Communication &amp; Stakeholder Management"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Communication &amp; Stakeholder Management", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Communication &amp; Stakeholder Management", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.935989Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:293-304:0024:e831c9d2", "doc_id": "DOC02", "version": "20251014", "chunk_index": 24, "block_start_index": 293, "block_end_index": 304, "text": "- Artifacts: 1-pager PRD/DRD, experiment pre-reg, weekly updates, launch checklist, post-mortems; living metric definitions. - Cadence: kickoffs, design reviews, decision meetings with options, read-outs with 'recommend / next steps / owner'. - Expectation setting: scope, assumptions, MDE/power limits, timelines; say 'no' with alternatives. - Collaboration: write clear specs (schemas, APIs), agree on data contracts, co-own monitoring. - Conflict handling: surface trade-offs, show scenario tables, log decisions; escalate with crisp evidence. - Anti-patterns: over-promising, burying the lede, jargon dumps, undocumented decisions.\n\nModern analytics lives on cloud warehouses and lakehouses. Warehouses (Snowflake, BigQuery, Redshift) prioritize ANSI SQL, elastic compute, and governance; lakehouses (Databricks + Delta, Apache Iceberg/Hudi on S3/GCS/ADLS) unify data lakes with ACID tables and open formats for ML at scale.\n\n- What they're best at\n\n- o Warehouses: fast SQL, concurrency, fine-grained RBAC, simple ops, reliable BI.\n\n- o Lakehouse: cheap storage, open formats (Parquet + Delta/Iceberg), ML-friendly files, streaming + batch unification.\n\n- Selection criteria\n\n- o Data size &amp; variety (semi-structured JSON/Avro), latency needs, cost model (on-demand vs slots/warehouses), governance (RBAC, data masking), interoperability with ML tools.\n\n- Performance &amp; cost hygiene\n\n- o Column pruning, clustering/partitioning, materialized views, result caching, avoid SELECT * , compressed columnar formats, storage lifecycle policies.\n\n- Reliability &amp; quality\n\n- o Data contracts , schema evolution, SCD handling, time-travel/versioned tables, CDC ingestion (Debezium/Fivetran).\n\n- Pitfalls\n\n- o Mixed grains in KPIs, unbounded costs from ad-hoc scans, unmanaged external tables without ACID.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Data Platforms (Snowflake / BigQuery / Redshift; Lakehouse)\n\n- Artifacts: 1-pager PRD/DRD, experiment pre-reg, weekly updates, launch checklist, post-mortems; living metric definitions. - Cadence: kickoffs, design reviews, decision meetings with options, read-outs with 'recommend / next steps / owner'. - Expectation setting: scope, assumptions, MDE/power limits, timelines; say 'no' with alternatives. - Collaboration: write clear specs (schemas, APIs), agree on data contracts, co-own monitoring. - Conflict handling: surface trade-offs, show scenario tables, log decisions; escalate with crisp evidence. - Anti-patterns: over-promising, burying the lede, jargon dumps, undocumented decisions.\n\nModern analytics lives on cloud warehouses and lakehouses. Warehouses (Snowflake, BigQuery, Redshift) prioritize ANSI SQL, elastic compute, and governance; lakehouses (Databricks + Delta, Apache Iceberg/Hudi on S3/GCS/ADLS) unify data lakes with ACID tables and open formats for ML at scale.\n\n- What they're best at\n\n- o Warehouses: fast SQL, concurrency, fine-grained RBAC, simple ops, reliable BI.\n\n- o Lakehouse: cheap storage, open formats (Parquet + Delta/Iceberg), ML-friendly files, streaming + batch unification.\n\n- Selection criteria\n\n- o Data size &amp; variety (semi-structured JSON/Avro), latency needs, cost model (on-demand vs slots/warehouses), governance (RBAC, data masking), interoperability with ML tools.\n\n- Performance &amp; cost hygiene\n\n- o Column pruning, clustering/partitioning, materialized views, result caching, avoid SELECT * , compressed columnar formats, storage lifecycle policies.\n\n- Reliability &amp; quality\n\n- o Data contracts , schema evolution, SCD handling, time-travel/versioned tables, CDC ingestion (Debezium/Fivetran).\n\n- Pitfalls\n\n- o Mixed grains in KPIs, unbounded costs from ad-hoc scans, unmanaged external tables without ACID.", "token_count": 388, "embedding_token_count": 411, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Data Platforms (Snowflake / BigQuery / Redshift; Lakehouse)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Data Platforms (Snowflake / BigQuery / Redshift; Lakehouse)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Data Platforms (Snowflake / BigQuery / Redshift; Lakehouse)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.936311Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:306-316:0025:9db3e449", "doc_id": "DOC02", "version": "20251014", "chunk_index": 25, "block_start_index": 306, "block_end_index": 316, "text": "size &amp; variety (semi-structured JSON/Avro), latency needs, cost model (on-demand vs slots/warehouses), governance (RBAC, data masking), interoperability with ML tools. - Performance &amp; cost hygiene - o Column pruning, clustering/partitioning, materialized views, result caching, avoid SELECT * , compressed columnar formats, storage lifecycle policies. - Reliability &amp; quality - o Data contracts , schema evolution, SCD handling, time-travel/versioned tables, CDC ingestion (Debezium/Fivetran). - Pitfalls - o Mixed grains in KPIs, unbounded costs from ad-hoc scans, unmanaged external tables without ACID.\n\nOrchestration schedules reliable pipelines; transformation frameworks make SQL changes testable and modular.\n\n- Roles\n\n- o Airflow (or Prefect/Dagster): DAG orchestration, retries, SLAs, backfills, dependencies.\n\n- o dbt: SQL-first transformations with tests , documentation , macros , and environments .\n\n- Best practices\n\n- o Separate extract/load from transform ; small idempotent tasks; explicit data dependencies; parameterize by date.\n\n- o In dbt: source freshness tests, unique/not-null tests, staging → marts layers, CI on pull requests, docs site.\n\n- Ops &amp; observability\n\n- o Task-level retries, alerting on SLAs/failures, lineage graphs, data quality gates pre-publish.\n\n- Pitfalls\n\n- o Spaghetti DAGs, hidden side effects, long monolithic SQL models, lack of backfills/version pins.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Orchestration &amp; Transformation (Airflow, dbt)\n\nsize &amp; variety (semi-structured JSON/Avro), latency needs, cost model (on-demand vs slots/warehouses), governance (RBAC, data masking), interoperability with ML tools. - Performance &amp; cost hygiene - o Column pruning, clustering/partitioning, materialized views, result caching, avoid SELECT * , compressed columnar formats, storage lifecycle policies. - Reliability &amp; quality - o Data contracts , schema evolution, SCD handling, time-travel/versioned tables, CDC ingestion (Debezium/Fivetran). - Pitfalls - o Mixed grains in KPIs, unbounded costs from ad-hoc scans, unmanaged external tables without ACID.\n\nOrchestration schedules reliable pipelines; transformation frameworks make SQL changes testable and modular.\n\n- Roles\n\n- o Airflow (or Prefect/Dagster): DAG orchestration, retries, SLAs, backfills, dependencies.\n\n- o dbt: SQL-first transformations with tests , documentation , macros , and environments .\n\n- Best practices\n\n- o Separate extract/load from transform ; small idempotent tasks; explicit data dependencies; parameterize by date.\n\n- o In dbt: source freshness tests, unique/not-null tests, staging → marts layers, CI on pull requests, docs site.\n\n- Ops &amp; observability\n\n- o Task-level retries, alerting on SLAs/failures, lineage graphs, data quality gates pre-publish.\n\n- Pitfalls\n\n- o Spaghetti DAGs, hidden side effects, long monolithic SQL models, lack of backfills/version pins.", "token_count": 291, "embedding_token_count": 313, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Orchestration &amp; Transformation (Airflow, dbt)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Orchestration &amp; Transformation (Airflow, dbt)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Orchestration &amp; Transformation (Airflow, dbt)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.936593Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:318-326:0026:6026ae46", "doc_id": "DOC02", "version": "20251014", "chunk_index": 26, "block_start_index": 318, "block_end_index": 326, "text": ", documentation , macros , and environments . - Best practices - o Separate extract/load from transform ; small idempotent tasks; explicit data dependencies; parameterize by date. - o In dbt: source freshness tests, unique/not-null tests, staging → marts layers, CI on pull requests, docs site. - Ops &amp; observability - o Task-level retries, alerting on SLAs/failures, lineage graphs, data quality gates pre-publish. - Pitfalls - o Spaghetti DAGs, hidden side effects, long monolithic SQL models, lack of backfills/version pins.\n\nNotebooks excel at EDA and narration; IDEs shine for production code, tests, and refactors. Use both deliberately.\n\n- Working pattern\n\n- o Start in Jupyter/JupyterLab for exploration; migrate stable code into VS Code modules with tests.\n\n- Reproducibility\n\n- o Pin environments, nbstripout outputs, parameterize with papermill/jupytext , set random seeds, cache dataset snapshots.\n\n- Productivity\n\n- o VS Code: remote dev containers, debugger, notebooks editor, type checking, test runner; Jupyter: rich widgets/plots.\n\n- Pitfalls\n\n- o Hidden state, out-of-order execution, large data in cells, notebooks as the only artifact.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Notebooks / IDEs (Jupyter, VS Code)\n\n, documentation , macros , and environments . - Best practices - o Separate extract/load from transform ; small idempotent tasks; explicit data dependencies; parameterize by date. - o In dbt: source freshness tests, unique/not-null tests, staging → marts layers, CI on pull requests, docs site. - Ops &amp; observability - o Task-level retries, alerting on SLAs/failures, lineage graphs, data quality gates pre-publish. - Pitfalls - o Spaghetti DAGs, hidden side effects, long monolithic SQL models, lack of backfills/version pins.\n\nNotebooks excel at EDA and narration; IDEs shine for production code, tests, and refactors. Use both deliberately.\n\n- Working pattern\n\n- o Start in Jupyter/JupyterLab for exploration; migrate stable code into VS Code modules with tests.\n\n- Reproducibility\n\n- o Pin environments, nbstripout outputs, parameterize with papermill/jupytext , set random seeds, cache dataset snapshots.\n\n- Productivity\n\n- o VS Code: remote dev containers, debugger, notebooks editor, type checking, test runner; Jupyter: rich widgets/plots.\n\n- Pitfalls\n\n- o Hidden state, out-of-order execution, large data in cells, notebooks as the only artifact.", "token_count": 236, "embedding_token_count": 257, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Notebooks / IDEs (Jupyter, VS Code)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Notebooks / IDEs (Jupyter, VS Code)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Notebooks / IDEs (Jupyter, VS Code)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.936827Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:328-340:0027:a196426e", "doc_id": "DOC02", "version": "20251014", "chunk_index": 27, "block_start_index": 328, "block_end_index": 340, "text": "code, tests, and refactors. Use both deliberately. - Working pattern - o Start in Jupyter/JupyterLab for exploration; migrate stable code into VS Code modules with tests. - Reproducibility - o Pin environments, nbstripout outputs, parameterize with papermill/jupytext , set random seeds, cache dataset snapshots. - Productivity - o VS Code: remote dev containers, debugger, notebooks editor, type checking, test runner; Jupyter: rich widgets/plots. - Pitfalls - o Hidden state, out-of-order execution, large data in cells, notebooks as the only artifact.\n\nPick the simplest library that meets accuracy/latency needs; prefer tabular SOTA (boosting) before deep nets unless representation learning is required.\n\n- Tabular &amp; classical\n\n- o scikit-learn: pipelines, metrics, model selection; baselines (logreg, RF).\n\n- o XGBoost/LightGBM/CatBoost: strong tabular performance; handle nonlinearity &amp; interactions.\n\n- Deep learning\n\n- o PyTorch/TensorFlow/Keras: images/text/sequence models, custom architectures, GPU acceleration.\n\n- o Ecosystem: Hugging Face (transformers/datasets/eval), timm, torchvision.\n\n- Training &amp; tuning\n\n- o Cross-validation, early stopping, class weights, Optuna/Bayes optimization, calibration (Platt/isotonic).\n\n- Serving\n\n- o Batch scoring vs real-time (ONNX/TensorRT), quantization/pruning, AOT compilers (TorchScript, XLA).\n\n- Pitfalls\n\n- o Leakage in feature engineering, chasing deep models on small tabular data, ignoring calibration/uncertainty.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > ML Libraries (scikit-learn, XGBoost/LightGBM, PyTorch/TensorFlow)\n\ncode, tests, and refactors. Use both deliberately. - Working pattern - o Start in Jupyter/JupyterLab for exploration; migrate stable code into VS Code modules with tests. - Reproducibility - o Pin environments, nbstripout outputs, parameterize with papermill/jupytext , set random seeds, cache dataset snapshots. - Productivity - o VS Code: remote dev containers, debugger, notebooks editor, type checking, test runner; Jupyter: rich widgets/plots. - Pitfalls - o Hidden state, out-of-order execution, large data in cells, notebooks as the only artifact.\n\nPick the simplest library that meets accuracy/latency needs; prefer tabular SOTA (boosting) before deep nets unless representation learning is required.\n\n- Tabular &amp; classical\n\n- o scikit-learn: pipelines, metrics, model selection; baselines (logreg, RF).\n\n- o XGBoost/LightGBM/CatBoost: strong tabular performance; handle nonlinearity &amp; interactions.\n\n- Deep learning\n\n- o PyTorch/TensorFlow/Keras: images/text/sequence models, custom architectures, GPU acceleration.\n\n- o Ecosystem: Hugging Face (transformers/datasets/eval), timm, torchvision.\n\n- Training &amp; tuning\n\n- o Cross-validation, early stopping, class weights, Optuna/Bayes optimization, calibration (Platt/isotonic).\n\n- Serving\n\n- o Batch scoring vs real-time (ONNX/TensorRT), quantization/pruning, AOT compilers (TorchScript, XLA).\n\n- Pitfalls\n\n- o Leakage in feature engineering, chasing deep models on small tabular data, ignoring calibration/uncertainty.", "token_count": 305, "embedding_token_count": 332, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "ML Libraries (scikit-learn, XGBoost/LightGBM, PyTorch/TensorFlow)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > ML Libraries (scikit-learn, XGBoost/LightGBM, PyTorch/TensorFlow)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|ML Libraries (scikit-learn, XGBoost/LightGBM, PyTorch/TensorFlow)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.937080Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:342-352:0028:c3723ab6", "doc_id": "DOC02", "version": "20251014", "chunk_index": 28, "block_start_index": 342, "block_end_index": 352, "text": "selection; baselines (logreg, RF). - o XGBoost/LightGBM/CatBoost: strong tabular performance; handle nonlinearity &amp; interactions. - Deep learning - o PyTorch/TensorFlow/Keras: images/text/sequence models, custom architectures, GPU acceleration. - o Ecosystem: Hugging Face (transformers/datasets/eval), timm, torchvision. - Training &amp; tuning - o Cross-validation, early stopping, class weights, Optuna/Bayes optimization, calibration (Platt/isotonic). - Serving - o Batch scoring vs real-time (ONNX/TensorRT), quantization/pruning, AOT compilers (TorchScript, XLA). - Pitfalls - o Leakage in feature engineering, chasing deep models on small tabular data, ignoring calibration/uncertainty.\n\nBI surfaces 'what is happening'; experiment platforms validate 'what works.' Both need trustworthy metrics and governance.\n\n- BI stacks\n\n- o Tableau/Power BI/Looker/Superset for dashboards, semantic layers, row-level security, scheduled extracts.\n\n- o Practices: certified dashboards, single source of truth for metric definitions, usage monitoring.\n\n- Experimentation\n\n- o Optimizely/Statsig/LaunchDarkly/GrowthBook or in-house: randomization, CUPED, sequential tests, guardrails.\n\n- o Checks: power/MDE, sample ratio mismatch (SRM), interference/spillover, pre-registration of hypotheses.\n\n- Communication\n\n- o Scorecards: treatment effects, CIs, cohort cuts, decision recommendations, rollback criteria.\n\n- Pitfalls\n\n- o Metric shopping, unmanaged KPI proliferation, running too many concurrent tests on same population.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > BI &amp; Experiment Tools (Tableau / Power BI; experiment platforms)\n\nselection; baselines (logreg, RF). - o XGBoost/LightGBM/CatBoost: strong tabular performance; handle nonlinearity &amp; interactions. - Deep learning - o PyTorch/TensorFlow/Keras: images/text/sequence models, custom architectures, GPU acceleration. - o Ecosystem: Hugging Face (transformers/datasets/eval), timm, torchvision. - Training &amp; tuning - o Cross-validation, early stopping, class weights, Optuna/Bayes optimization, calibration (Platt/isotonic). - Serving - o Batch scoring vs real-time (ONNX/TensorRT), quantization/pruning, AOT compilers (TorchScript, XLA). - Pitfalls - o Leakage in feature engineering, chasing deep models on small tabular data, ignoring calibration/uncertainty.\n\nBI surfaces 'what is happening'; experiment platforms validate 'what works.' Both need trustworthy metrics and governance.\n\n- BI stacks\n\n- o Tableau/Power BI/Looker/Superset for dashboards, semantic layers, row-level security, scheduled extracts.\n\n- o Practices: certified dashboards, single source of truth for metric definitions, usage monitoring.\n\n- Experimentation\n\n- o Optimizely/Statsig/LaunchDarkly/GrowthBook or in-house: randomization, CUPED, sequential tests, guardrails.\n\n- o Checks: power/MDE, sample ratio mismatch (SRM), interference/spillover, pre-registration of hypotheses.\n\n- Communication\n\n- o Scorecards: treatment effects, CIs, cohort cuts, decision recommendations, rollback criteria.\n\n- Pitfalls\n\n- o Metric shopping, unmanaged KPI proliferation, running too many concurrent tests on same population.", "token_count": 308, "embedding_token_count": 335, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "BI &amp; Experiment Tools (Tableau / Power BI; experiment platforms)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > BI &amp; Experiment Tools (Tableau / Power BI; experiment platforms)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|BI &amp; Experiment Tools (Tableau / Power BI; experiment platforms)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.937345Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:354-365:0029:518abe72", "doc_id": "DOC02", "version": "20251014", "chunk_index": 29, "block_start_index": 354, "block_end_index": 365, "text": "o Tableau/Power BI/Looker/Superset for dashboards, semantic layers, row-level security, scheduled extracts. - o Practices: certified dashboards, single source of truth for metric definitions, usage monitoring. - Experimentation - o Optimizely/Statsig/LaunchDarkly/GrowthBook or in-house: randomization, CUPED, sequential tests, guardrails. - o Checks: power/MDE, sample ratio mismatch (SRM), interference/spillover, pre-registration of hypotheses. - Communication - o Scorecards: treatment effects, CIs, cohort cuts, decision recommendations, rollback criteria. - Pitfalls - o Metric shopping, unmanaged KPI proliferation, running too many concurrent tests on same population.\n\nCloud gives elastic compute/storage; containers/Kubernetes provide portability and repeatable deployment.\n\n- Core building blocks\n\n- o Compute: EC2/GCE/VMSS, serverless (Lambda/Cloud Functions), batch; GPU (A100/H100) for DL.\n\n- o Storage: S3/GCS/ADLS (data lake), EBS/PD (block), EFS/Filestore (shared).\n\n- o Networking &amp; security: VPCs, private subnets, security groups, IAM least privilege, KMS/CMK.\n\n- o Containers: Docker + Kubernetes (EKS/GKE/AKS) for scaling model APIs and jobs.\n\n- Ops &amp; cost\n\n- o Autoscaling, spot/preemptible for training, node pools by workload, cost tags/budgets, artifact registries.\n\n- MLOps services\n\n- o Managed notebooks, model endpoints (SageMaker/Vertex/AML), pipelines, feature stores, monitoring.\n\n- Pitfalls\n\n- o Secrets in code, no egress controls, over-provisioned GPUs, missing resource limits/requests on K8s.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Cloud &amp; Infra (AWS / GCP / Azure; containers)\n\no Tableau/Power BI/Looker/Superset for dashboards, semantic layers, row-level security, scheduled extracts. - o Practices: certified dashboards, single source of truth for metric definitions, usage monitoring. - Experimentation - o Optimizely/Statsig/LaunchDarkly/GrowthBook or in-house: randomization, CUPED, sequential tests, guardrails. - o Checks: power/MDE, sample ratio mismatch (SRM), interference/spillover, pre-registration of hypotheses. - Communication - o Scorecards: treatment effects, CIs, cohort cuts, decision recommendations, rollback criteria. - Pitfalls - o Metric shopping, unmanaged KPI proliferation, running too many concurrent tests on same population.\n\nCloud gives elastic compute/storage; containers/Kubernetes provide portability and repeatable deployment.\n\n- Core building blocks\n\n- o Compute: EC2/GCE/VMSS, serverless (Lambda/Cloud Functions), batch; GPU (A100/H100) for DL.\n\n- o Storage: S3/GCS/ADLS (data lake), EBS/PD (block), EFS/Filestore (shared).\n\n- o Networking &amp; security: VPCs, private subnets, security groups, IAM least privilege, KMS/CMK.\n\n- o Containers: Docker + Kubernetes (EKS/GKE/AKS) for scaling model APIs and jobs.\n\n- Ops &amp; cost\n\n- o Autoscaling, spot/preemptible for training, node pools by workload, cost tags/budgets, artifact registries.\n\n- MLOps services\n\n- o Managed notebooks, model endpoints (SageMaker/Vertex/AML), pipelines, feature stores, monitoring.\n\n- Pitfalls\n\n- o Secrets in code, no egress controls, over-provisioned GPUs, missing resource limits/requests on K8s.", "token_count": 332, "embedding_token_count": 358, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Cloud &amp; Infra (AWS / GCP / Azure; containers)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Cloud &amp; Infra (AWS / GCP / Azure; containers)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Cloud &amp; Infra (AWS / GCP / Azure; containers)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.937614Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:367-377:0030:b653647c", "doc_id": "DOC02", "version": "20251014", "chunk_index": 30, "block_start_index": 367, "block_end_index": 377, "text": "EBS/PD (block), EFS/Filestore (shared). - o Networking &amp; security: VPCs, private subnets, security groups, IAM least privilege, KMS/CMK. - o Containers: Docker + Kubernetes (EKS/GKE/AKS) for scaling model APIs and jobs. - Ops &amp; cost - o Autoscaling, spot/preemptible for training, node pools by workload, cost tags/budgets, artifact registries. - MLOps services - o Managed notebooks, model endpoints (SageMaker/Vertex/AML), pipelines, feature stores, monitoring. - Pitfalls - o Secrets in code, no egress controls, over-provisioned GPUs, missing resource limits/requests on K8s.\n\nWithout tracking, you can't reproduce or govern models. Use experiment tracking + registries + feature stores for consistency.\n\n- Experiment tracking\n\n- o MLflow/W&amp;B/Vertex Experiments: log params, metrics, artifacts, datasets, code versions; compare runs.\n\n- Model registry\n\n- o Lifecycle states (Staging/Production/Archived), approval workflows, model versioning, deployable flavors (pyfunc, ONNX).\n\n- Feature stores\n\n- o Feast/Tecton/SageMaker/Vertex: define features once, serve online/offline consistently, backfills, point-in-time correctness.\n\n- Monitoring &amp; lineage\n\n- o Data drift, concept drift, label latency, feature freshness; lineage from raw sources → features → models → endpoints.\n\n- Pitfalls\n\n- o Training/serving skew, ad-hoc features in notebooks, no audit trail for promoted models.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Tracking &amp; Registry (MLflow; model/feature stores)\n\nEBS/PD (block), EFS/Filestore (shared). - o Networking &amp; security: VPCs, private subnets, security groups, IAM least privilege, KMS/CMK. - o Containers: Docker + Kubernetes (EKS/GKE/AKS) for scaling model APIs and jobs. - Ops &amp; cost - o Autoscaling, spot/preemptible for training, node pools by workload, cost tags/budgets, artifact registries. - MLOps services - o Managed notebooks, model endpoints (SageMaker/Vertex/AML), pipelines, feature stores, monitoring. - Pitfalls - o Secrets in code, no egress controls, over-provisioned GPUs, missing resource limits/requests on K8s.\n\nWithout tracking, you can't reproduce or govern models. Use experiment tracking + registries + feature stores for consistency.\n\n- Experiment tracking\n\n- o MLflow/W&amp;B/Vertex Experiments: log params, metrics, artifacts, datasets, code versions; compare runs.\n\n- Model registry\n\n- o Lifecycle states (Staging/Production/Archived), approval workflows, model versioning, deployable flavors (pyfunc, ONNX).\n\n- Feature stores\n\n- o Feast/Tecton/SageMaker/Vertex: define features once, serve online/offline consistently, backfills, point-in-time correctness.\n\n- Monitoring &amp; lineage\n\n- o Data drift, concept drift, label latency, feature freshness; lineage from raw sources → features → models → endpoints.\n\n- Pitfalls\n\n- o Training/serving skew, ad-hoc features in notebooks, no audit trail for promoted models.", "token_count": 307, "embedding_token_count": 332, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Tracking &amp; Registry (MLflow; model/feature stores)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Tracking &amp; Registry (MLflow; model/feature stores)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Tracking &amp; Registry (MLflow; model/feature stores)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.937901Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:379-389:0031:b4aa1a58", "doc_id": "DOC02", "version": "20251014", "chunk_index": 31, "block_start_index": 379, "block_end_index": 389, "text": "log params, metrics, artifacts, datasets, code versions; compare runs. - Model registry - o Lifecycle states (Staging/Production/Archived), approval workflows, model versioning, deployable flavors (pyfunc, ONNX). - Feature stores - o Feast/Tecton/SageMaker/Vertex: define features once, serve online/offline consistently, backfills, point-in-time correctness. - Monitoring &amp; lineage - o Data drift, concept drift, label latency, feature freshness; lineage from raw sources → features → models → endpoints. - Pitfalls - o Training/serving skew, ad-hoc features in notebooks, no audit trail for promoted models.\n\nVersioning code, data, and environments makes work reviewable and recoverable.\n\n- Git workflow\n\n- o Branching strategy (feature → PR → main), small PRs, code owners , protected branches, semantic commits and tags.\n\n- o CI on PRs: tests, lint, dbt build/test , data contract checks; release notes and changelogs.\n\n- Environments\n\n- o venv/conda/Poetry with pinned versions and lockfiles; separate runtime (serving) from training envs.\n\n- o Reproducible containers (Dockerfiles) with minimal bases; pre-commit hooks (black/ruff/isort).\n\n- Data &amp; artifact versioning\n\n- o DVC/lakehouse time-travel for datasets; artifact registries for models.\n\n- Pitfalls\n\n- o 'Works on my machine,' unpinned dependencies, committing notebooks with large outputs, no lock to model code version.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Version Control &amp; Environments (Git, venv/conda)\n\nlog params, metrics, artifacts, datasets, code versions; compare runs. - Model registry - o Lifecycle states (Staging/Production/Archived), approval workflows, model versioning, deployable flavors (pyfunc, ONNX). - Feature stores - o Feast/Tecton/SageMaker/Vertex: define features once, serve online/offline consistently, backfills, point-in-time correctness. - Monitoring &amp; lineage - o Data drift, concept drift, label latency, feature freshness; lineage from raw sources → features → models → endpoints. - Pitfalls - o Training/serving skew, ad-hoc features in notebooks, no audit trail for promoted models.\n\nVersioning code, data, and environments makes work reviewable and recoverable.\n\n- Git workflow\n\n- o Branching strategy (feature → PR → main), small PRs, code owners , protected branches, semantic commits and tags.\n\n- o CI on PRs: tests, lint, dbt build/test , data contract checks; release notes and changelogs.\n\n- Environments\n\n- o venv/conda/Poetry with pinned versions and lockfiles; separate runtime (serving) from training envs.\n\n- o Reproducible containers (Dockerfiles) with minimal bases; pre-commit hooks (black/ruff/isort).\n\n- Data &amp; artifact versioning\n\n- o DVC/lakehouse time-travel for datasets; artifact registries for models.\n\n- Pitfalls\n\n- o 'Works on my machine,' unpinned dependencies, committing notebooks with large outputs, no lock to model code version.", "token_count": 293, "embedding_token_count": 318, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Version Control &amp; Environments (Git, venv/conda)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Version Control &amp; Environments (Git, venv/conda)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Version Control &amp; Environments (Git, venv/conda)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.938179Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:392-404:0032:f8837f9d", "doc_id": "DOC02", "version": "20251014", "chunk_index": 32, "block_start_index": 392, "block_end_index": 404, "text": "- o CI on PRs: tests, lint, dbt build/test , data contract checks; release notes and changelogs. - Environments - o venv/conda/Poetry with pinned versions and lockfiles; separate runtime (serving) from training envs. - o Reproducible containers (Dockerfiles) with minimal bases; pre-commit hooks (black/ruff/isort). - Data &amp; artifact versioning - o DVC/lakehouse time-travel for datasets; artifact registries for models. - Pitfalls - o 'Works on my machine,' unpinned dependencies, committing notebooks with large outputs, no lock to model code version.\n\nA good EDA clarifies what the data can and cannot support before modeling. It surfaces leakage, label latency, missingness patterns, and cohort biases; it also states whether the dataset is decision-grade.\n\n- What to include\n\n- o Context: purpose, target decision/KPI, unit of analysis, time window.\n\n- o Schema profile: fields, types, cardinality, allowed ranges, PII flags.\n\n- o Target audit: base rates, drift over time, label latency, leakage risks.\n\n- o Missingness &amp; anomalies: MCAR/MAR/MNAR hypotheses, outliers, duplicates.\n\n- o Cohorts &amp; bias: performance-critical slices (geo, product line, device), prevalence by slice.\n\n- o Data lineage &amp; freshness: source systems, ingestion lag, backfills, known quirks.\n\n- o Decision note: 'Fit for purpose' verdict + gaps and mitigation plan.\n\n- Acceptance criteria\n\n- o Reproducible notebook/script; summary one-pager with charts and a go/no-go statement.\n\n- Common pitfalls\n\n- o Mixed grains, silent de-duplication, using post-outcome fields (leakage), ignoring seasonality.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > EDA &amp; Data Quality reports\n\n- o CI on PRs: tests, lint, dbt build/test , data contract checks; release notes and changelogs. - Environments - o venv/conda/Poetry with pinned versions and lockfiles; separate runtime (serving) from training envs. - o Reproducible containers (Dockerfiles) with minimal bases; pre-commit hooks (black/ruff/isort). - Data &amp; artifact versioning - o DVC/lakehouse time-travel for datasets; artifact registries for models. - Pitfalls - o 'Works on my machine,' unpinned dependencies, committing notebooks with large outputs, no lock to model code version.\n\nA good EDA clarifies what the data can and cannot support before modeling. It surfaces leakage, label latency, missingness patterns, and cohort biases; it also states whether the dataset is decision-grade.\n\n- What to include\n\n- o Context: purpose, target decision/KPI, unit of analysis, time window.\n\n- o Schema profile: fields, types, cardinality, allowed ranges, PII flags.\n\n- o Target audit: base rates, drift over time, label latency, leakage risks.\n\n- o Missingness &amp; anomalies: MCAR/MAR/MNAR hypotheses, outliers, duplicates.\n\n- o Cohorts &amp; bias: performance-critical slices (geo, product line, device), prevalence by slice.\n\n- o Data lineage &amp; freshness: source systems, ingestion lag, backfills, known quirks.\n\n- o Decision note: 'Fit for purpose' verdict + gaps and mitigation plan.\n\n- Acceptance criteria\n\n- o Reproducible notebook/script; summary one-pager with charts and a go/no-go statement.\n\n- Common pitfalls\n\n- o Mixed grains, silent de-duplication, using post-outcome fields (leakage), ignoring seasonality.", "token_count": 348, "embedding_token_count": 367, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "EDA &amp; Data Quality reports"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > EDA &amp; Data Quality reports", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|EDA &amp; Data Quality reports", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.938466Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:406-416:0033:59e42780", "doc_id": "DOC02", "version": "20251014", "chunk_index": 33, "block_start_index": 406, "block_end_index": 416, "text": "risks. - o Missingness &amp; anomalies: MCAR/MAR/MNAR hypotheses, outliers, duplicates. - o Cohorts &amp; bias: performance-critical slices (geo, product line, device), prevalence by slice. - o Data lineage &amp; freshness: source systems, ingestion lag, backfills, known quirks. - o Decision note: 'Fit for purpose' verdict + gaps and mitigation plan. - Acceptance criteria - o Reproducible notebook/script; summary one-pager with charts and a go/no-go statement. - Common pitfalls - o Mixed grains, silent de-duplication, using post-outcome fields (leakage), ignoring seasonality.\n\nFeature work turns raw tables into stable, error-tolerant signals. A dataset/feature contract encodes definitions so upstream changes don't silently break models.\n\n- What to include\n\n- o Feature spec: name, business meaning, SQL/py pseudocode, allowed nulls, range, timezone, unit.\n\n- o Point-in-time correctness: time joins, label windows, leakage checks.\n\n- o Training/serving parity: same logic (dbt/Feast/feature store), backfill strategy.\n\n- o Quality tests: uniqueness, referential integrity, freshness SLAs, anomaly thresholds.\n\n- o Versioning: semantic version + change log, deprecation policy.\n\n- Deliverables\n\n- o dbt models (or feature store defs), validation tests, and a rendered contract page.\n\n- Common pitfalls\n\n- o Target/mean encoding without leakage guards; recomputing features differently online vs offline.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Feature engineering &amp; dataset contracts\n\nrisks. - o Missingness &amp; anomalies: MCAR/MAR/MNAR hypotheses, outliers, duplicates. - o Cohorts &amp; bias: performance-critical slices (geo, product line, device), prevalence by slice. - o Data lineage &amp; freshness: source systems, ingestion lag, backfills, known quirks. - o Decision note: 'Fit for purpose' verdict + gaps and mitigation plan. - Acceptance criteria - o Reproducible notebook/script; summary one-pager with charts and a go/no-go statement. - Common pitfalls - o Mixed grains, silent de-duplication, using post-outcome fields (leakage), ignoring seasonality.\n\nFeature work turns raw tables into stable, error-tolerant signals. A dataset/feature contract encodes definitions so upstream changes don't silently break models.\n\n- What to include\n\n- o Feature spec: name, business meaning, SQL/py pseudocode, allowed nulls, range, timezone, unit.\n\n- o Point-in-time correctness: time joins, label windows, leakage checks.\n\n- o Training/serving parity: same logic (dbt/Feast/feature store), backfill strategy.\n\n- o Quality tests: uniqueness, referential integrity, freshness SLAs, anomaly thresholds.\n\n- o Versioning: semantic version + change log, deprecation policy.\n\n- Deliverables\n\n- o dbt models (or feature store defs), validation tests, and a rendered contract page.\n\n- Common pitfalls\n\n- o Target/mean encoding without leakage guards; recomputing features differently online vs offline.", "token_count": 301, "embedding_token_count": 320, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Feature engineering &amp; dataset contracts"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Feature engineering &amp; dataset contracts", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Feature engineering &amp; dataset contracts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.938713Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:418-430:0034:c0514248", "doc_id": "DOC02", "version": "20251014", "chunk_index": 34, "block_start_index": 418, "block_end_index": 430, "text": "pseudocode, allowed nulls, range, timezone, unit. - o Point-in-time correctness: time joins, label windows, leakage checks. - o Training/serving parity: same logic (dbt/Feast/feature store), backfill strategy. - o Quality tests: uniqueness, referential integrity, freshness SLAs, anomaly thresholds. - o Versioning: semantic version + change log, deprecation policy. - Deliverables - o dbt models (or feature store defs), validation tests, and a rendered contract page. - Common pitfalls - o Target/mean encoding without leakage guards; recomputing features differently online vs offline.\n\nThis document explains why this model is the right choice , how it was validated, and when it should be retrained or rolled back.\n\n- What to include\n\n- o Problem framing: prediction target, horizon, actionability, constraints (latency, fairness).\n\n- o Baselines &amp; contenders: simple baseline → advanced models; ablations; cost/latency table.\n\n- o Validation design: temporal CV/rolling origin; stratification; data leakage controls.\n\n- o Metrics: offline (AUC/F1/PR-AUC/RMSE), calibration (ECE/Brier), cohort breakdowns.\n\n- o Interpretability: SHAP/global vs local, limitations, stability checks.\n\n- o Risk register: failure modes, guardrails, rollback triggers; monitoring plan.\n\n- o Lifecycle: retrain cadence, data drift thresholds, owner/rotation.\n\n- Deliverables\n\n- o Readable memo + MLflow run links + model card.\n\n- Common pitfalls\n\n- o Reporting a single metric; tuning on test; ignoring calibration and cost-sensitive thresholds.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Modeling &amp; Evaluation write-ups\n\npseudocode, allowed nulls, range, timezone, unit. - o Point-in-time correctness: time joins, label windows, leakage checks. - o Training/serving parity: same logic (dbt/Feast/feature store), backfill strategy. - o Quality tests: uniqueness, referential integrity, freshness SLAs, anomaly thresholds. - o Versioning: semantic version + change log, deprecation policy. - Deliverables - o dbt models (or feature store defs), validation tests, and a rendered contract page. - Common pitfalls - o Target/mean encoding without leakage guards; recomputing features differently online vs offline.\n\nThis document explains why this model is the right choice , how it was validated, and when it should be retrained or rolled back.\n\n- What to include\n\n- o Problem framing: prediction target, horizon, actionability, constraints (latency, fairness).\n\n- o Baselines &amp; contenders: simple baseline → advanced models; ablations; cost/latency table.\n\n- o Validation design: temporal CV/rolling origin; stratification; data leakage controls.\n\n- o Metrics: offline (AUC/F1/PR-AUC/RMSE), calibration (ECE/Brier), cohort breakdowns.\n\n- o Interpretability: SHAP/global vs local, limitations, stability checks.\n\n- o Risk register: failure modes, guardrails, rollback triggers; monitoring plan.\n\n- o Lifecycle: retrain cadence, data drift thresholds, owner/rotation.\n\n- Deliverables\n\n- o Readable memo + MLflow run links + model card.\n\n- Common pitfalls\n\n- o Reporting a single metric; tuning on test; ignoring calibration and cost-sensitive thresholds.", "token_count": 317, "embedding_token_count": 337, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Modeling &amp; Evaluation write-ups"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Modeling &amp; Evaluation write-ups", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Modeling &amp; Evaluation write-ups", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.939008Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:432-442:0035:9bb5cf60", "doc_id": "DOC02", "version": "20251014", "chunk_index": 35, "block_start_index": 432, "block_end_index": 442, "text": "table. - o Validation design: temporal CV/rolling origin; stratification; data leakage controls. - o Metrics: offline (AUC/F1/PR-AUC/RMSE), calibration (ECE/Brier), cohort breakdowns. - o Interpretability: SHAP/global vs local, limitations, stability checks. - o Risk register: failure modes, guardrails, rollback triggers; monitoring plan. - o Lifecycle: retrain cadence, data drift thresholds, owner/rotation. - Deliverables - o Readable memo + MLflow run links + model card. - Common pitfalls - o Reporting a single metric; tuning on test; ignoring calibration and cost-sensitive thresholds.\n\nExperiments convert model quality into causal business impact. The artifacts prevent p-hacking and make decisions auditable.\n\n- Pre-registration (before launch)\n\n- o Hypothesis &amp; success criteria ; north-star &amp; guardrails; MDE/power ; randomization unit; segmentation plan; SRM checks; duration cap; stopping rules.\n\n- Analysis plan\n\n- o CUPED/stratification, non-parametric backups, treatment compliance handling, interference mitigation.\n\n- Readout (after)\n\n- o Effect sizes + CIs; cohort cuts; diagnostics (SRM, pre-trend); recommendation with rollout plan/rollback .\n\n- Deliverables\n\n- o One-pager scorecard + appendix notebook; link to flags/config used.\n\n- Common pitfalls\n\n- o Peeking, metric shopping, overlapping experiments, ignoring seasonality/holidays.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Experiment design, analysis, and readouts\n\ntable. - o Validation design: temporal CV/rolling origin; stratification; data leakage controls. - o Metrics: offline (AUC/F1/PR-AUC/RMSE), calibration (ECE/Brier), cohort breakdowns. - o Interpretability: SHAP/global vs local, limitations, stability checks. - o Risk register: failure modes, guardrails, rollback triggers; monitoring plan. - o Lifecycle: retrain cadence, data drift thresholds, owner/rotation. - Deliverables - o Readable memo + MLflow run links + model card. - Common pitfalls - o Reporting a single metric; tuning on test; ignoring calibration and cost-sensitive thresholds.\n\nExperiments convert model quality into causal business impact. The artifacts prevent p-hacking and make decisions auditable.\n\n- Pre-registration (before launch)\n\n- o Hypothesis &amp; success criteria ; north-star &amp; guardrails; MDE/power ; randomization unit; segmentation plan; SRM checks; duration cap; stopping rules.\n\n- Analysis plan\n\n- o CUPED/stratification, non-parametric backups, treatment compliance handling, interference mitigation.\n\n- Readout (after)\n\n- o Effect sizes + CIs; cohort cuts; diagnostics (SRM, pre-trend); recommendation with rollout plan/rollback .\n\n- Deliverables\n\n- o One-pager scorecard + appendix notebook; link to flags/config used.\n\n- Common pitfalls\n\n- o Peeking, metric shopping, overlapping experiments, ignoring seasonality/holidays.", "token_count": 281, "embedding_token_count": 300, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Experiment design, analysis, and readouts"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Experiment design, analysis, and readouts", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Experiment design, analysis, and readouts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.939239Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:444-454:0036:5da9c033", "doc_id": "DOC02", "version": "20251014", "chunk_index": 36, "block_start_index": 444, "block_end_index": 454, "text": "launch) - o Hypothesis &amp; success criteria ; north-star &amp; guardrails; MDE/power ; randomization unit; segmentation plan; SRM checks; duration cap; stopping rules. - Analysis plan - o CUPED/stratification, non-parametric backups, treatment compliance handling, interference mitigation. - Readout (after) - o Effect sizes + CIs; cohort cuts; diagnostics (SRM, pre-trend); recommendation with rollout plan/rollback . - Deliverables - o One-pager scorecard + appendix notebook; link to flags/config used. - Common pitfalls - o Peeking, metric shopping, overlapping experiments, ignoring seasonality/holidays.\n\nDashboards inform operational decisions ; metric definitions ensure everyone calculates KPIs the same way.\n\n- Metric catalog\n\n- o Name, formula, grain, filters , inclusion/exclusion rules, time zone, SLO/SLA, owner.\n\n- o Guardrails (e.g., complaint rate, latency) alongside north-star metrics.\n\n- Dashboard design\n\n- o Executive view (few KPIs + trends + annotations) → drill-downs by cohort; freshness badges and data quality warnings.\n\n- o Comparisons (WoW/MoM), cause-and-effect panels for experiments/launches.\n\n- Deliverables\n\n- o Certified dashboard with usage monitoring + living metric doc.\n\n- Common pitfalls\n\n- o KPI soup, mixed grains, no annotations for breaks/backfills, stale tiles.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Dashboards &amp; Metric definitions\n\nlaunch) - o Hypothesis &amp; success criteria ; north-star &amp; guardrails; MDE/power ; randomization unit; segmentation plan; SRM checks; duration cap; stopping rules. - Analysis plan - o CUPED/stratification, non-parametric backups, treatment compliance handling, interference mitigation. - Readout (after) - o Effect sizes + CIs; cohort cuts; diagnostics (SRM, pre-trend); recommendation with rollout plan/rollback . - Deliverables - o One-pager scorecard + appendix notebook; link to flags/config used. - Common pitfalls - o Peeking, metric shopping, overlapping experiments, ignoring seasonality/holidays.\n\nDashboards inform operational decisions ; metric definitions ensure everyone calculates KPIs the same way.\n\n- Metric catalog\n\n- o Name, formula, grain, filters , inclusion/exclusion rules, time zone, SLO/SLA, owner.\n\n- o Guardrails (e.g., complaint rate, latency) alongside north-star metrics.\n\n- Dashboard design\n\n- o Executive view (few KPIs + trends + annotations) → drill-downs by cohort; freshness badges and data quality warnings.\n\n- o Comparisons (WoW/MoM), cause-and-effect panels for experiments/launches.\n\n- Deliverables\n\n- o Certified dashboard with usage monitoring + living metric doc.\n\n- Common pitfalls\n\n- o KPI soup, mixed grains, no annotations for breaks/backfills, stale tiles.", "token_count": 273, "embedding_token_count": 291, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Dashboards &amp; Metric definitions"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Dashboards &amp; Metric definitions", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Dashboards &amp; Metric definitions", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.939508Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:456-463:0037:b01d1475", "doc_id": "DOC02", "version": "20251014", "chunk_index": 37, "block_start_index": 456, "block_end_index": 463, "text": "Name, formula, grain, filters , inclusion/exclusion rules, time zone, SLO/SLA, owner. - o Guardrails (e.g., complaint rate, latency) alongside north-star metrics. - Dashboard design - o Executive view (few KPIs + trends + annotations) → drill-downs by cohort; freshness badges and data quality warnings. - o Comparisons (WoW/MoM), cause-and-effect panels for experiments/launches. - Deliverables - o Certified dashboard with usage monitoring + living metric doc. - Common pitfalls - o KPI soup, mixed grains, no annotations for breaks/backfills, stale tiles.\n\nA short, persuasive memo that ties analysis to an explicit decision, surfaces alternatives, and quantifies risk/ROI.\n\n- Structure (4-6 pages or less)\n\n- o Context &amp; goal , options considered, evidence summary, cost/benefit table, risks &amp; mitigations, recommendation and next steps/owners.\n\n- o Scenario analysis (best/base/worst), fairness &amp; compliance impact, kill/rollback criteria.\n\n- Deliverables\n\n- o Decision Review Doc (DRD) with appendix links (EDA, model, experiment).\n\n- Common pitfalls\n\n- o Presenting results without recommending an action; hiding uncertainty; no owner or timeline.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Decision/Strategy docs (trade-offs, risks)\n\nName, formula, grain, filters , inclusion/exclusion rules, time zone, SLO/SLA, owner. - o Guardrails (e.g., complaint rate, latency) alongside north-star metrics. - Dashboard design - o Executive view (few KPIs + trends + annotations) → drill-downs by cohort; freshness badges and data quality warnings. - o Comparisons (WoW/MoM), cause-and-effect panels for experiments/launches. - Deliverables - o Certified dashboard with usage monitoring + living metric doc. - Common pitfalls - o KPI soup, mixed grains, no annotations for breaks/backfills, stale tiles.\n\nA short, persuasive memo that ties analysis to an explicit decision, surfaces alternatives, and quantifies risk/ROI.\n\n- Structure (4-6 pages or less)\n\n- o Context &amp; goal , options considered, evidence summary, cost/benefit table, risks &amp; mitigations, recommendation and next steps/owners.\n\n- o Scenario analysis (best/base/worst), fairness &amp; compliance impact, kill/rollback criteria.\n\n- Deliverables\n\n- o Decision Review Doc (DRD) with appendix links (EDA, model, experiment).\n\n- Common pitfalls\n\n- o Presenting results without recommending an action; hiding uncertainty; no owner or timeline.", "token_count": 255, "embedding_token_count": 278, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Decision/Strategy docs (trade-offs, risks)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Decision/Strategy docs (trade-offs, risks)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Decision/Strategy docs (trade-offs, risks)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.939757Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:465-474:0038:8e7b2c3e", "doc_id": "DOC02", "version": "20251014", "chunk_index": 38, "block_start_index": 465, "block_end_index": 474, "text": "memo that ties analysis to an explicit decision, surfaces alternatives, and quantifies risk/ROI. - Structure (4-6 pages or less) - o Context &amp; goal , options considered, evidence summary, cost/benefit table, risks &amp; mitigations, recommendation and next steps/owners. - o Scenario analysis (best/base/worst), fairness &amp; compliance impact, kill/rollback criteria. - Deliverables - o Decision Review Doc (DRD) with appendix links (EDA, model, experiment). - Common pitfalls - o Presenting results without recommending an action; hiding uncertainty; no owner or timeline.\n\nBridge from notebook to production. Provide a minimal, well-specified service or batch job plus clear ownership for ongoing support.\n\n- What to include\n\n- o Interface spec: request/response schema, units, error codes, timeouts, SLOs; idempotency for batch.\n\n- o Artifacts: container/Dockerfile, sample requests, load test results, dependency lockfile.\n\n- o Operational docs: monitoring signals, on-call runbook, rate limits, data retention, PII handling.\n\n- o Acceptance: shadow test or limited rollout with agreement on success gates.\n\n- Deliverables\n\n- o OpenAPI spec + reference client + canary plan; or scheduled batch with checksum reports.\n\n- Common pitfalls\n\n- o Training/serving skew, no backpressure strategy, secrets in code, undefined retry semantics.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Prototypes/APIs &amp; handoffs to engineering\n\nmemo that ties analysis to an explicit decision, surfaces alternatives, and quantifies risk/ROI. - Structure (4-6 pages or less) - o Context &amp; goal , options considered, evidence summary, cost/benefit table, risks &amp; mitigations, recommendation and next steps/owners. - o Scenario analysis (best/base/worst), fairness &amp; compliance impact, kill/rollback criteria. - Deliverables - o Decision Review Doc (DRD) with appendix links (EDA, model, experiment). - Common pitfalls - o Presenting results without recommending an action; hiding uncertainty; no owner or timeline.\n\nBridge from notebook to production. Provide a minimal, well-specified service or batch job plus clear ownership for ongoing support.\n\n- What to include\n\n- o Interface spec: request/response schema, units, error codes, timeouts, SLOs; idempotency for batch.\n\n- o Artifacts: container/Dockerfile, sample requests, load test results, dependency lockfile.\n\n- o Operational docs: monitoring signals, on-call runbook, rate limits, data retention, PII handling.\n\n- o Acceptance: shadow test or limited rollout with agreement on success gates.\n\n- Deliverables\n\n- o OpenAPI spec + reference client + canary plan; or scheduled batch with checksum reports.\n\n- Common pitfalls\n\n- o Training/serving skew, no backpressure strategy, secrets in code, undefined retry semantics.", "token_count": 273, "embedding_token_count": 294, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Prototypes/APIs &amp; handoffs to engineering"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Prototypes/APIs &amp; handoffs to engineering", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Prototypes/APIs &amp; handoffs to engineering", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.939985Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:476-486:0039:2e3d6f98", "doc_id": "DOC02", "version": "20251014", "chunk_index": 39, "block_start_index": 476, "block_end_index": 486, "text": "units, error codes, timeouts, SLOs; idempotency for batch. - o Artifacts: container/Dockerfile, sample requests, load test results, dependency lockfile. - o Operational docs: monitoring signals, on-call runbook, rate limits, data retention, PII handling. - o Acceptance: shadow test or limited rollout with agreement on success gates. - Deliverables - o OpenAPI spec + reference client + canary plan; or scheduled batch with checksum reports. - Common pitfalls - o Training/serving skew, no backpressure strategy, secrets in code, undefined retry semantics.\n\nIf you can't reproduce it, you can't trust it. Reproducibility enables audits, handovers, and safe iteration.\n\n- Practices\n\n- o Code: modular repo; pinned deps (requirements.txt/poetry.lock), CI tests, linting, type hints.\n\n- o Runs: MLflow/W&amp;B run IDs; log data checksums, feature versions, random seeds, commit SHA.\n\n- o Data: time-stamped snapshots or lakehouse time-travel; point-in-time joins; sample artifacts for reviews.\n\n- o Envs: Docker image tags; training vs serving env parity; deterministic ops where feasible.\n\n- o Docs: README with exact commands; makefile/CLI; changelog for models/features.\n\n- Deliverables\n\n- o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model).\n\n- Common pitfalls\n\n- o Hidden notebook state, unpinned versions, overwriting training data, missing seeds.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Reproducibility (code, seeds, data snapshots)\n\nunits, error codes, timeouts, SLOs; idempotency for batch. - o Artifacts: container/Dockerfile, sample requests, load test results, dependency lockfile. - o Operational docs: monitoring signals, on-call runbook, rate limits, data retention, PII handling. - o Acceptance: shadow test or limited rollout with agreement on success gates. - Deliverables - o OpenAPI spec + reference client + canary plan; or scheduled batch with checksum reports. - Common pitfalls - o Training/serving skew, no backpressure strategy, secrets in code, undefined retry semantics.\n\nIf you can't reproduce it, you can't trust it. Reproducibility enables audits, handovers, and safe iteration.\n\n- Practices\n\n- o Code: modular repo; pinned deps (requirements.txt/poetry.lock), CI tests, linting, type hints.\n\n- o Runs: MLflow/W&amp;B run IDs; log data checksums, feature versions, random seeds, commit SHA.\n\n- o Data: time-stamped snapshots or lakehouse time-travel; point-in-time joins; sample artifacts for reviews.\n\n- o Envs: Docker image tags; training vs serving env parity; deterministic ops where feasible.\n\n- o Docs: README with exact commands; makefile/CLI; changelog for models/features.\n\n- Deliverables\n\n- o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model).\n\n- Common pitfalls\n\n- o Hidden notebook state, unpinned versions, overwriting training data, missing seeds.", "token_count": 299, "embedding_token_count": 320, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Reproducibility (code, seeds, data snapshots)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Reproducibility (code, seeds, data snapshots)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Reproducibility (code, seeds, data snapshots)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.940225Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:489-489:0040:4e5b2e68", "doc_id": "DOC02", "version": "20251014", "chunk_index": 40, "block_start_index": 489, "block_end_index": 489, "text": "log data checksums, feature versions, random seeds, commit SHA. - o Data: time-stamped snapshots or lakehouse time-travel; point-in-time joins; sample artifacts for reviews. - o Envs: Docker image tags; training vs serving env parity; deterministic ops where feasible. - o Docs: README with exact commands; makefile/CLI; changelog for models/features. - Deliverables - o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model). - Common pitfalls - o Hidden notebook state, unpinned versions, overwriting training data, missing seeds.\n\nA solid data-science hire blends statistical rigor, product sense, and production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Minimum vs preferred qualifications\n\nlog data checksums, feature versions, random seeds, commit SHA. - o Data: time-stamped snapshots or lakehouse time-travel; point-in-time joins; sample artifacts for reviews. - o Envs: Docker image tags; training vs serving env parity; deterministic ops where feasible. - o Docs: README with exact commands; makefile/CLI; changelog for models/features. - Deliverables - o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model). - Common pitfalls - o Hidden notebook state, unpinned versions, overwriting training data, missing seeds.\n\nA solid data-science hire blends statistical rigor, product sense, and production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact.", "token_count": 163, "embedding_token_count": 179, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Minimum vs preferred qualifications"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Minimum vs preferred qualifications", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Minimum vs preferred qualifications", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.940384Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:491-495:0041:832753ca", "doc_id": "DOC02", "version": "20251014", "chunk_index": 41, "block_start_index": 491, "block_end_index": 495, "text": "Envs: Docker image tags; training vs serving env parity; deterministic ops where feasible. - o Docs: README with exact commands; makefile/CLI; changelog for models/features. - Deliverables - o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model). - Common pitfalls - o Hidden notebook state, unpinned versions, overwriting training data, missing seeds. A solid data-science hire blends statistical rigor, product sense, and production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact.\n\n- Foundations: probability/inference basics; supervised learning (linear/logistic, trees/boosting), validation, and metrics.\n\n- Core tooling: SQL (joins, window functions), Python (pandas/numpy/sklearn), notebooks → scripts.\n\n- Data literacy: tidy data, leakage awareness, time-based splits, cohorting, missing-data strategies.\n\n- Communication: can explain a model and its business purpose in clear, non-jargon language.\n\n- Evidence: at least one reproducible project or internship showing EDA → model → evaluation.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > What 'minimum' usually means (must-have):\n\nEnvs: Docker image tags; training vs serving env parity; deterministic ops where feasible. - o Docs: README with exact commands; makefile/CLI; changelog for models/features. - Deliverables - o 'Reproduce in 1 command' script; audit bundle (code, config, data slice, model). - Common pitfalls - o Hidden notebook state, unpinned versions, overwriting training data, missing seeds. A solid data-science hire blends statistical rigor, product sense, and production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact.\n\n- Foundations: probability/inference basics; supervised learning (linear/logistic, trees/boosting), validation, and metrics.\n\n- Core tooling: SQL (joins, window functions), Python (pandas/numpy/sklearn), notebooks → scripts.\n\n- Data literacy: tidy data, leakage awareness, time-based splits, cohorting, missing-data strategies.\n\n- Communication: can explain a model and its business purpose in clear, non-jargon language.\n\n- Evidence: at least one reproducible project or internship showing EDA → model → evaluation.", "token_count": 230, "embedding_token_count": 254, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "What 'minimum' usually means (must-have):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > What 'minimum' usually means (must-have):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|What 'minimum' usually means (must-have):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.940613Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:497-502:0042:b631bab0", "doc_id": "DOC02", "version": "20251014", "chunk_index": 42, "block_start_index": 497, "block_end_index": 502, "text": "and production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact. - Foundations: probability/inference basics; supervised learning (linear/logistic, trees/boosting), validation, and metrics. - Core tooling: SQL (joins, window functions), Python (pandas/numpy/sklearn), notebooks → scripts. - Data literacy: tidy data, leakage awareness, time-based splits, cohorting, missing-data strategies. - Communication: can explain a model and its business purpose in clear, non-jargon language. - Evidence: at least one reproducible project or internship showing EDA → model → evaluation.\n\n- Advanced methods: causal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty.\n\n- MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime).\n\n- Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs.\n\n- Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers.\n\n- Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos.\n\n'", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > What 'preferred' looks like (nice-to-have / bar-raising):\n\nand production-minded coding. 'Minimum' ensures they can contribute safely; 'preferred' signals they can lead end-to-end impact. - Foundations: probability/inference basics; supervised learning (linear/logistic, trees/boosting), validation, and metrics. - Core tooling: SQL (joins, window functions), Python (pandas/numpy/sklearn), notebooks → scripts. - Data literacy: tidy data, leakage awareness, time-based splits, cohorting, missing-data strategies. - Communication: can explain a model and its business purpose in clear, non-jargon language. - Evidence: at least one reproducible project or internship showing EDA → model → evaluation.\n\n- Advanced methods: causal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty.\n\n- MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime).\n\n- Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs.\n\n- Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers.\n\n- Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos.\n\n'", "token_count": 243, "embedding_token_count": 273, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "What 'preferred' looks like (nice-to-have / bar-raising):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > What 'preferred' looks like (nice-to-have / bar-raising):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|What 'preferred' looks like (nice-to-have / bar-raising):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.940852Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:504-504:0043:5aec3064", "doc_id": "DOC02", "version": "20251014", "chunk_index": 43, "block_start_index": 504, "block_end_index": 504, "text": "purpose in clear, non-jargon language. - Evidence: at least one reproducible project or internship showing EDA → model → evaluation. - Advanced methods: causal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty. - MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime). - Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs. - Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers. - Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos. '\n\nPortfolios should show problems that matter, decisions driven, and code that others can run. Curate 3-4 flagship pieces rather than many half-finished ones.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Portfolio expectations (repos, case studies, notebooks)\n\npurpose in clear, non-jargon language. - Evidence: at least one reproducible project or internship showing EDA → model → evaluation. - Advanced methods: causal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty. - MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime). - Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs. - Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers. - Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos. '\n\nPortfolios should show problems that matter, decisions driven, and code that others can run. Curate 3-4 flagship pieces rather than many half-finished ones.", "token_count": 163, "embedding_token_count": 185, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Portfolio expectations (repos, case studies, notebooks)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Portfolio expectations (repos, case studies, notebooks)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Portfolio expectations (repos, case studies, notebooks)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941025Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:506-510:0044:4f5bae9f", "doc_id": "DOC02", "version": "20251014", "chunk_index": 44, "block_start_index": 506, "block_end_index": 510, "text": "causal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty. - MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime). - Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs. - Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers. - Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos. ' Portfolios should show problems that matter, decisions driven, and code that others can run. Curate 3-4 flagship pieces rather than many half-finished ones.\n\n- Readable README: problem statement, data source, decision/KPI, how to run (make run or one command), and results.\n\n- Reproducibility: requirements.txt / pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs.\n\n- End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis.\n\n- Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks.\n\n- Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > What great portfolios include:\n\ncausal inference/experimentation, recsys, NLP/CV, or forecasting depth; calibration and uncertainty. - MLOps awareness: CI tests, experiment tracking, model registry, basic deployment patterns (batch/realtime). - Product thinking: can translate metrics into ROI; proposes guardrails; designs A/Bs. - Scale &amp; reliability: dbt or similar, Airflow/Prefect, cloud (AWS/GCP/Azure), containers. - Leadership signals: mentoring, scoping ambiguous problems, cross-functional decision memos. ' Portfolios should show problems that matter, decisions driven, and code that others can run. Curate 3-4 flagship pieces rather than many half-finished ones.\n\n- Readable README: problem statement, data source, decision/KPI, how to run (make run or one command), and results.\n\n- Reproducibility: requirements.txt / pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs.\n\n- End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis.\n\n- Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks.\n\n- Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec.", "token_count": 259, "embedding_token_count": 276, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "What great portfolios include:"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > What great portfolios include:", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|What great portfolios include:", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941278Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:512-512:0045:a5476403", "doc_id": "DOC02", "version": "20251014", "chunk_index": 45, "block_start_index": 512, "block_end_index": 512, "text": "ones. - Readable README: problem statement, data source, decision/KPI, how to run (make run or one command), and results. - Reproducibility: requirements.txt / pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs. - End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec.\n\n- Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Case-study template (1-2 pages each):\n\nones. - Readable README: problem statement, data source, decision/KPI, how to run (make run or one command), and results. - Reproducibility: requirements.txt / pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs. - End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec.\n\n- Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps.", "token_count": 161, "embedding_token_count": 185, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Case-study template (1-2 pages each):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Case-study template (1-2 pages each):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Case-study template (1-2 pages each):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941451Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:514-514:0046:7cf7d81f", "doc_id": "DOC02", "version": "20251014", "chunk_index": 46, "block_start_index": 514, "block_end_index": 514, "text": "/ pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs. - End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps.\n\n- Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Common misses:\n\n/ pyproject.toml, seed control, small sample dataset, MLflow/W&amp;B links or logs. - End-to-end story: EDA → features (with a brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps.\n\n- Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value.", "token_count": 149, "embedding_token_count": 164, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Common misses:"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Common misses:", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Common misses:", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941609Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:516-516:0047:94e41a17", "doc_id": "DOC02", "version": "20251014", "chunk_index": 47, "block_start_index": 516, "block_end_index": 516, "text": "a brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value.\n\nExpect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Interview stages (screen → SQL/Python → ML/case → system design → behavioral)\n\na brief contract ) → model baselines → validation design → calibration → error analysis. - Decision framing: a short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value.\n\nExpect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own.", "token_count": 134, "embedding_token_count": 164, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Interview stages (screen → SQL/Python → ML/case → system design → behavioral)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Interview stages (screen → SQL/Python → ML/case → system design → behavioral)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Interview stages (screen → SQL/Python → ML/case → system design → behavioral)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941767Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:518-518:0048:fa541b3e", "doc_id": "DOC02", "version": "20251014", "chunk_index": 48, "block_start_index": 518, "block_end_index": 518, "text": "short 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own.\n\n- 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > 1) Recruiter/phone screen (15-30 min):\n\nshort 'so what' section: suggested threshold/policy, projected impact, risks/rollbacks. - Live artifact: Streamlit/Gradio demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own.\n\n- 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact.", "token_count": 136, "embedding_token_count": 161, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "1) Recruiter/phone screen (15-30 min):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > 1) Recruiter/phone screen (15-30 min):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|1) Recruiter/phone screen (15-30 min):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.941919Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:520-521:0049:a2e3a9a2", "doc_id": "DOC02", "version": "20251014", "chunk_index": 49, "block_start_index": 520, "block_end_index": 521, "text": "demo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own. - 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact.\n\n- SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain.\n\n- Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection).", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > 2) SQL/Python practical (45-60 min):\n\ndemo or a REST stub + OpenAPI spec. - Context &amp; hypothesis → Data &amp; caveats → Methods &amp; baselines → Results (with CIs) → Business recommendation → Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own. - 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact.\n\n- SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain.\n\n- Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection).", "token_count": 160, "embedding_token_count": 185, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "2) SQL/Python practical (45-60 min):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > 2) SQL/Python practical (45-60 min):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|2) SQL/Python practical (45-60 min):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942089Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:523-524:0050:d2519eec", "doc_id": "DOC02", "version": "20251014", "chunk_index": 50, "block_start_index": 523, "block_end_index": 524, "text": "Limitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own. - 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact. - SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain. - Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection).\n\n- Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy.\n\n- Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > 3) ML/case interview (60 min):\n\nLimitations/next steps. - Hidden notebook state; no baseline; single metric; no leakage checks; unreproducible env; no explanation of business value. Expect a funnel testing signals from breadth to depth to collaboration. Each stage should be passable on its own. - 2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact. - SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain. - Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection).\n\n- Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy.\n\n- Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan.", "token_count": 189, "embedding_token_count": 212, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "3) ML/case interview (60 min):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > 3) ML/case interview (60 min):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|3) ML/case interview (60 min):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942257Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:526-527:0051:59c310ce", "doc_id": "DOC02", "version": "20251014", "chunk_index": 51, "block_start_index": 526, "block_end_index": 527, "text": "2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact. - SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain. - Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection). - Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan.\n\n- Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence.\n\n- Call out feature stores, training/serving skew, canary/rollback, privacy and access controls.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > 4) ML system design (45-60 min):\n\n2-minute project pitch, clarity of role fit, compensation/logistics, high-level signals of impact. - SQL: multi-table joins, window functions, deduping, cohorting, sessionization, correctness at the right grain. - Python: data wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection). - Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan.\n\n- Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence.\n\n- Call out feature stores, training/serving skew, canary/rollback, privacy and access controls.", "token_count": 188, "embedding_token_count": 212, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "4) ML system design (45-60 min):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > 4) ML system design (45-60 min):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|4) ML system design (45-60 min):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942436Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:529-530:0052:7d16000a", "doc_id": "DOC02", "version": "20251014", "chunk_index": 52, "block_start_index": 529, "block_end_index": 530, "text": "wrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection). - Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls.\n\n- STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy.\n\n- Look for ownership, clarity, reflection, and stakeholder empathy.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > 5) Behavioral (45 min):\n\nwrangling, writing a small function/class, tests, time/space reasoning (not Big-O perfection). - Frame a real product problem (e.g., churn prediction): define target/horizon, leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls.\n\n- STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy.\n\n- Look for ownership, clarity, reflection, and stakeholder empathy.", "token_count": 173, "embedding_token_count": 193, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "5) Behavioral (45 min):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > 5) Behavioral (45 min):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|5) Behavioral (45 min):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942574Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:532-532:0053:44d33c4e", "doc_id": "DOC02", "version": "20251014", "chunk_index": 53, "block_start_index": 532, "block_end_index": 532, "text": "leakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy.\n\nBoth formats can be fair if scoped tightly and graded with a rubric.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Take-home vs live exercises (rubrics, timelines)\n\nleakage risks, metrics, baselines, validation, calibration, and how to select a threshold or policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy.\n\nBoth formats can be fair if scoped tightly and graded with a rubric.", "token_count": 143, "embedding_token_count": 166, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Take-home vs live exercises (rubrics, timelines)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Take-home vs live exercises (rubrics, timelines)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Take-home vs live exercises (rubrics, timelines)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942704Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:534-536:0054:3bb2c593", "doc_id": "DOC02", "version": "20251014", "chunk_index": 54, "block_start_index": 534, "block_end_index": 536, "text": "policy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy. Both formats can be fair if scoped tightly and graded with a rubric.\n\n- Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code.\n\n- Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene.\n\n- Good signs: measured scope (one model + baseline), explicit evaluation, simple demo.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Take-home (typ. 4-8 hrs, 3-7 days window):\n\npolicy. - Expect trade-offs (AUC vs PR-AUC, cost weighting), error analysis, and a quick experiment plan. - Design training → registry → deployment (batch/real-time) → monitoring (data/Concept drift, latency, cost) → retrain cadence. - Call out feature stores, training/serving skew, canary/rollback, privacy and access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy. Both formats can be fair if scoped tightly and graded with a rubric.\n\n- Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code.\n\n- Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene.\n\n- Good signs: measured scope (one model + baseline), explicit evaluation, simple demo.", "token_count": 186, "embedding_token_count": 216, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Take-home (typ. 4-8 hrs, 3-7 days window):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Take-home (typ. 4-8 hrs, 3-7 days window):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Take-home (typ. 4-8 hrs, 3-7 days window):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942859Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:538-539:0055:0aec88f1", "doc_id": "DOC02", "version": "20251014", "chunk_index": 55, "block_start_index": 538, "block_end_index": 539, "text": "and access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy. Both formats can be fair if scoped tightly and graded with a rubric. - Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo.\n\n- Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process.\n\n- Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Live exercises:\n\nand access controls. - STAR format on conflict, influencing without authority, moving a metric, handling failures/ethics/privacy. - Look for ownership, clarity, reflection, and stakeholder empathy. Both formats can be fair if scoped tightly and graded with a rubric. - Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo.\n\n- Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process.\n\n- Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving.", "token_count": 149, "embedding_token_count": 164, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Live exercises:"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Live exercises:", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Live exercises:", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.942970Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:541-541:0056:1c78d35d", "doc_id": "DOC02", "version": "20251014", "chunk_index": 56, "block_start_index": 541, "block_end_index": 541, "text": "fair if scoped tightly and graded with a rubric. - Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo. - Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process. - Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving.\n\n- Vague objectives, unrealistic data sizes/time limits, evaluating on style over substance, penalizing candidates for not using the interviewer's favorite library.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Anti-patterns (for both):\n\nfair if scoped tightly and graded with a rubric. - Brief: clearly stated goal, dataset, constraints, deliverables; require a decision memo + runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo. - Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process. - Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving.\n\n- Vague objectives, unrealistic data sizes/time limits, evaluating on style over substance, penalizing candidates for not using the interviewer's favorite library.", "token_count": 136, "embedding_token_count": 156, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Anti-patterns (for both):"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Anti-patterns (for both):", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Anti-patterns (for both):", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943075Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:543-553:0057:10a5288c", "doc_id": "DOC02", "version": "20251014", "chunk_index": 57, "block_start_index": 543, "block_end_index": 553, "text": "+ runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo. - Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process. - Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving. - Vague objectives, unrealistic data sizes/time limits, evaluating on style over substance, penalizing candidates for not using the interviewer's favorite library.\n\nDefine the bar before interviewing and stick to it. A sample scoring split (adjust as needed):\n\n- Technical correctness (30%)\n\n- o Sound stats/ML, proper validation (temporal CV where needed), leakage avoided, calibrated outputs when decisions require.\n\n- Problem framing &amp; impact (25%)\n\n- o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails.\n\n- Data &amp; SQL (15%)\n\n- o Correct joins, windowing, grain control, performance awareness, tidy outputs.\n\n- Code quality &amp; reproducibility (15%)\n\n- o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs.\n\n- Communication &amp; collaboration (15%)\n\n- o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Evaluation criteria (correctness, clarity, impact thinking)\n\n+ runnable code. - Rubric: correctness, validation design, clarity of reasoning, reproducibility, business recommendation, and code hygiene. - Good signs: measured scope (one model + baseline), explicit evaluation, simple demo. - Pairing style: work on SQL or a modeling vignette together; interviewer probes thought process. - Rubric: communication under time pressure, ability to ask clarifying questions, incremental problem solving. - Vague objectives, unrealistic data sizes/time limits, evaluating on style over substance, penalizing candidates for not using the interviewer's favorite library.\n\nDefine the bar before interviewing and stick to it. A sample scoring split (adjust as needed):\n\n- Technical correctness (30%)\n\n- o Sound stats/ML, proper validation (temporal CV where needed), leakage avoided, calibrated outputs when decisions require.\n\n- Problem framing &amp; impact (25%)\n\n- o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails.\n\n- Data &amp; SQL (15%)\n\n- o Correct joins, windowing, grain control, performance awareness, tidy outputs.\n\n- Code quality &amp; reproducibility (15%)\n\n- o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs.\n\n- Communication &amp; collaboration (15%)\n\n- o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback.", "token_count": 278, "embedding_token_count": 300, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Evaluation criteria (correctness, clarity, impact thinking)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Evaluation criteria (correctness, clarity, impact thinking)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Evaluation criteria (correctness, clarity, impact thinking)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943256Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:555-555:0058:d7266ffb", "doc_id": "DOC02", "version": "20251014", "chunk_index": 58, "block_start_index": 555, "block_end_index": 555, "text": "(temporal CV where needed), leakage avoided, calibrated outputs when decisions require. - Problem framing &amp; impact (25%) - o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails. - Data &amp; SQL (15%) - o Correct joins, windowing, grain control, performance awareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback.\n\n- Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Bonus signals:\n\n(temporal CV where needed), leakage avoided, calibrated outputs when decisions require. - Problem framing &amp; impact (25%) - o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails. - Data &amp; SQL (15%) - o Correct joins, windowing, grain control, performance awareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback.\n\n- Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty.", "token_count": 147, "embedding_token_count": 162, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Bonus signals:"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Bonus signals:", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Bonus signals:", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943371Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:558-558:0059:5db95fcf", "doc_id": "DOC02", "version": "20251014", "chunk_index": 59, "block_start_index": 558, "block_end_index": 558, "text": "require. - Problem framing &amp; impact (25%) - o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails. - Data &amp; SQL (15%) - o Correct joins, windowing, grain control, performance awareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty.\n\nProgression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others .", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Level expectations (Junior → Senior → Staff/Principal)\n\nrequire. - Problem framing &amp; impact (25%) - o Clear target/KPI, thoughtful thresholds, experiment plan, cost/benefit reasoning, risks &amp; guardrails. - Data &amp; SQL (15%) - o Correct joins, windowing, grain control, performance awareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty.\n\nProgression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others .", "token_count": 172, "embedding_token_count": 195, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Level expectations (Junior → Senior → Staff/Principal)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Level expectations (Junior → Senior → Staff/Principal)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Level expectations (Junior → Senior → Staff/Principal)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943514Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:560-562:0060:1f98f4b4", "doc_id": "DOC02", "version": "20251014", "chunk_index": 60, "block_start_index": 560, "block_end_index": 562, "text": "awareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty. Progression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others .\n\n- o Executes well-defined tasks: clean datasets, build baselines, run evaluations.\n\n- o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV).\n\n- o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Junior / Associate (entry-~2 years)\n\nawareness, tidy outputs. - Code quality &amp; reproducibility (15%) - o Structure, tests, env pinning, docs, deterministic seeds, small clean PRs. - Communication &amp; collaboration (15%) - o Clear narrative, stakeholder alignment, trade-off articulation, receptive to feedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty. Progression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others .\n\n- o Executes well-defined tasks: clean datasets, build baselines, run evaluations.\n\n- o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV).\n\n- o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes.", "token_count": 166, "embedding_token_count": 189, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Junior / Associate (entry-~2 years)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Junior / Associate (entry-~2 years)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Junior / Associate (entry-~2 years)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943645Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:564-566:0061:85f518d9", "doc_id": "DOC02", "version": "20251014", "chunk_index": 61, "block_start_index": 564, "block_end_index": 566, "text": "feedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty. Progression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others . - o Executes well-defined tasks: clean datasets, build baselines, run evaluations. - o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV). - o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes.\n\n- o Owns a feature/model end-to-end under a PM/tech lead's umbrella.\n\n- o Anticipates edge cases, writes reproducible code, proposes reasonable baselines &amp; metrics.\n\n- o Collaborates smoothly with Eng/PM; can run a small A/B.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Mid-Level (solid individual contributor)\n\nfeedback. - Sensitivity analyses, thoughtful error slicing, fairness considerations, measuring uncertainty. Progression reflects increasing independence, scope, and risk ownership . Titles vary, but the capability curve is consistent: from executing clearly scoped tasks to setting technical direction and de-risking ambiguous problems for others . - o Executes well-defined tasks: clean datasets, build baselines, run evaluations. - o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV). - o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes.\n\n- o Owns a feature/model end-to-end under a PM/tech lead's umbrella.\n\n- o Anticipates edge cases, writes reproducible code, proposes reasonable baselines &amp; metrics.\n\n- o Collaborates smoothly with Eng/PM; can run a small A/B.", "token_count": 164, "embedding_token_count": 185, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Mid-Level (solid individual contributor)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Mid-Level (solid individual contributor)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Mid-Level (solid individual contributor)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943772Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:568-574:0062:e8774ad9", "doc_id": "DOC02", "version": "20251014", "chunk_index": 62, "block_start_index": 568, "block_end_index": 574, "text": "technical direction and de-risking ambiguous problems for others . - o Executes well-defined tasks: clean datasets, build baselines, run evaluations. - o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV). - o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes. - o Owns a feature/model end-to-end under a PM/tech lead's umbrella. - o Anticipates edge cases, writes reproducible code, proposes reasonable baselines &amp; metrics. - o Collaborates smoothly with Eng/PM; can run a small A/B.\n\n- o Frames problems with stakeholders; chooses approach, metrics, experiment design.\n\n- o Multimodel comparisons, calibration, monitoring plans; mentors juniors.\n\n- o Predictably ships business impact; drafts decision docs and rollout/rollback plans.\n\n- Staff (org-level problem solver)\n\n- o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs).\n\n- o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction.\n\n- o Mentors seniors; coordinates multi-team initiatives.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Senior\n\ntechnical direction and de-risking ambiguous problems for others . - o Executes well-defined tasks: clean datasets, build baselines, run evaluations. - o Needs guidance to frame problems and avoid pitfalls (leakage, invalid CV). - o Learning focus: strong SQL/EDA, validation patterns, writing crisp analysis notes. - o Owns a feature/model end-to-end under a PM/tech lead's umbrella. - o Anticipates edge cases, writes reproducible code, proposes reasonable baselines &amp; metrics. - o Collaborates smoothly with Eng/PM; can run a small A/B.\n\n- o Frames problems with stakeholders; chooses approach, metrics, experiment design.\n\n- o Multimodel comparisons, calibration, monitoring plans; mentors juniors.\n\n- o Predictably ships business impact; drafts decision docs and rollout/rollback plans.\n\n- Staff (org-level problem solver)\n\n- o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs).\n\n- o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction.\n\n- o Mentors seniors; coordinates multi-team initiatives.", "token_count": 227, "embedding_token_count": 241, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Senior"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Senior", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Senior", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.943928Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:576-578:0063:3869ee51", "doc_id": "DOC02", "version": "20251014", "chunk_index": 63, "block_start_index": 576, "block_end_index": 578, "text": "Collaborates smoothly with Eng/PM; can run a small A/B. - o Frames problems with stakeholders; chooses approach, metrics, experiment design. - o Multimodel comparisons, calibration, monitoring plans; mentors juniors. - o Predictably ships business impact; drafts decision docs and rollout/rollback plans. - Staff (org-level problem solver) - o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs). - o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction. - o Mentors seniors; coordinates multi-team initiatives.\n\n- o Sets long-horizon strategy (6-24 months): methodology, platforms, cross-org investments.\n\n- o Creates widely adopted patterns; represents the company externally; shapes hiring bar.\n\n- o Solves 'no known playbook' problems, often with applied research depth.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Principal / Distinguished\n\nCollaborates smoothly with Eng/PM; can run a small A/B. - o Frames problems with stakeholders; chooses approach, metrics, experiment design. - o Multimodel comparisons, calibration, monitoring plans; mentors juniors. - o Predictably ships business impact; drafts decision docs and rollout/rollback plans. - Staff (org-level problem solver) - o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs). - o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction. - o Mentors seniors; coordinates multi-team initiatives.\n\n- o Sets long-horizon strategy (6-24 months): methodology, platforms, cross-org investments.\n\n- o Creates widely adopted patterns; represents the company externally; shapes hiring bar.\n\n- o Solves 'no known playbook' problems, often with applied research depth.", "token_count": 177, "embedding_token_count": 193, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Principal / Distinguished"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Principal / Distinguished", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Principal / Distinguished", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944059Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:580-584:0064:95553363", "doc_id": "DOC02", "version": "20251014", "chunk_index": 64, "block_start_index": 580, "block_end_index": 584, "text": "decision docs and rollout/rollback plans. - Staff (org-level problem solver) - o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs). - o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction. - o Mentors seniors; coordinates multi-team initiatives. - o Sets long-horizon strategy (6-24 months): methodology, platforms, cross-org investments. - o Creates widely adopted patterns; represents the company externally; shapes hiring bar. - o Solves 'no known playbook' problems, often with applied research depth.\n\nAs you rise, scope widens from a project to a portfolio/product area to organizational concerns; autonomy tracks your ability to manage risk and trade-offs.\n\n- Project scope (Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing.\n\n- Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails.\n\n- Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps.\n\n- Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Scope &amp; autonomy by level (project → product area → org)\n\ndecision docs and rollout/rollback plans. - Staff (org-level problem solver) - o Tackles high-ambiguity problems across teams; creates standards (e.g., feature contracts, KPI catalogs). - o De-risks complex launches; drives technical roadmaps; influences headcount and platform direction. - o Mentors seniors; coordinates multi-team initiatives. - o Sets long-horizon strategy (6-24 months): methodology, platforms, cross-org investments. - o Creates widely adopted patterns; represents the company externally; shapes hiring bar. - o Solves 'no known playbook' problems, often with applied research depth.\n\nAs you rise, scope widens from a project to a portfolio/product area to organizational concerns; autonomy tracks your ability to manage risk and trade-offs.\n\n- Project scope (Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing.\n\n- Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails.\n\n- Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps.\n\n- Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation.", "token_count": 259, "embedding_token_count": 286, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Scope &amp; autonomy by level (project → product area → org)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Scope &amp; autonomy by level (project → product area → org)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Scope &amp; autonomy by level (project → product area → org)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944256Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:586-586:0065:ea81eb48", "doc_id": "DOC02", "version": "20251014", "chunk_index": 65, "block_start_index": 586, "block_end_index": 586, "text": "project to a portfolio/product area to organizational concerns; autonomy tracks your ability to manage risk and trade-offs. - Project scope (Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing. - Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails. - Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation.\n\nTwo equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process .", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > IC vs Manager tracks (people leadership vs deep IC)\n\nproject to a portfolio/product area to organizational concerns; autonomy tracks your ability to manage risk and trade-offs. - Project scope (Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing. - Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails. - Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation.\n\nTwo equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process .", "token_count": 148, "embedding_token_count": 171, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "IC vs Manager tracks (people leadership vs deep IC)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > IC vs Manager tracks (people leadership vs deep IC)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|IC vs Manager tracks (people leadership vs deep IC)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944389Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:588-590:0066:8550f6e9", "doc_id": "DOC02", "version": "20251014", "chunk_index": 66, "block_start_index": 588, "block_end_index": 590, "text": "(Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing. - Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails. - Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation. Two equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process .\n\n- o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence.\n\n- o Mentors and unblocks others; no formal performance management.\n\n- o Ideal if you enjoy building, teaching through code/docs, and setting technical standards.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · IC (Senior/Staff/Principal)\n\n(Junior/Mid): a single model or dashboard with clear acceptance criteria; relies on lead for framing. - Product-area scope (Senior): multiple related models/metrics; balances impact vs complexity; defines success metrics &amp; guardrails. - Org scope (Staff+): platforms, standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation. Two equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process .\n\n- o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence.\n\n- o Mentors and unblocks others; no formal performance management.\n\n- o Ideal if you enjoy building, teaching through code/docs, and setting technical standards.", "token_count": 173, "embedding_token_count": 194, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· IC (Senior/Staff/Principal)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · IC (Senior/Staff/Principal)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· IC (Senior/Staff/Principal)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944522Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:592-594:0067:ae504df0", "doc_id": "DOC02", "version": "20251014", "chunk_index": 67, "block_start_index": 592, "block_end_index": 594, "text": "standards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation. Two equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process . - o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence. - o Mentors and unblocks others; no formal performance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards.\n\n- o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery.\n\n- o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets.\n\n- o Ideal if you like coaching, portfolio management, and cross-functional negotiation.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Manager (DS Manager → Sr. Manager → Director/Head)\n\nstandards, or multi-team efforts; resolves cross-team dependencies; sets roadmaps. - Autonomy signals: reduces manager load; proactively raises risks; quantifies trade-offs; aligns peers without escalation. Two equally senior ladders. ICs lead via technical direction and execution ; managers lead through people, outcomes, and process . - o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence. - o Mentors and unblocks others; no formal performance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards.\n\n- o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery.\n\n- o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets.\n\n- o Ideal if you like coaching, portfolio management, and cross-functional negotiation.", "token_count": 169, "embedding_token_count": 195, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Manager (DS Manager → Sr. Manager → Director/Head)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Manager (DS Manager → Sr. Manager → Director/Head)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Manager (DS Manager → Sr. Manager → Director/Head)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944664Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:596-597:0068:e545b6d3", "doc_id": "DOC02", "version": "20251014", "chunk_index": 68, "block_start_index": 596, "block_end_index": 597, "text": "; managers lead through people, outcomes, and process . - o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence. - o Mentors and unblocks others; no formal performance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards. - o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation.\n\n- o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf.\n\n- o Mgr → IC: desire for depth, complex design work, less interest in performance cycles.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Switching considerations\n\n; managers lead through people, outcomes, and process . - o Deep technical ownership, design reviews, de-risking plans, cross-team technical influence. - o Mentors and unblocks others; no formal performance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards. - o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation.\n\n- o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf.\n\n- o Mgr → IC: desire for depth, complex design work, less interest in performance cycles.", "token_count": 157, "embedding_token_count": 172, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Switching considerations"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Switching considerations", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Switching considerations", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944794Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:599-599:0069:465afbc4", "doc_id": "DOC02", "version": "20251014", "chunk_index": 69, "block_start_index": 599, "block_end_index": 599, "text": "performance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards. - o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation. - o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf. - o Mgr → IC: desire for depth, complex design work, less interest in performance cycles.\n\nComp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > Comp components (base, bonus, equity) &amp; drivers (region, stage, sector)\n\nperformance management. - o Ideal if you enjoy building, teaching through code/docs, and setting technical standards. - o Owns hiring, growth, performance, prioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation. - o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf. - o Mgr → IC: desire for depth, complex design work, less interest in performance cycles.\n\nComp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles.", "token_count": 152, "embedding_token_count": 184, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "Comp components (base, bonus, equity) &amp; drivers (region, stage, sector)"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > Comp components (base, bonus, equity) &amp; drivers (region, stage, sector)", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|Comp components (base, bonus, equity) &amp; drivers (region, stage, sector)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.944928Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:601-604:0070:e4ab4f81", "doc_id": "DOC02", "version": "20251014", "chunk_index": 70, "block_start_index": 601, "block_end_index": 604, "text": "prioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation. - o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf. - o Mgr → IC: desire for depth, complex design work, less interest in performance cycles. Comp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles.\n\n- o Base: predictable monthly cash; banded by level/location.\n\n- o Bonus: % of base; tied to company + individual performance.\n\n- o Equity: options (strike price, 4-yr vest, 1-yr cliff) or RSUs (vest on schedule/liquidity).\n\n- o Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS).", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Components\n\nprioritization, stakeholder alignment, and delivery. - o Builds teams &amp; processes (on-call, QA, experiment review), manages budgets. - o Ideal if you like coaching, portfolio management, and cross-functional negotiation. - o IC → Mgr: proven mentorship, reliable project delivery, appetite for hiring/perf. - o Mgr → IC: desire for depth, complex design work, less interest in performance cycles. Comp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles.\n\n- o Base: predictable monthly cash; banded by level/location.\n\n- o Bonus: % of base; tied to company + individual performance.\n\n- o Equity: options (strike price, 4-yr vest, 1-yr cliff) or RSUs (vest on schedule/liquidity).\n\n- o Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS).", "token_count": 205, "embedding_token_count": 219, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Components"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Components", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Components", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.945080Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:606-609:0071:613f6934", "doc_id": "DOC02", "version": "20251014", "chunk_index": 71, "block_start_index": 606, "block_end_index": 609, "text": "complex design work, less interest in performance cycles. Comp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles. - o Base: predictable monthly cash; banded by level/location. - o Bonus: % of base; tied to company + individual performance. - o Equity: options (strike price, 4-yr vest, 1-yr cliff) or RSUs (vest on schedule/liquidity). - o Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS).\n\n- o Region: US/EU metros pay &gt; emerging markets; fully-remote often aligns to geo bands.\n\n- o Company stage: late-stage/FAANG-ish = higher cash + RSUs; early-stage = lower cash + higher options (higher variance).\n\n- o Sector: finance/quant, ads, and AI platform infra pay premiums; non-profit/public sector lower but stable.\n\n- o Skill scarcity: recsys, causal inference at scale, LLM safety/eval, or strong MLOps can command premiums.", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Key drivers\n\ncomplex design work, less interest in performance cycles. Comp is a bundle: base salary (cash), bonus (performance), equity (ownership), sometimes sign-on and benefits . Market levels move with macro cycles. - o Base: predictable monthly cash; banded by level/location. - o Bonus: % of base; tied to company + individual performance. - o Equity: options (strike price, 4-yr vest, 1-yr cliff) or RSUs (vest on schedule/liquidity). - o Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS).\n\n- o Region: US/EU metros pay &gt; emerging markets; fully-remote often aligns to geo bands.\n\n- o Company stage: late-stage/FAANG-ish = higher cash + RSUs; early-stage = lower cash + higher options (higher variance).\n\n- o Sector: finance/quant, ads, and AI platform infra pay premiums; non-profit/public sector lower but stable.\n\n- o Skill scarcity: recsys, causal inference at scale, LLM safety/eval, or strong MLOps can command premiums.", "token_count": 236, "embedding_token_count": 251, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Key drivers"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Key drivers", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Key drivers", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.945239Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC02:20251014:611-613:0072:d3654094", "doc_id": "DOC02", "version": "20251014", "chunk_index": 72, "block_start_index": 611, "block_end_index": 613, "text": "o Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS). - o Region: US/EU metros pay &gt; emerging markets; fully-remote often aligns to geo bands. - o Company stage: late-stage/FAANG-ish = higher cash + RSUs; early-stage = lower cash + higher options (higher variance). - o Sector: finance/quant, ads, and AI platform infra pay premiums; non-profit/public sector lower but stable. - o Skill scarcity: recsys, causal inference at scale, LLM safety/eval, or strong MLOps can command premiums.\n\n- o Options need an exit to realize value; RSUs are more liquid at public companies.\n\n- o Refreshers typically start at Senior+; ask about refresher cadence and promotion equity.\n\n(Avoid quoting hard numbers in docs you'll reuse; bands fluctuate by quarter and geography.)", "embedding_text": "1) Role Definition &amp; Scope - Data Scientist > · Equity nuances\n\no Extras: sign-on, relocation, education, WFH stipend, on-call pay (less common for DS). - o Region: US/EU metros pay &gt; emerging markets; fully-remote often aligns to geo bands. - o Company stage: late-stage/FAANG-ish = higher cash + RSUs; early-stage = lower cash + higher options (higher variance). - o Sector: finance/quant, ads, and AI platform infra pay premiums; non-profit/public sector lower but stable. - o Skill scarcity: recsys, causal inference at scale, LLM safety/eval, or strong MLOps can command premiums.\n\n- o Options need an exit to realize value; RSUs are more liquid at public companies.\n\n- o Refreshers typically start at Senior+; ask about refresher cadence and promotion equity.\n\n(Avoid quoting hard numbers in docs you'll reuse; bands fluctuate by quarter and geography.)", "token_count": 187, "embedding_token_count": 202, "section_path": ["1) Role Definition &amp; Scope - Data Scientist", "· Equity nuances"], "breadcrumb": "1) Role Definition &amp; Scope - Data Scientist > · Equity nuances", "section_group_id": "1) Role Definition &amp; Scope - Data Scientist|· Equity nuances", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.945379Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
