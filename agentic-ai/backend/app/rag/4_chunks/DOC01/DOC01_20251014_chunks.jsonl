{"chunk_id": "DOC01:20251014:1-1:0000:dd6a4539", "doc_id": "DOC01", "version": "20251014", "chunk_index": 0, "block_start_index": 1, "block_end_index": 1, "text": "Data Science (DS) is the disciplined process of turning raw data into useful knowledge, predictions, and decisions. It blends statistics , computing , and domain expertise to discover patterns, build models, and deliver measurable impact (dashboards, forecasts, automated decisions).", "embedding_text": "Definition &amp; scope (what DS is / isn't)\n\nData Science (DS) is the disciplined process of turning raw data into useful knowledge, predictions, and decisions. It blends statistics , computing , and domain expertise to discover patterns, build models, and deliver measurable impact (dashboards, forecasts, automated decisions).", "token_count": 51, "embedding_token_count": 65, "section_path": ["Definition &amp; scope (what DS is / isn't)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't)", "section_group_id": "Definition &amp; scope (what DS is / isn't)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.804212Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}}
{"chunk_id": "DOC01:20251014:3-9:0001:bc8afbed", "doc_id": "DOC01", "version": "20251014", "chunk_index": 1, "block_start_index": 3, "block_end_index": 9, "text": "- o A problem-solving practice grounded in data, experimentation, and evidence.\n\n- o End-to-end: from collecting/cleaning data → analysis → modeling → communicating results → deploying and monitoring solutions.\n\n- o Output can be insights (reports, dashboards) or systems (recommendation APIs, fraud detectors).\n\n- What DS isn't\n\n- o Not just coding or math in isolation; context and decision impact matter.\n\n- o Not 'AI for AI's sake'-the goal is business/user value , not fancy algorithms.\n\n- o Not a one-off report; good DS plans for maintenance and monitoring .", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · What DS is\n\n- o A problem-solving practice grounded in data, experimentation, and evidence.\n\n- o End-to-end: from collecting/cleaning data → analysis → modeling → communicating results → deploying and monitoring solutions.\n\n- o Output can be insights (reports, dashboards) or systems (recommendation APIs, fraud detectors).\n\n- What DS isn't\n\n- o Not just coding or math in isolation; context and decision impact matter.\n\n- o Not 'AI for AI's sake'-the goal is business/user value , not fancy algorithms.\n\n- o Not a one-off report; good DS plans for maintenance and monitoring .", "token_count": 126, "embedding_token_count": 145, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· What DS is"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · What DS is", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· What DS is", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.804463Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}}
{"chunk_id": "DOC01:20251014:11-17:0002:431c94cd", "doc_id": "DOC01", "version": "20251014", "chunk_index": 2, "block_start_index": 11, "block_end_index": 17, "text": "evidence. - o End-to-end: from collecting/cleaning data → analysis → modeling → communicating results → deploying and monitoring solutions. - o Output can be insights (reports, dashboards) or systems (recommendation APIs, fraud detectors). - What DS isn't - o Not just coding or math in isolation; context and decision impact matter. - o Not 'AI for AI's sake'-the goal is business/user value , not fancy algorithms. - o Not a one-off report; good DS plans for maintenance and monitoring .\n\nData Science evolved by layering new computation and data availability on top of classic statistics.\n\n- Pre-1990s: Statistical inference (sampling, regression) in academia &amp; industry.\n\n- 1990s-2000s: Machine Learning (SVMs, ensembles) + data warehousing; web logs at scale.\n\n- 2010s: 'Big Data' (Hadoop/Spark), cloud computing; deep learning breakthroughs (CNNs for vision, RNN/LSTMs for sequence).\n\n- Late 2010s-early 2020s: MLOps maturation; model monitoring, data versioning, model registries.\n\n- 2023+ (GenAI era): Foundation models/transformers (LLMs, multimodal) enable powerful generation and reasoning ; retrieval-augmented systems; governance/regulation ramp up.\n\nWhy it matters: each wave lowered friction-cheaper storage/compute, better algorithms-expanding what's feasible for organizations.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Short history &amp; key milestones (stats → ML → big data → deep learning → GenAI)\n\nevidence. - o End-to-end: from collecting/cleaning data → analysis → modeling → communicating results → deploying and monitoring solutions. - o Output can be insights (reports, dashboards) or systems (recommendation APIs, fraud detectors). - What DS isn't - o Not just coding or math in isolation; context and decision impact matter. - o Not 'AI for AI's sake'-the goal is business/user value , not fancy algorithms. - o Not a one-off report; good DS plans for maintenance and monitoring .\n\nData Science evolved by layering new computation and data availability on top of classic statistics.\n\n- Pre-1990s: Statistical inference (sampling, regression) in academia &amp; industry.\n\n- 1990s-2000s: Machine Learning (SVMs, ensembles) + data warehousing; web logs at scale.\n\n- 2010s: 'Big Data' (Hadoop/Spark), cloud computing; deep learning breakthroughs (CNNs for vision, RNN/LSTMs for sequence).\n\n- Late 2010s-early 2020s: MLOps maturation; model monitoring, data versioning, model registries.\n\n- 2023+ (GenAI era): Foundation models/transformers (LLMs, multimodal) enable powerful generation and reasoning ; retrieval-augmented systems; governance/regulation ramp up.\n\nWhy it matters: each wave lowered friction-cheaper storage/compute, better algorithms-expanding what's feasible for organizations.", "token_count": 277, "embedding_token_count": 312, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Short history &amp; key milestones (stats → ML → big data → deep learning → GenAI)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Short history &amp; key milestones (stats → ML → big data → deep learning → GenAI)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Short history &amp; key milestones (stats → ML → big data → deep learning → GenAI)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.804779Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:19-32:0003:1b3868bf", "doc_id": "DOC01", "version": "20251014", "chunk_index": 3, "block_start_index": 19, "block_end_index": 32, "text": "(sampling, regression) in academia &amp; industry. - 1990s-2000s: Machine Learning (SVMs, ensembles) + data warehousing; web logs at scale. - 2010s: 'Big Data' (Hadoop/Spark), cloud computing; deep learning breakthroughs (CNNs for vision, RNN/LSTMs for sequence). - Late 2010s-early 2020s: MLOps maturation; model monitoring, data versioning, model registries. - 2023+ (GenAI era): Foundation models/transformers (LLMs, multimodal) enable powerful generation and reasoning ; retrieval-augmented systems; governance/regulation ramp up. Why it matters: each wave lowered friction-cheaper storage/compute, better algorithms-expanding what's feasible for organizations.\n\nGreat DS sits on four mutually reinforcing pillars.\n\n- Data\n\n- o Understanding schemas, quality issues, sampling, bias.\n\n- o Judging if data can actually answer the question.\n\n- Math/Statistics\n\n- o Probability, estimation, hypothesis testing, causal thinking.\n\n- o Bias-variance trade-offs; uncertainty quantification.\n\n- Computing\n\n- o Programming (Python/SQL), data pipelines, version control, cloud.\n\n- o Performance and reliability (batch vs real-time, memory/latency).\n\n- Domain Expertise\n\n- o Vocabulary, constraints, and success metrics of the field (finance, health, ops).\n\n- o Knowing which mistakes are costly and which signals matter.\n\nRule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Core pillars (data, math/stats, computing, domain)\n\n(sampling, regression) in academia &amp; industry. - 1990s-2000s: Machine Learning (SVMs, ensembles) + data warehousing; web logs at scale. - 2010s: 'Big Data' (Hadoop/Spark), cloud computing; deep learning breakthroughs (CNNs for vision, RNN/LSTMs for sequence). - Late 2010s-early 2020s: MLOps maturation; model monitoring, data versioning, model registries. - 2023+ (GenAI era): Foundation models/transformers (LLMs, multimodal) enable powerful generation and reasoning ; retrieval-augmented systems; governance/regulation ramp up. Why it matters: each wave lowered friction-cheaper storage/compute, better algorithms-expanding what's feasible for organizations.\n\nGreat DS sits on four mutually reinforcing pillars.\n\n- Data\n\n- o Understanding schemas, quality issues, sampling, bias.\n\n- o Judging if data can actually answer the question.\n\n- Math/Statistics\n\n- o Probability, estimation, hypothesis testing, causal thinking.\n\n- o Bias-variance trade-offs; uncertainty quantification.\n\n- Computing\n\n- o Programming (Python/SQL), data pipelines, version control, cloud.\n\n- o Performance and reliability (batch vs real-time, memory/latency).\n\n- Domain Expertise\n\n- o Vocabulary, constraints, and success metrics of the field (finance, health, ops).\n\n- o Knowing which mistakes are costly and which signals matter.\n\nRule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem).", "token_count": 299, "embedding_token_count": 327, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Core pillars (data, math/stats, computing, domain)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Core pillars (data, math/stats, computing, domain)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Core pillars (data, math/stats, computing, domain)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.805088Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:34-40:0004:6bd7365a", "doc_id": "DOC01", "version": "20251014", "chunk_index": 4, "block_start_index": 34, "block_end_index": 40, "text": "the question. - Math/Statistics - o Probability, estimation, hypothesis testing, causal thinking. - o Bias-variance trade-offs; uncertainty quantification. - Computing - o Programming (Python/SQL), data pipelines, version control, cloud. - o Performance and reliability (batch vs real-time, memory/latency). - Domain Expertise - o Vocabulary, constraints, and success metrics of the field (finance, health, ops). - o Knowing which mistakes are costly and which signals matter. Rule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem).\n\nData comes in different shapes and from many places; that shape influences tools and methods.\n\n- Structured: tables with fixed columns (transactions, customers).\n\nFits SQL well; easy for classical ML.\n\n- Semi-structured: JSON, logs, event streams.\n\n- Needs parsing/flattening; great for behavioral analytics.\n\n- •\n\n- Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data types &amp; sources (structured, semi-structured, unstructured; internal vs external)\n\nthe question. - Math/Statistics - o Probability, estimation, hypothesis testing, causal thinking. - o Bias-variance trade-offs; uncertainty quantification. - Computing - o Programming (Python/SQL), data pipelines, version control, cloud. - o Performance and reliability (batch vs real-time, memory/latency). - Domain Expertise - o Vocabulary, constraints, and success metrics of the field (finance, health, ops). - o Knowing which mistakes are costly and which signals matter. Rule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem).\n\nData comes in different shapes and from many places; that shape influences tools and methods.\n\n- Structured: tables with fixed columns (transactions, customers).\n\nFits SQL well; easy for classical ML.\n\n- Semi-structured: JSON, logs, event streams.\n\n- Needs parsing/flattening; great for behavioral analytics.\n\n- •\n\n- Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search.", "token_count": 211, "embedding_token_count": 245, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data types &amp; sources (structured, semi-structured, unstructured; internal vs external)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data types &amp; sources (structured, semi-structured, unstructured; internal vs external)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data types &amp; sources (structured, semi-structured, unstructured; internal vs external)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.805409Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:42-45:0005:c2a0b95e", "doc_id": "DOC01", "version": "20251014", "chunk_index": 5, "block_start_index": 42, "block_end_index": 45, "text": "mistakes are costly and which signals matter. Rule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem). Data comes in different shapes and from many places; that shape influences tools and methods. - Structured: tables with fixed columns (transactions, customers). Fits SQL well; easy for classical ML. - Semi-structured: JSON, logs, event streams. - Needs parsing/flattening; great for behavioral analytics. - • - Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search.\n\n- Internal: product databases, CRM/ERP, sensors/IoT, support tickets, app telemetry.\n\n- External: public datasets, vendors, web/APIs, social media, satellite feeds.\n\n- Acquisition modes: batch exports, CDC/streams, webhooks, surveys, scraping (with ethics/legal checks).\n\nQuality dimensions to note: completeness, accuracy, timeliness/freshness, consistency, and lineage (where it came from).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Sources\n\nmistakes are costly and which signals matter. Rule of thumb: weak in any pillar → brittle outcomes (e.g., great model, wrong problem). Data comes in different shapes and from many places; that shape influences tools and methods. - Structured: tables with fixed columns (transactions, customers). Fits SQL well; easy for classical ML. - Semi-structured: JSON, logs, event streams. - Needs parsing/flattening; great for behavioral analytics. - • - Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search.\n\n- Internal: product databases, CRM/ERP, sensors/IoT, support tickets, app telemetry.\n\n- External: public datasets, vendors, web/APIs, social media, satellite feeds.\n\n- Acquisition modes: batch exports, CDC/streams, webhooks, surveys, scraping (with ethics/legal checks).\n\nQuality dimensions to note: completeness, accuracy, timeliness/freshness, consistency, and lineage (where it came from).", "token_count": 203, "embedding_token_count": 219, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Sources"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Sources", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Sources", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.805625Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:47-69:0006:7c269b46", "doc_id": "DOC01", "version": "20251014", "chunk_index": 6, "block_start_index": 47, "block_end_index": 69, "text": "SQL well; easy for classical ML. - Semi-structured: JSON, logs, event streams. - Needs parsing/flattening; great for behavioral analytics. - • - Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search. - Internal: product databases, CRM/ERP, sensors/IoT, support tickets, app telemetry. - External: public datasets, vendors, web/APIs, social media, satellite feeds. - Acquisition modes: batch exports, CDC/streams, webhooks, surveys, scraping (with ethics/legal checks). Quality dimensions to note: completeness, accuracy, timeliness/freshness, consistency, and lineage (where it came from).\n\nA standard lifecycle reduces risk and creates repeatability.\n\n1. Problem framing\n\n2. o Define users, decisions, and success metrics (business + technical).\n\n3. o Clarify constraints (latency, privacy, budget).\n\n2. Data access &amp; preparation\n\n5. o Collect, join, and clean data; document assumptions.\n\n6. o Handle missing values/outliers; guard against leakage.\n\n3. EDA (Exploratory Data Analysis)\n\n8. o Summaries, distributions, correlations; spot drift or bias.\n\n9. o Form hypotheses; refine features and targets.\n\n4. Modeling\n\n11. o Baselines first; iterate with suitable algorithms.\n\n12. o Cross-validation; hyperparameter tuning; calibration if probabilities matter.\n\n5. Evaluation\n\n14. o Use task-appropriate metrics (e.g., F1 vs RMSE) and subgroup checks.\n\n15. o Stress tests (temporal splits, robustness).\n\n6. Deployment\n\n17. o Package as batch job, API, or stream processor; document interfaces.\n\n18. o Add observability (logging, tracing) and fallback behavior.\n\n7. Monitoring &amp; improvement\n\n20. o Track data quality, drift, performance, cost, and usage.\n\n21. o Retraining/rollback playbooks; periodic reviews with stakeholders.\n\nDeliverables to expect: data spec → EDA brief → experiment log → model card → runbook.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Typical lifecycle (problem → data → EDA → modeling → evaluation → deployment → monitoring)\n\nSQL well; easy for classical ML. - Semi-structured: JSON, logs, event streams. - Needs parsing/flattening; great for behavioral analytics. - • - Unstructured: text, images, audio, video. Often needs embeddings, deep learning, vector search. - Internal: product databases, CRM/ERP, sensors/IoT, support tickets, app telemetry. - External: public datasets, vendors, web/APIs, social media, satellite feeds. - Acquisition modes: batch exports, CDC/streams, webhooks, surveys, scraping (with ethics/legal checks). Quality dimensions to note: completeness, accuracy, timeliness/freshness, consistency, and lineage (where it came from).\n\nA standard lifecycle reduces risk and creates repeatability.\n\n1. Problem framing\n\n2. o Define users, decisions, and success metrics (business + technical).\n\n3. o Clarify constraints (latency, privacy, budget).\n\n2. Data access &amp; preparation\n\n5. o Collect, join, and clean data; document assumptions.\n\n6. o Handle missing values/outliers; guard against leakage.\n\n3. EDA (Exploratory Data Analysis)\n\n8. o Summaries, distributions, correlations; spot drift or bias.\n\n9. o Form hypotheses; refine features and targets.\n\n4. Modeling\n\n11. o Baselines first; iterate with suitable algorithms.\n\n12. o Cross-validation; hyperparameter tuning; calibration if probabilities matter.\n\n5. Evaluation\n\n14. o Use task-appropriate metrics (e.g., F1 vs RMSE) and subgroup checks.\n\n15. o Stress tests (temporal splits, robustness).\n\n6. Deployment\n\n17. o Package as batch job, API, or stream processor; document interfaces.\n\n18. o Add observability (logging, tracing) and fallback behavior.\n\n7. Monitoring &amp; improvement\n\n20. o Track data quality, drift, performance, cost, and usage.\n\n21. o Retraining/rollback playbooks; periodic reviews with stakeholders.\n\nDeliverables to expect: data spec → EDA brief → experiment log → model card → runbook.", "token_count": 405, "embedding_token_count": 437, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Typical lifecycle (problem → data → EDA → modeling → evaluation → deployment → monitoring)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Typical lifecycle (problem → data → EDA → modeling → evaluation → deployment → monitoring)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Typical lifecycle (problem → data → EDA → modeling → evaluation → deployment → monitoring)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.806028Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:71-81:0007:31dfb432", "doc_id": "DOC01", "version": "20251014", "chunk_index": 7, "block_start_index": 71, "block_end_index": 81, "text": "14. o Use task-appropriate metrics (e.g., F1 vs RMSE) and subgroup checks. 15. o Stress tests (temporal splits, robustness). 6. Deployment 17. o Package as batch job, API, or stream processor; document interfaces. 18. o Add observability (logging, tracing) and fallback behavior. 7. Monitoring &amp; improvement 20. o Track data quality, drift, performance, cost, and usage. 21. o Retraining/rollback playbooks; periodic reviews with stakeholders. Deliverables to expect: data spec → EDA brief → experiment log → model card → runbook.\n\nDS shows up anywhere decisions repeat and data exists.\n\n- Finance: credit scoring, fraud detection, customer churn, ATM cash forecasting.\n\n- Retail/e-commerce: recommendations, dynamic pricing, demand forecasting, A/B testing.\n\n- Healthcare: readmission risk, triage prioritization, medical imaging support, capacity planning.\n\n- Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization.\n\n- Logistics: route optimization, ETA prediction, inventory positioning.\n\n- Public sector: tax anomaly detection, traffic management, disaster response analytics.\n\n- HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning.\n\n- Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction.\n\n- Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction.\n\nPattern: predict + prescribe → make the next action cheaper, faster, or more accurate.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Where it's used (everyday examples across industries)\n\n14. o Use task-appropriate metrics (e.g., F1 vs RMSE) and subgroup checks. 15. o Stress tests (temporal splits, robustness). 6. Deployment 17. o Package as batch job, API, or stream processor; document interfaces. 18. o Add observability (logging, tracing) and fallback behavior. 7. Monitoring &amp; improvement 20. o Track data quality, drift, performance, cost, and usage. 21. o Retraining/rollback playbooks; periodic reviews with stakeholders. Deliverables to expect: data spec → EDA brief → experiment log → model card → runbook.\n\nDS shows up anywhere decisions repeat and data exists.\n\n- Finance: credit scoring, fraud detection, customer churn, ATM cash forecasting.\n\n- Retail/e-commerce: recommendations, dynamic pricing, demand forecasting, A/B testing.\n\n- Healthcare: readmission risk, triage prioritization, medical imaging support, capacity planning.\n\n- Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization.\n\n- Logistics: route optimization, ETA prediction, inventory positioning.\n\n- Public sector: tax anomaly detection, traffic management, disaster response analytics.\n\n- HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning.\n\n- Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction.\n\n- Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction.\n\nPattern: predict + prescribe → make the next action cheaper, faster, or more accurate.", "token_count": 292, "embedding_token_count": 318, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Where it's used (everyday examples across industries)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Where it's used (everyday examples across industries)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Where it's used (everyday examples across industries)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.806323Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:83-83:0008:07347ec9", "doc_id": "DOC01", "version": "20251014", "chunk_index": 8, "block_start_index": 83, "block_end_index": 83, "text": "Healthcare: readmission risk, triage prioritization, medical imaging support, capacity planning. - Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization. - Logistics: route optimization, ETA prediction, inventory positioning. - Public sector: tax anomaly detection, traffic management, disaster response analytics. - HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning. - Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction. - Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction. Pattern: predict + prescribe → make the next action cheaper, faster, or more accurate.\n\nReal-world DS must navigate technical, ethical, and operational hurdles.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Limits &amp; challenges (data quality, bias, drift, privacy, governance)\n\nHealthcare: readmission risk, triage prioritization, medical imaging support, capacity planning. - Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization. - Logistics: route optimization, ETA prediction, inventory positioning. - Public sector: tax anomaly detection, traffic management, disaster response analytics. - HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning. - Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction. - Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction. Pattern: predict + prescribe → make the next action cheaper, faster, or more accurate.\n\nReal-world DS must navigate technical, ethical, and operational hurdles.", "token_count": 137, "embedding_token_count": 169, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Limits &amp; challenges (data quality, bias, drift, privacy, governance)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Limits &amp; challenges (data quality, bias, drift, privacy, governance)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Limits &amp; challenges (data quality, bias, drift, privacy, governance)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.806493Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:85-98:0009:19644f52", "doc_id": "DOC01", "version": "20251014", "chunk_index": 9, "block_start_index": 85, "block_end_index": 98, "text": "planning. - Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization. - Logistics: route optimization, ETA prediction, inventory positioning. - Public sector: tax anomaly detection, traffic management, disaster response analytics. - HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning. - Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction. - Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction. Pattern: predict + prescribe → make the next action cheaper, faster, or more accurate. Real-world DS must navigate technical, ethical, and operational hurdles.\n\n- o Missing, noisy, or biased data; label quality issues.\n\n- o Non-stationarity &amp; drift (data or concept changes over time).\n\n- o Leakage (using future info), small samples, class imbalance.\n\n- Model/metrics\n\n- o Overfitting; poorly calibrated probabilities.\n\n- o Metric mismatch (optimizing AUC when cost is asymmetric).\n\n- Ethics/privacy\n\n- o PII/PHI handling, consent, retention limits.\n\n- o Fairness: subgroup performance gaps; disparate impact.\n\n- o Transparency &amp; contestability (explainability, human-in-the-loop).\n\n- Operations/governance\n\n- o Reproducibility, versioning, and approvals (model risk management).\n\n- o Monitoring blind spots; ownership after go-live.\n\n- o Cost control (compute/storage/egress) and vendor lock-in.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Data/technical\n\nplanning. - Manufacturing/IoT: predictive maintenance, quality inspection, supply-chain optimization. - Logistics: route optimization, ETA prediction, inventory positioning. - Public sector: tax anomaly detection, traffic management, disaster response analytics. - HR/People analytics: attrition risk, hiring funnel diagnostics, workforce planning. - Customer support: intent classification, auto-reply, ticket routing, satisfaction prediction. - Marketing: segmentation, uplift modeling, campaign optimization, LTV prediction. Pattern: predict + prescribe → make the next action cheaper, faster, or more accurate. Real-world DS must navigate technical, ethical, and operational hurdles.\n\n- o Missing, noisy, or biased data; label quality issues.\n\n- o Non-stationarity &amp; drift (data or concept changes over time).\n\n- o Leakage (using future info), small samples, class imbalance.\n\n- Model/metrics\n\n- o Overfitting; poorly calibrated probabilities.\n\n- o Metric mismatch (optimizing AUC when cost is asymmetric).\n\n- Ethics/privacy\n\n- o PII/PHI handling, consent, retention limits.\n\n- o Fairness: subgroup performance gaps; disparate impact.\n\n- o Transparency &amp; contestability (explainability, human-in-the-loop).\n\n- Operations/governance\n\n- o Reproducibility, versioning, and approvals (model risk management).\n\n- o Monitoring blind spots; ownership after go-live.\n\n- o Cost control (compute/storage/egress) and vendor lock-in.", "token_count": 289, "embedding_token_count": 308, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Data/technical"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Data/technical", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Data/technical", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.806771Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:100-103:0010:629bf286", "doc_id": "DOC01", "version": "20251014", "chunk_index": 10, "block_start_index": 100, "block_end_index": 103, "text": "time). - o Leakage (using future info), small samples, class imbalance. - Model/metrics - o Overfitting; poorly calibrated probabilities. - o Metric mismatch (optimizing AUC when cost is asymmetric). - Ethics/privacy - o PII/PHI handling, consent, retention limits. - o Fairness: subgroup performance gaps; disparate impact. - o Transparency &amp; contestability (explainability, human-in-the-loop). - Operations/governance - o Reproducibility, versioning, and approvals (model risk management). - o Monitoring blind spots; ownership after go-live. - o Cost control (compute/storage/egress) and vendor lock-in.\n\n- Data contracts/SLAs, quality checks, and lineage.\n\n- Robust validation schemes, model cards, and review boards.\n\n- Privacy-by-design (minimize data, access controls, audits).\n\n- Clear runbooks for retraining, rollback, and incident response.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Mitigations\n\ntime). - o Leakage (using future info), small samples, class imbalance. - Model/metrics - o Overfitting; poorly calibrated probabilities. - o Metric mismatch (optimizing AUC when cost is asymmetric). - Ethics/privacy - o PII/PHI handling, consent, retention limits. - o Fairness: subgroup performance gaps; disparate impact. - o Transparency &amp; contestability (explainability, human-in-the-loop). - Operations/governance - o Reproducibility, versioning, and approvals (model risk management). - o Monitoring blind spots; ownership after go-live. - o Cost control (compute/storage/egress) and vendor lock-in.\n\n- Data contracts/SLAs, quality checks, and lineage.\n\n- Robust validation schemes, model cards, and review boards.\n\n- Privacy-by-design (minimize data, access controls, audits).\n\n- Clear runbooks for retraining, rollback, and incident response.", "token_count": 188, "embedding_token_count": 204, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Mitigations"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Mitigations", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Mitigations", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.806964Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:105-117:0011:459070a3", "doc_id": "DOC01", "version": "20251014", "chunk_index": 11, "block_start_index": 105, "block_end_index": 117, "text": "o PII/PHI handling, consent, retention limits. - o Fairness: subgroup performance gaps; disparate impact. - o Transparency &amp; contestability (explainability, human-in-the-loop). - Operations/governance - o Reproducibility, versioning, and approvals (model risk management). - o Monitoring blind spots; ownership after go-live. - o Cost control (compute/storage/egress) and vendor lock-in. - Data contracts/SLAs, quality checks, and lineage. - Robust validation schemes, model cards, and review boards. - Privacy-by-design (minimize data, access controls, audits). - Clear runbooks for retraining, rollback, and incident response.\n\nMost real projects fit one (or a blend) of these patterns. Knowing the archetype clarifies data needs, metrics, and delivery shape.\n\n- Prediction (regression): predict a continuous value (price, demand, time-to-fail).\n\n- Data: tabular, time series. Metrics: MAE/RMSE/MAPE. Actions: price updates, inventory levels.\n\n- •\n\n- Classification: assign labels (fraud/not, churn/not). Data: tabular, logs, events. Metrics: Precision/Recall/F1/AUC; cost-sensitive metrics.\n\n- Recommendation/Ranking: order items/users/ads.\n\n- Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session.\n\n- NLP: classify/extract/summarize/search text.\n\n- Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search).\n\n- Vision: detect/classify/segment images or video.\n\n- Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate.\n\n- •\n\n- Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Common project archetypes\n\no PII/PHI handling, consent, retention limits. - o Fairness: subgroup performance gaps; disparate impact. - o Transparency &amp; contestability (explainability, human-in-the-loop). - Operations/governance - o Reproducibility, versioning, and approvals (model risk management). - o Monitoring blind spots; ownership after go-live. - o Cost control (compute/storage/egress) and vendor lock-in. - Data contracts/SLAs, quality checks, and lineage. - Robust validation schemes, model cards, and review boards. - Privacy-by-design (minimize data, access controls, audits). - Clear runbooks for retraining, rollback, and incident response.\n\nMost real projects fit one (or a blend) of these patterns. Knowing the archetype clarifies data needs, metrics, and delivery shape.\n\n- Prediction (regression): predict a continuous value (price, demand, time-to-fail).\n\n- Data: tabular, time series. Metrics: MAE/RMSE/MAPE. Actions: price updates, inventory levels.\n\n- •\n\n- Classification: assign labels (fraud/not, churn/not). Data: tabular, logs, events. Metrics: Precision/Recall/F1/AUC; cost-sensitive metrics.\n\n- Recommendation/Ranking: order items/users/ads.\n\n- Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session.\n\n- NLP: classify/extract/summarize/search text.\n\n- Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search).\n\n- Vision: detect/classify/segment images or video.\n\n- Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate.\n\n- •\n\n- Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect.", "token_count": 409, "embedding_token_count": 427, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Common project archetypes"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Common project archetypes", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Common project archetypes", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.807339Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:119-119:0012:c35c29fe", "doc_id": "DOC01", "version": "20251014", "chunk_index": 12, "block_start_index": 119, "block_end_index": 119, "text": "Data: tabular, logs, events. Metrics: Precision/Recall/F1/AUC; cost-sensitive metrics. - Recommendation/Ranking: order items/users/ads. - Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session. - NLP: classify/extract/summarize/search text. - Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect.\n\nEach domain has 'typical' problems, constraints, and KPIs.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Domain snapshots\n\nData: tabular, logs, events. Metrics: Precision/Recall/F1/AUC; cost-sensitive metrics. - Recommendation/Ranking: order items/users/ads. - Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session. - NLP: classify/extract/summarize/search text. - Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect.\n\nEach domain has 'typical' problems, constraints, and KPIs.", "token_count": 192, "embedding_token_count": 209, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Domain snapshots"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Domain snapshots", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Domain snapshots", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.807530Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:121-123:0013:e00d73b8", "doc_id": "DOC01", "version": "20251014", "chunk_index": 13, "block_start_index": 121, "block_end_index": 123, "text": "- Recommendation/Ranking: order items/users/ads. - Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session. - NLP: classify/extract/summarize/search text. - Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs.\n\n- o Use cases: credit scoring, fraud, AML, LTV prediction.\n\n- o Constraints: explainability, low latency, strict audit.\n\n- o KPIs: default rate, fraud loss prevented, approval lift at constant risk.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Finance\n\n- Recommendation/Ranking: order items/users/ads. - Data: interactions, catalogs, embeddings. Metrics: CTR, MAP/NDCG, coverage/diversity, revenue per session. - NLP: classify/extract/summarize/search text. - Data: tickets, emails, chats, docs. Metrics: F1 (NER/cls), ROUGE/BERTScore (gen), MRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs.\n\n- o Use cases: credit scoring, fraud, AML, LTV prediction.\n\n- o Constraints: explainability, low latency, strict audit.\n\n- o KPIs: default rate, fraud loss prevented, approval lift at constant risk.", "token_count": 213, "embedding_token_count": 230, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Finance"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Finance", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Finance", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.807746Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:125-127:0014:53f9f10c", "doc_id": "DOC01", "version": "20251014", "chunk_index": 14, "block_start_index": 125, "block_end_index": 127, "text": "MRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs. - o Use cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk.\n\n- o Use cases: readmission risk, imaging assist, triage, capacity.\n\n- o Constraints: PHI privacy, safety, clinical validation.\n\n- o KPIs: sensitivity at fixed specificity, LOS reduction, throughput.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Healthcare\n\nMRR/NDCG (search). - Vision: detect/classify/segment images or video. - Data: images, frames, annotations. Metrics: mAP, IoU, recall at fixed FP rate. - • - Anomaly/Quality: find rare/novel events (fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs. - o Use cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk.\n\n- o Use cases: readmission risk, imaging assist, triage, capacity.\n\n- o Constraints: PHI privacy, safety, clinical validation.\n\n- o KPIs: sensitivity at fixed specificity, LOS reduction, throughput.", "token_count": 178, "embedding_token_count": 195, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Healthcare"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Healthcare", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Healthcare", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.808035Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:129-131:0015:4849f039", "doc_id": "DOC01", "version": "20251014", "chunk_index": 15, "block_start_index": 129, "block_end_index": 131, "text": "(fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs. - o Use cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk. - o Use cases: readmission risk, imaging assist, triage, capacity. - o Constraints: PHI privacy, safety, clinical validation. - o KPIs: sensitivity at fixed specificity, LOS reduction, throughput.\n\n- o Use cases: personalization, demand forecasting, promo uplift.\n\n- o Constraints: seasonality, catalog churn.\n\n- o KPIs: revenue/visit, conversion, inventory turns, margin.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Retail/Marketing\n\n(fraud, defects). Data: sensor logs, transactions. Metrics: precision at k, alert yield, time-to-detect. Each domain has 'typical' problems, constraints, and KPIs. - o Use cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk. - o Use cases: readmission risk, imaging assist, triage, capacity. - o Constraints: PHI privacy, safety, clinical validation. - o KPIs: sensitivity at fixed specificity, LOS reduction, throughput.\n\n- o Use cases: personalization, demand forecasting, promo uplift.\n\n- o Constraints: seasonality, catalog churn.\n\n- o KPIs: revenue/visit, conversion, inventory turns, margin.", "token_count": 163, "embedding_token_count": 182, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Retail/Marketing"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Retail/Marketing", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Retail/Marketing", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.808208Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:133-139:0016:272ecbdb", "doc_id": "DOC01", "version": "20251014", "chunk_index": 16, "block_start_index": 133, "block_end_index": 139, "text": "Use cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk. - o Use cases: readmission risk, imaging assist, triage, capacity. - o Constraints: PHI privacy, safety, clinical validation. - o KPIs: sensitivity at fixed specificity, LOS reduction, throughput. - o Use cases: personalization, demand forecasting, promo uplift. - o Constraints: seasonality, catalog churn. - o KPIs: revenue/visit, conversion, inventory turns, margin.\n\n- o Use cases: predictive maintenance, visual QA, yield optimization.\n\n- o Constraints: edge inference, downtime cost.\n\n- o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ .\n\n- Public sector\n\n- o Use cases: risk scoring, resource allocation, mobility.\n\n- o Constraints: transparency, fairness/public trust.\n\n- o KPIs: service time ↓ , coverage ↑ , equitable outcomes.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Manufacturing\n\nUse cases: credit scoring, fraud, AML, LTV prediction. - o Constraints: explainability, low latency, strict audit. - o KPIs: default rate, fraud loss prevented, approval lift at constant risk. - o Use cases: readmission risk, imaging assist, triage, capacity. - o Constraints: PHI privacy, safety, clinical validation. - o KPIs: sensitivity at fixed specificity, LOS reduction, throughput. - o Use cases: personalization, demand forecasting, promo uplift. - o Constraints: seasonality, catalog churn. - o KPIs: revenue/visit, conversion, inventory turns, margin.\n\n- o Use cases: predictive maintenance, visual QA, yield optimization.\n\n- o Constraints: edge inference, downtime cost.\n\n- o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ .\n\n- Public sector\n\n- o Use cases: risk scoring, resource allocation, mobility.\n\n- o Constraints: transparency, fairness/public trust.\n\n- o KPIs: service time ↓ , coverage ↑ , equitable outcomes.", "token_count": 204, "embedding_token_count": 221, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Manufacturing"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Manufacturing", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Manufacturing", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.808415Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:141-143:0017:73995af4", "doc_id": "DOC01", "version": "20251014", "chunk_index": 17, "block_start_index": 141, "block_end_index": 143, "text": "forecasting, promo uplift. - o Constraints: seasonality, catalog churn. - o KPIs: revenue/visit, conversion, inventory turns, margin. - o Use cases: predictive maintenance, visual QA, yield optimization. - o Constraints: edge inference, downtime cost. - o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ . - Public sector - o Use cases: risk scoring, resource allocation, mobility. - o Constraints: transparency, fairness/public trust. - o KPIs: service time ↓ , coverage ↑ , equitable outcomes.\n\n- o Use cases: search/ranking, abuse detection, growth analytics.\n\n- o Constraints: rapid iteration, scale, cost.\n\n- o KPIs: retention, DAU/MAU, abuse rate ↓ , infra cost per user.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Tech/Startups\n\nforecasting, promo uplift. - o Constraints: seasonality, catalog churn. - o KPIs: revenue/visit, conversion, inventory turns, margin. - o Use cases: predictive maintenance, visual QA, yield optimization. - o Constraints: edge inference, downtime cost. - o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ . - Public sector - o Use cases: risk scoring, resource allocation, mobility. - o Constraints: transparency, fairness/public trust. - o KPIs: service time ↓ , coverage ↑ , equitable outcomes.\n\n- o Use cases: search/ranking, abuse detection, growth analytics.\n\n- o Constraints: rapid iteration, scale, cost.\n\n- o KPIs: retention, DAU/MAU, abuse rate ↓ , infra cost per user.", "token_count": 158, "embedding_token_count": 177, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Tech/Startups"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Tech/Startups", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Tech/Startups", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.808583Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:145-152:0018:f54ebf87", "doc_id": "DOC01", "version": "20251014", "chunk_index": 18, "block_start_index": 145, "block_end_index": 152, "text": "Constraints: edge inference, downtime cost. - o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ . - Public sector - o Use cases: risk scoring, resource allocation, mobility. - o Constraints: transparency, fairness/public trust. - o KPIs: service time ↓ , coverage ↑ , equitable outcomes. - o Use cases: search/ranking, abuse detection, growth analytics. - o Constraints: rapid iteration, scale, cost. - o KPIs: retention, DAU/MAU, abuse rate ↓ , infra cost per user.\n\nPick processing patterns to match the 'shape' of data.\n\n- Volume: GB → TB → PB impacts storage (Parquet/Delta), compute (Spark), and sampling strategy.\n\n- Velocity: daily batches vs near-real-time streams (Kafka/Kinesis, CDC).\n\n- Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores.\n\n- Processing modes\n\n- o Batch: cheaper, great for nightly refresh (pricing, forecasts).\n\n- o Streaming/real-time: low-latency actions (fraud, recommendations).\n\n- Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data characteristics (volume, velocity, variety; batch vs streaming)\n\nConstraints: edge inference, downtime cost. - o KPIs: MTBF ↑ , scrap ↓ , OEE ↑ , false-stop rate ↓ . - Public sector - o Use cases: risk scoring, resource allocation, mobility. - o Constraints: transparency, fairness/public trust. - o KPIs: service time ↓ , coverage ↑ , equitable outcomes. - o Use cases: search/ranking, abuse detection, growth analytics. - o Constraints: rapid iteration, scale, cost. - o KPIs: retention, DAU/MAU, abuse rate ↓ , infra cost per user.\n\nPick processing patterns to match the 'shape' of data.\n\n- Volume: GB → TB → PB impacts storage (Parquet/Delta), compute (Spark), and sampling strategy.\n\n- Velocity: daily batches vs near-real-time streams (Kafka/Kinesis, CDC).\n\n- Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores.\n\n- Processing modes\n\n- o Batch: cheaper, great for nightly refresh (pricing, forecasts).\n\n- o Streaming/real-time: low-latency actions (fraud, recommendations).\n\n- Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors.", "token_count": 246, "embedding_token_count": 274, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data characteristics (volume, velocity, variety; batch vs streaming)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data characteristics (volume, velocity, variety; batch vs streaming)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data characteristics (volume, velocity, variety; batch vs streaming)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.808825Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:154-154:0019:4634c15f", "doc_id": "DOC01", "version": "20251014", "chunk_index": 19, "block_start_index": 154, "block_end_index": 154, "text": "cost per user. Pick processing patterns to match the 'shape' of data. - Volume: GB → TB → PB impacts storage (Parquet/Delta), compute (Spark), and sampling strategy. - Velocity: daily batches vs near-real-time streams (Kafka/Kinesis, CDC). - Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores. - Processing modes - o Batch: cheaper, great for nightly refresh (pricing, forecasts). - o Streaming/real-time: low-latency actions (fraud, recommendations). - Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors.\n\nYou'll use Python to manipulate data and build models, and SQL to fetch/prepare the data from databases. Mastering both lets you move from raw tables to clean feature sets fast and reproducibly.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Programming &amp; SQL (Python basics, pandas/NumPy, SQL joins/window funcs)\n\ncost per user. Pick processing patterns to match the 'shape' of data. - Volume: GB → TB → PB impacts storage (Parquet/Delta), compute (Spark), and sampling strategy. - Velocity: daily batches vs near-real-time streams (Kafka/Kinesis, CDC). - Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores. - Processing modes - o Batch: cheaper, great for nightly refresh (pricing, forecasts). - o Streaming/real-time: low-latency actions (fraud, recommendations). - Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors.\n\nYou'll use Python to manipulate data and build models, and SQL to fetch/prepare the data from databases. Mastering both lets you move from raw tables to clean feature sets fast and reproducibly.", "token_count": 176, "embedding_token_count": 210, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Programming &amp; SQL (Python basics, pandas/NumPy, SQL joins/window funcs)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Programming &amp; SQL (Python basics, pandas/NumPy, SQL joins/window funcs)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Programming &amp; SQL (Python basics, pandas/NumPy, SQL joins/window funcs)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.809027Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:156-168:0020:00a57408", "doc_id": "DOC01", "version": "20251014", "chunk_index": 20, "block_start_index": 156, "block_end_index": 168, "text": "near-real-time streams (Kafka/Kinesis, CDC). - Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores. - Processing modes - o Batch: cheaper, great for nightly refresh (pricing, forecasts). - o Streaming/real-time: low-latency actions (fraud, recommendations). - Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors. You'll use Python to manipulate data and build models, and SQL to fetch/prepare the data from databases. Mastering both lets you move from raw tables to clean feature sets fast and reproducibly.\n\n- o Python: variables, control flow, functions, modules, virtual envs; file I/O; error handling.\n\n- o NumPy: arrays, broadcasting, vectorization, basic linear algebra.\n\n- o pandas: Series/DataFrame, indexing, groupby/agg, merge/join, tidy vs wide, time-series basics.\n\n- o SQL: SELECT/WHERE, JOIN types (inner/left/right/full), GROUP BY/HAVING, subqueries, CTEs.\n\n- o Window functions : ROW\\_NUMBER, RANK, LAG/LEAD, moving averages.\n\n- Practice\n\n- o Recreate Excel tasks in SQL (joins instead of VLOOKUP; window funcs for running totals).\n\n- o Build an end-to-end Python script: read CSV → clean → join → save Parquet.\n\n- Pitfalls\n\n- o Chained pandas operations without .copy() → SettingWithCopy bugs.\n\n- o Cartesian products from careless joins; forgetting primary keys.\n\n- Artifacts\n\n- o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\nnear-real-time streams (Kafka/Kinesis, CDC). - Variety: tabular + logs + text/images ⇒ need schemas, contracts, and feature stores. - Processing modes - o Batch: cheaper, great for nightly refresh (pricing, forecasts). - o Streaming/real-time: low-latency actions (fraud, recommendations). - Quality/lineage: contracts, validation (Great Expectations), lineage tracking, drift monitors. You'll use Python to manipulate data and build models, and SQL to fetch/prepare the data from databases. Mastering both lets you move from raw tables to clean feature sets fast and reproducibly.\n\n- o Python: variables, control flow, functions, modules, virtual envs; file I/O; error handling.\n\n- o NumPy: arrays, broadcasting, vectorization, basic linear algebra.\n\n- o pandas: Series/DataFrame, indexing, groupby/agg, merge/join, tidy vs wide, time-series basics.\n\n- o SQL: SELECT/WHERE, JOIN types (inner/left/right/full), GROUP BY/HAVING, subqueries, CTEs.\n\n- o Window functions : ROW\\_NUMBER, RANK, LAG/LEAD, moving averages.\n\n- Practice\n\n- o Recreate Excel tasks in SQL (joins instead of VLOOKUP; window funcs for running totals).\n\n- o Build an end-to-end Python script: read CSV → clean → join → save Parquet.\n\n- Pitfalls\n\n- o Chained pandas operations without .copy() → SettingWithCopy bugs.\n\n- o Cartesian products from careless joins; forgetting primary keys.\n\n- Artifacts\n\n- o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env.", "token_count": 334, "embedding_token_count": 351, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.809329Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:170-170:0021:3ae923be", "doc_id": "DOC01", "version": "20251014", "chunk_index": 21, "block_start_index": 170, "block_end_index": 170, "text": "subqueries, CTEs. - o Window functions : ROW\\_NUMBER, RANK, LAG/LEAD, moving averages. - Practice - o Recreate Excel tasks in SQL (joins instead of VLOOKUP; window funcs for running totals). - o Build an end-to-end Python script: read CSV → clean → join → save Parquet. - Pitfalls - o Chained pandas operations without .copy() → SettingWithCopy bugs. - o Cartesian products from careless joins; forgetting primary keys. - Artifacts - o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env.\n\nStats helps you quantify uncertainty and avoid wrong conclusions. You'll use it to compare groups, design experiments, and reason about noisy data.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Statistics &amp; Probability (distributions, sampling, testing, intervals)\n\nsubqueries, CTEs. - o Window functions : ROW\\_NUMBER, RANK, LAG/LEAD, moving averages. - Practice - o Recreate Excel tasks in SQL (joins instead of VLOOKUP; window funcs for running totals). - o Build an end-to-end Python script: read CSV → clean → join → save Parquet. - Pitfalls - o Chained pandas operations without .copy() → SettingWithCopy bugs. - o Cartesian products from careless joins; forgetting primary keys. - Artifacts - o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env.\n\nStats helps you quantify uncertainty and avoid wrong conclusions. You'll use it to compare groups, design experiments, and reason about noisy data.", "token_count": 143, "embedding_token_count": 172, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Statistics &amp; Probability (distributions, sampling, testing, intervals)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Statistics &amp; Probability (distributions, sampling, testing, intervals)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Statistics &amp; Probability (distributions, sampling, testing, intervals)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.809500Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:172-184:0022:1838a16a", "doc_id": "DOC01", "version": "20251014", "chunk_index": 22, "block_start_index": 172, "block_end_index": 184, "text": "instead of VLOOKUP; window funcs for running totals). - o Build an end-to-end Python script: read CSV → clean → join → save Parquet. - Pitfalls - o Chained pandas operations without .copy() → SettingWithCopy bugs. - o Cartesian products from careless joins; forgetting primary keys. - Artifacts - o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env. Stats helps you quantify uncertainty and avoid wrong conclusions. You'll use it to compare groups, design experiments, and reason about noisy data.\n\n- o Random variables &amp; common distributions (Bernoulli, Binomial, Normal, Poisson).\n\n- o Law of large numbers &amp; Central Limit Theorem (why means stabilize).\n\n- o Estimation: confidence intervals (what 95% really means).\n\n- o Hypothesis testing: null/alternative, p-values, type I/II errors, power &amp; effect size.\n\n- o Sampling: simple vs stratified; bias and variance; bootstrap.\n\n- Practice\n\n- o Compute a 95% CI for a mean and a proportion; explain it in plain language.\n\n- o A/B test readout: difference in conversion, CI, p-value, and business impact.\n\n- Pitfalls\n\n- o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction.\n\n- o Correlation ≠ causation; Simpson's paradox.\n\n- Artifacts\n\n- o Stats cheat sheet (tests by scenario); experiment readout template.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\ninstead of VLOOKUP; window funcs for running totals). - o Build an end-to-end Python script: read CSV → clean → join → save Parquet. - Pitfalls - o Chained pandas operations without .copy() → SettingWithCopy bugs. - o Cartesian products from careless joins; forgetting primary keys. - Artifacts - o Reusable data-cleaning script; SQL snippets library; requirements.txt/conda env. Stats helps you quantify uncertainty and avoid wrong conclusions. You'll use it to compare groups, design experiments, and reason about noisy data.\n\n- o Random variables &amp; common distributions (Bernoulli, Binomial, Normal, Poisson).\n\n- o Law of large numbers &amp; Central Limit Theorem (why means stabilize).\n\n- o Estimation: confidence intervals (what 95% really means).\n\n- o Hypothesis testing: null/alternative, p-values, type I/II errors, power &amp; effect size.\n\n- o Sampling: simple vs stratified; bias and variance; bootstrap.\n\n- Practice\n\n- o Compute a 95% CI for a mean and a proportion; explain it in plain language.\n\n- o A/B test readout: difference in conversion, CI, p-value, and business impact.\n\n- Pitfalls\n\n- o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction.\n\n- o Correlation ≠ causation; Simpson's paradox.\n\n- Artifacts\n\n- o Stats cheat sheet (tests by scenario); experiment readout template.", "token_count": 295, "embedding_token_count": 312, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.809773Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:186-186:0023:07559495", "doc_id": "DOC01", "version": "20251014", "chunk_index": 23, "block_start_index": 186, "block_end_index": 186, "text": "power &amp; effect size. - o Sampling: simple vs stratified; bias and variance; bootstrap. - Practice - o Compute a 95% CI for a mean and a proportion; explain it in plain language. - o A/B test readout: difference in conversion, CI, p-value, and business impact. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction. - o Correlation ≠ causation; Simpson's paradox. - Artifacts - o Stats cheat sheet (tests by scenario); experiment readout template.\n\nML turns patterns into predictions and decisions. Start simple, then iterate with validation and tuning to avoid overfitting.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Machine Learning (supervised/unsupervised, model selection, tuning)\n\npower &amp; effect size. - o Sampling: simple vs stratified; bias and variance; bootstrap. - Practice - o Compute a 95% CI for a mean and a proportion; explain it in plain language. - o A/B test readout: difference in conversion, CI, p-value, and business impact. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction. - o Correlation ≠ causation; Simpson's paradox. - Artifacts - o Stats cheat sheet (tests by scenario); experiment readout template.\n\nML turns patterns into predictions and decisions. Start simple, then iterate with validation and tuning to avoid overfitting.", "token_count": 136, "embedding_token_count": 163, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Machine Learning (supervised/unsupervised, model selection, tuning)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Machine Learning (supervised/unsupervised, model selection, tuning)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Machine Learning (supervised/unsupervised, model selection, tuning)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.809980Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:188-200:0024:ad2f3416", "doc_id": "DOC01", "version": "20251014", "chunk_index": 24, "block_start_index": 188, "block_end_index": 200, "text": "Compute a 95% CI for a mean and a proportion; explain it in plain language. - o A/B test readout: difference in conversion, CI, p-value, and business impact. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction. - o Correlation ≠ causation; Simpson's paradox. - Artifacts - o Stats cheat sheet (tests by scenario); experiment readout template. ML turns patterns into predictions and decisions. Start simple, then iterate with validation and tuning to avoid overfitting.\n\n- o Task types: regression vs classification; clustering &amp; anomaly detection.\n\n- o Pipelines: split → baseline → features → model → tune → evaluate.\n\n- o Algorithms: Linear/Logistic, Trees/Random Forest, Gradient Boosting (XGBoost/LightGBM), k-means, PCA.\n\n- o Model selection : cross-validation; compare on the right metrics.\n\n- o Tuning : grid/random search; early stopping; regularization; class weights.\n\n- Practice\n\n- o Build a baseline (mean/majority) then beat it with 2 models; log results.\n\n- o Tune max\\_depth/learning\\_rate on a GBM; plot validation curves.\n\n- Pitfalls\n\n- o Data leakage (using future/target info in features); improper CV for time series.\n\n- o Chasing tiny metric gains with very complex models.\n\n- Artifacts\n\n- o Experiment log; model card (what/why/how/limits); saved pipeline.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\nCompute a 95% CI for a mean and a proportion; explain it in plain language. - o A/B test readout: difference in conversion, CI, p-value, and business impact. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; multiple comparisons without correction. - o Correlation ≠ causation; Simpson's paradox. - Artifacts - o Stats cheat sheet (tests by scenario); experiment readout template. ML turns patterns into predictions and decisions. Start simple, then iterate with validation and tuning to avoid overfitting.\n\n- o Task types: regression vs classification; clustering &amp; anomaly detection.\n\n- o Pipelines: split → baseline → features → model → tune → evaluate.\n\n- o Algorithms: Linear/Logistic, Trees/Random Forest, Gradient Boosting (XGBoost/LightGBM), k-means, PCA.\n\n- o Model selection : cross-validation; compare on the right metrics.\n\n- o Tuning : grid/random search; early stopping; regularization; class weights.\n\n- Practice\n\n- o Build a baseline (mean/majority) then beat it with 2 models; log results.\n\n- o Tune max\\_depth/learning\\_rate on a GBM; plot validation curves.\n\n- Pitfalls\n\n- o Data leakage (using future/target info in features); improper CV for time series.\n\n- o Chasing tiny metric gains with very complex models.\n\n- Artifacts\n\n- o Experiment log; model card (what/why/how/limits); saved pipeline.", "token_count": 297, "embedding_token_count": 314, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.810265Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:202-214:0025:0889e968", "doc_id": "DOC01", "version": "20251014", "chunk_index": 25, "block_start_index": 202, "block_end_index": 214, "text": "cross-validation; compare on the right metrics. - o Tuning : grid/random search; early stopping; regularization; class weights. - Practice - o Build a baseline (mean/majority) then beat it with 2 models; log results. - o Tune max\\_depth/learning\\_rate on a GBM; plot validation curves. - Pitfalls - o Data leakage (using future/target info in features); improper CV for time series. - o Chasing tiny metric gains with very complex models. - Artifacts - o Experiment log; model card (what/why/how/limits); saved pipeline.\n\nEDA is where you learn the dataset's shape, spot issues, and form hypotheses. Good visuals make insights obvious to others.\n\n- Learn\n\n- o Univariate: histograms/boxplots; central tendency &amp; spread; skew/kurtosis.\n\n- o Bivariate: scatter/violin/heatmaps; correlation (Pearson/Spearman).\n\n- o Target analysis: class balance; partial plots for key features.\n\n- o Dashboard basics: filters, drilldowns, KPIs; tidy layout and labeling.\n\n- Practice\n\n- o Produce a one-page EDA summary: 5 key charts + 5 bullet insights.\n\n- o Compare log vs raw scale for skewed variables; justify transformations.\n\n- Pitfalls\n\n- o Misleading axes/scales; ignoring missingness patterns; overtrusting correlations.\n\n- Artifacts\n\n- o EDA notebook + PNGs; lightweight narrative with 'so-what' insights.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > EDA &amp; Visualization (descriptives, correlations, dashboards)\n\ncross-validation; compare on the right metrics. - o Tuning : grid/random search; early stopping; regularization; class weights. - Practice - o Build a baseline (mean/majority) then beat it with 2 models; log results. - o Tune max\\_depth/learning\\_rate on a GBM; plot validation curves. - Pitfalls - o Data leakage (using future/target info in features); improper CV for time series. - o Chasing tiny metric gains with very complex models. - Artifacts - o Experiment log; model card (what/why/how/limits); saved pipeline.\n\nEDA is where you learn the dataset's shape, spot issues, and form hypotheses. Good visuals make insights obvious to others.\n\n- Learn\n\n- o Univariate: histograms/boxplots; central tendency &amp; spread; skew/kurtosis.\n\n- o Bivariate: scatter/violin/heatmaps; correlation (Pearson/Spearman).\n\n- o Target analysis: class balance; partial plots for key features.\n\n- o Dashboard basics: filters, drilldowns, KPIs; tidy layout and labeling.\n\n- Practice\n\n- o Produce a one-page EDA summary: 5 key charts + 5 bullet insights.\n\n- o Compare log vs raw scale for skewed variables; justify transformations.\n\n- Pitfalls\n\n- o Misleading axes/scales; ignoring missingness patterns; overtrusting correlations.\n\n- Artifacts\n\n- o EDA notebook + PNGs; lightweight narrative with 'so-what' insights.", "token_count": 285, "embedding_token_count": 312, "section_path": ["Definition &amp; scope (what DS is / isn't)", "EDA &amp; Visualization (descriptives, correlations, dashboards)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > EDA &amp; Visualization (descriptives, correlations, dashboards)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|EDA &amp; Visualization (descriptives, correlations, dashboards)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.810508Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:216-229:0026:0df367a8", "doc_id": "DOC01", "version": "20251014", "chunk_index": 26, "block_start_index": 216, "block_end_index": 229, "text": "- o Bivariate: scatter/violin/heatmaps; correlation (Pearson/Spearman). - o Target analysis: class balance; partial plots for key features. - o Dashboard basics: filters, drilldowns, KPIs; tidy layout and labeling. - Practice - o Produce a one-page EDA summary: 5 key charts + 5 bullet insights. - o Compare log vs raw scale for skewed variables; justify transformations. - Pitfalls - o Misleading axes/scales; ignoring missingness patterns; overtrusting correlations. - Artifacts - o EDA notebook + PNGs; lightweight narrative with 'so-what' insights.\n\nMost performance comes from good features. Engineer them systematically while preventing leakage and overfitting.\n\n- Learn\n\n- o Missing/outliers: imputation strategies; winsorization vs clipping.\n\n- o Encodings: one-hot vs ordinal; target/mean encoding (with CV).\n\n- o Scaling: standard vs min-max; when models require it (SVM, KNN).\n\n- o Transformations: log, binning, interactions, ratios; date/time features; rolling windows for TS.\n\n- o Leakage guards : fit imputers/encoders inside CV folds or pipeline.\n\n- Practice\n\n- o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end.\n\n- o Create rolling-mean features only from past windows on TS.\n\n- Pitfalls\n\n- o Fitting scalers on full data; using target statistics without proper CV.\n\n- Artifacts\n\n- o Reusable feature pipeline; data dictionary of engineered features.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Feature Engineering (encoding, scaling, transformations, leakage avoidance)\n\n- o Bivariate: scatter/violin/heatmaps; correlation (Pearson/Spearman). - o Target analysis: class balance; partial plots for key features. - o Dashboard basics: filters, drilldowns, KPIs; tidy layout and labeling. - Practice - o Produce a one-page EDA summary: 5 key charts + 5 bullet insights. - o Compare log vs raw scale for skewed variables; justify transformations. - Pitfalls - o Misleading axes/scales; ignoring missingness patterns; overtrusting correlations. - Artifacts - o EDA notebook + PNGs; lightweight narrative with 'so-what' insights.\n\nMost performance comes from good features. Engineer them systematically while preventing leakage and overfitting.\n\n- Learn\n\n- o Missing/outliers: imputation strategies; winsorization vs clipping.\n\n- o Encodings: one-hot vs ordinal; target/mean encoding (with CV).\n\n- o Scaling: standard vs min-max; when models require it (SVM, KNN).\n\n- o Transformations: log, binning, interactions, ratios; date/time features; rolling windows for TS.\n\n- o Leakage guards : fit imputers/encoders inside CV folds or pipeline.\n\n- Practice\n\n- o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end.\n\n- o Create rolling-mean features only from past windows on TS.\n\n- Pitfalls\n\n- o Fitting scalers on full data; using target statistics without proper CV.\n\n- Artifacts\n\n- o Reusable feature pipeline; data dictionary of engineered features.", "token_count": 289, "embedding_token_count": 316, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Feature Engineering (encoding, scaling, transformations, leakage avoidance)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Feature Engineering (encoding, scaling, transformations, leakage avoidance)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Feature Engineering (encoding, scaling, transformations, leakage avoidance)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.810755Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:231-231:0027:5ae8c736", "doc_id": "DOC01", "version": "20251014", "chunk_index": 27, "block_start_index": 231, "block_end_index": 231, "text": "models require it (SVM, KNN). - o Transformations: log, binning, interactions, ratios; date/time features; rolling windows for TS. - o Leakage guards : fit imputers/encoders inside CV folds or pipeline. - Practice - o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end. - o Create rolling-mean features only from past windows on TS. - Pitfalls - o Fitting scalers on full data; using target statistics without proper CV. - Artifacts - o Reusable feature pipeline; data dictionary of engineered features.\n\nPick metrics that match decisions and costs. Validate the right way for your data's structure.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Model Evaluation (classification/regression metrics, validation schemes)\n\nmodels require it (SVM, KNN). - o Transformations: log, binning, interactions, ratios; date/time features; rolling windows for TS. - o Leakage guards : fit imputers/encoders inside CV folds or pipeline. - Practice - o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end. - o Create rolling-mean features only from past windows on TS. - Pitfalls - o Fitting scalers on full data; using target statistics without proper CV. - Artifacts - o Reusable feature pipeline; data dictionary of engineered features.\n\nPick metrics that match decisions and costs. Validate the right way for your data's structure.", "token_count": 131, "embedding_token_count": 157, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Model Evaluation (classification/regression metrics, validation schemes)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Model Evaluation (classification/regression metrics, validation schemes)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Model Evaluation (classification/regression metrics, validation schemes)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.810898Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:233-240:0028:a7ff3802", "doc_id": "DOC01", "version": "20251014", "chunk_index": 28, "block_start_index": 233, "block_end_index": 240, "text": "windows for TS. - o Leakage guards : fit imputers/encoders inside CV folds or pipeline. - Practice - o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end. - o Create rolling-mean features only from past windows on TS. - Pitfalls - o Fitting scalers on full data; using target statistics without proper CV. - Artifacts - o Reusable feature pipeline; data dictionary of engineered features. Pick metrics that match decisions and costs. Validate the right way for your data's structure.\n\n- o Classification: accuracy, precision/recall/F1, ROC-AUC vs PR-AUC; confusion matrix.\n\n- o Thresholding: cost-sensitive decisions; precision-at-k; lift/gain.\n\n- o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges.\n\n- o Calibration: reliability curves; Brier score (probability quality).\n\n- o Validation: holdout, k-fold/stratified, time-series CV (rolling windows).\n\n- Practice\n\n- o Plot ROC and PR curves; pick a threshold based on a cost matrix.\n\n- o Run rolling-origin evaluation on a TS model; report WAPE by horizon.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\nwindows for TS. - o Leakage guards : fit imputers/encoders inside CV folds or pipeline. - Practice - o Build a sklearn Pipeline/ColumnTransformer handling numeric/categorical features end-to-end. - o Create rolling-mean features only from past windows on TS. - Pitfalls - o Fitting scalers on full data; using target statistics without proper CV. - Artifacts - o Reusable feature pipeline; data dictionary of engineered features. Pick metrics that match decisions and costs. Validate the right way for your data's structure.\n\n- o Classification: accuracy, precision/recall/F1, ROC-AUC vs PR-AUC; confusion matrix.\n\n- o Thresholding: cost-sensitive decisions; precision-at-k; lift/gain.\n\n- o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges.\n\n- o Calibration: reliability curves; Brier score (probability quality).\n\n- o Validation: holdout, k-fold/stratified, time-series CV (rolling windows).\n\n- Practice\n\n- o Plot ROC and PR curves; pick a threshold based on a cost matrix.\n\n- o Run rolling-origin evaluation on a TS model; report WAPE by horizon.", "token_count": 237, "embedding_token_count": 254, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811111Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:242-242:0029:c492f495", "doc_id": "DOC01", "version": "20251014", "chunk_index": 29, "block_start_index": 242, "block_end_index": 242, "text": "structure. - o Classification: accuracy, precision/recall/F1, ROC-AUC vs PR-AUC; confusion matrix. - o Thresholding: cost-sensitive decisions; precision-at-k; lift/gain. - o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon.\n\n- o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Pitfalls\n\nstructure. - o Classification: accuracy, precision/recall/F1, ROC-AUC vs PR-AUC; confusion matrix. - o Thresholding: cost-sensitive decisions; precision-at-k; lift/gain. - o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon.\n\n- o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data.", "token_count": 157, "embedding_token_count": 174, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Pitfalls"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Pitfalls", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Pitfalls", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811262Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:244-244:0030:7232d799", "doc_id": "DOC01", "version": "20251014", "chunk_index": 30, "block_start_index": 244, "block_end_index": 244, "text": "precision-at-k; lift/gain. - o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data.\n\n- o Evaluation report with metric table, curves, and threshold rationale.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Artifacts\n\nprecision-at-k; lift/gain. - o Regression: MAE vs RMSE vs R²; MAPE/WMAPE; quantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data.\n\n- o Evaluation report with metric table, curves, and threshold rationale.", "token_count": 137, "embedding_token_count": 154, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Artifacts"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Artifacts", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Artifacts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811402Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:246-246:0031:56633206", "doc_id": "DOC01", "version": "20251014", "chunk_index": 31, "block_start_index": 246, "block_end_index": 246, "text": "quantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale.\n\nMLOps makes models reliable in production. Package, serve, and monitor with versioned data and models.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > MLOps Basics (packaging, APIs, monitoring, data/model versioning)\n\nquantile loss if predicting ranges. - o Calibration: reliability curves; Brier score (probability quality). - o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale.\n\nMLOps makes models reliable in production. Package, serve, and monitor with versioned data and models.", "token_count": 132, "embedding_token_count": 161, "section_path": ["Definition &amp; scope (what DS is / isn't)", "MLOps Basics (packaging, APIs, monitoring, data/model versioning)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > MLOps Basics (packaging, APIs, monitoring, data/model versioning)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|MLOps Basics (packaging, APIs, monitoring, data/model versioning)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811537Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:248-252:0032:a9301d9b", "doc_id": "DOC01", "version": "20251014", "chunk_index": 32, "block_start_index": 248, "block_end_index": 252, "text": "o Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale. MLOps makes models reliable in production. Package, serve, and monitor with versioned data and models.\n\n- o Packaging: requirements.txt/conda; Docker basics; reproducible seeds.\n\n- o Serving: batch jobs vs REST APIs (FastAPI) vs streaming; auth &amp; rate limits.\n\n- o Monitoring: data quality (missing/drift), performance, latency/cost; alerts &amp; dashboards.\n\n- o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD.\n\n- o Runbooks: rollback, retraining cadence, ownership.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\no Validation: holdout, k-fold/stratified, time-series CV (rolling windows). - Practice - o Plot ROC and PR curves; pick a threshold based on a cost matrix. - o Run rolling-origin evaluation on a TS model; report WAPE by horizon. - o Reporting AUC when you deploy a fixed-capacity top-k list; random splits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale. MLOps makes models reliable in production. Package, serve, and monitor with versioned data and models.\n\n- o Packaging: requirements.txt/conda; Docker basics; reproducible seeds.\n\n- o Serving: batch jobs vs REST APIs (FastAPI) vs streaming; auth &amp; rate limits.\n\n- o Monitoring: data quality (missing/drift), performance, latency/cost; alerts &amp; dashboards.\n\n- o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD.\n\n- o Runbooks: rollback, retraining cadence, ownership.", "token_count": 205, "embedding_token_count": 222, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811726Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:254-259:0033:ae160abf", "doc_id": "DOC01", "version": "20251014", "chunk_index": 33, "block_start_index": 254, "block_end_index": 259, "text": "splits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale. MLOps makes models reliable in production. Package, serve, and monitor with versioned data and models. - o Packaging: requirements.txt/conda; Docker basics; reproducible seeds. - o Serving: batch jobs vs REST APIs (FastAPI) vs streaming; auth &amp; rate limits. - o Monitoring: data quality (missing/drift), performance, latency/cost; alerts &amp; dashboards. - o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD. - o Runbooks: rollback, retraining cadence, ownership.\n\n- o Serve a trained model via FastAPI; log requests/responses; save a model to a registry.\n\n- o Add a simple drift check (population stability index or KS test) on input features.\n\n- Pitfalls\n\n- o 'Works on my machine'; no rollback plan; silent model decay.\n\n- Artifacts\n\n- o Dockerfile + API spec; monitoring dashboard; runbook; model registry record.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Practice\n\nsplits on temporal data. - o Evaluation report with metric table, curves, and threshold rationale. MLOps makes models reliable in production. Package, serve, and monitor with versioned data and models. - o Packaging: requirements.txt/conda; Docker basics; reproducible seeds. - o Serving: batch jobs vs REST APIs (FastAPI) vs streaming; auth &amp; rate limits. - o Monitoring: data quality (missing/drift), performance, latency/cost; alerts &amp; dashboards. - o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD. - o Runbooks: rollback, retraining cadence, ownership.\n\n- o Serve a trained model via FastAPI; log requests/responses; save a model to a registry.\n\n- o Add a simple drift check (population stability index or KS test) on input features.\n\n- Pitfalls\n\n- o 'Works on my machine'; no rollback plan; silent model decay.\n\n- Artifacts\n\n- o Dockerfile + API spec; monitoring dashboard; runbook; model registry record.", "token_count": 209, "embedding_token_count": 226, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Practice"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Practice", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Practice", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.811920Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:261-273:0034:27e1ce18", "doc_id": "DOC01", "version": "20251014", "chunk_index": 34, "block_start_index": 261, "block_end_index": 273, "text": "latency/cost; alerts &amp; dashboards. - o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD. - o Runbooks: rollback, retraining cadence, ownership. - o Serve a trained model via FastAPI; log requests/responses; save a model to a registry. - o Add a simple drift check (population stability index or KS test) on input features. - Pitfalls - o 'Works on my machine'; no rollback plan; silent model decay. - Artifacts - o Dockerfile + API spec; monitoring dashboard; runbook; model registry record.\n\nUse distributed tools when data is too large/fast for a single machine. Prefer columnar storage and efficient queries.\n\n- Learn\n\n- o Storage: Parquet/Delta (columnar, compressed), partitioning, file sizing.\n\n- o Compute: Spark DataFrame ops; joins, windowing, UDF pitfalls.\n\n- o Platforms: data lake vs warehouse (ELT), query engines (Hive/Presto/Trino).\n\n- o When to stream (Kafka/PubSub) vs batch; CDC patterns.\n\n- Practice\n\n- o Convert CSV → Parquet; benchmark query times; add partitioning (by date).\n\n- o Re-implement a pandas pipeline in PySpark and validate parity.\n\n- Pitfalls\n\n- o Too many small files; skewed joins; using Python UDFs unnecessarily.\n\n- Artifacts\n\n- o Spark notebook; lake/warehouse schema + partition plan; cost notes.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Big Data &amp; Distributed (Spark, data lakes/warehouses, Parquet)\n\nlatency/cost; alerts &amp; dashboards. - o Versioning: MLflow/DVC (params, artifacts, models); registries; CI/CD. - o Runbooks: rollback, retraining cadence, ownership. - o Serve a trained model via FastAPI; log requests/responses; save a model to a registry. - o Add a simple drift check (population stability index or KS test) on input features. - Pitfalls - o 'Works on my machine'; no rollback plan; silent model decay. - Artifacts - o Dockerfile + API spec; monitoring dashboard; runbook; model registry record.\n\nUse distributed tools when data is too large/fast for a single machine. Prefer columnar storage and efficient queries.\n\n- Learn\n\n- o Storage: Parquet/Delta (columnar, compressed), partitioning, file sizing.\n\n- o Compute: Spark DataFrame ops; joins, windowing, UDF pitfalls.\n\n- o Platforms: data lake vs warehouse (ELT), query engines (Hive/Presto/Trino).\n\n- o When to stream (Kafka/PubSub) vs batch; CDC patterns.\n\n- Practice\n\n- o Convert CSV → Parquet; benchmark query times; add partitioning (by date).\n\n- o Re-implement a pandas pipeline in PySpark and validate parity.\n\n- Pitfalls\n\n- o Too many small files; skewed joins; using Python UDFs unnecessarily.\n\n- Artifacts\n\n- o Spark notebook; lake/warehouse schema + partition plan; cost notes.", "token_count": 283, "embedding_token_count": 314, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Big Data &amp; Distributed (Spark, data lakes/warehouses, Parquet)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Big Data &amp; Distributed (Spark, data lakes/warehouses, Parquet)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Big Data &amp; Distributed (Spark, data lakes/warehouses, Parquet)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.812212Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:275-287:0035:0448b3cd", "doc_id": "DOC01", "version": "20251014", "chunk_index": 35, "block_start_index": 275, "block_end_index": 287, "text": "DataFrame ops; joins, windowing, UDF pitfalls. - o Platforms: data lake vs warehouse (ELT), query engines (Hive/Presto/Trino). - o When to stream (Kafka/PubSub) vs batch; CDC patterns. - Practice - o Convert CSV → Parquet; benchmark query times; add partitioning (by date). - o Re-implement a pandas pipeline in PySpark and validate parity. - Pitfalls - o Too many small files; skewed joins; using Python UDFs unnecessarily. - Artifacts - o Spark notebook; lake/warehouse schema + partition plan; cost notes.\n\nSpecialized modalities need tailored preprocessing and models. Start with transfer learning and simple baselines.\n\n- NLP (text)\n\n- o Learn: tokenization, TF-IDF vs embeddings, classification/NER, RAG basics.\n\n- o Practice: classify support tickets; build a keyword extractor; evaluate F1.\n\n- o Pitfalls: data leakage via document splits; domain shift; hallucinations in gen models.\n\n- CV (images)\n\n- o Learn: augmentation, CNNs, transfer learning, detection/segmentation metrics (mAP, IoU).\n\n- o Practice: fine-tune a pre-trained model on a small image dataset; evaluate confusion matrix &amp; IoU.\n\n- o Pitfalls: train/val leakage from near-duplicate images; class imbalance.\n\n- TS (time series)\n\n- o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM).\n\n- o Practice: forecast demand with quantile loss; compare naive vs model WAPE.\n\n- o Pitfalls: using future info in features; not respecting temporal splits.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > NLP/CV/TS (intros to text, images, time series, embeddings)\n\nDataFrame ops; joins, windowing, UDF pitfalls. - o Platforms: data lake vs warehouse (ELT), query engines (Hive/Presto/Trino). - o When to stream (Kafka/PubSub) vs batch; CDC patterns. - Practice - o Convert CSV → Parquet; benchmark query times; add partitioning (by date). - o Re-implement a pandas pipeline in PySpark and validate parity. - Pitfalls - o Too many small files; skewed joins; using Python UDFs unnecessarily. - Artifacts - o Spark notebook; lake/warehouse schema + partition plan; cost notes.\n\nSpecialized modalities need tailored preprocessing and models. Start with transfer learning and simple baselines.\n\n- NLP (text)\n\n- o Learn: tokenization, TF-IDF vs embeddings, classification/NER, RAG basics.\n\n- o Practice: classify support tickets; build a keyword extractor; evaluate F1.\n\n- o Pitfalls: data leakage via document splits; domain shift; hallucinations in gen models.\n\n- CV (images)\n\n- o Learn: augmentation, CNNs, transfer learning, detection/segmentation metrics (mAP, IoU).\n\n- o Practice: fine-tune a pre-trained model on a small image dataset; evaluate confusion matrix &amp; IoU.\n\n- o Pitfalls: train/val leakage from near-duplicate images; class imbalance.\n\n- TS (time series)\n\n- o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM).\n\n- o Practice: forecast demand with quantile loss; compare naive vs model WAPE.\n\n- o Pitfalls: using future info in features; not respecting temporal splits.", "token_count": 320, "embedding_token_count": 352, "section_path": ["Definition &amp; scope (what DS is / isn't)", "NLP/CV/TS (intros to text, images, time series, embeddings)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > NLP/CV/TS (intros to text, images, time series, embeddings)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|NLP/CV/TS (intros to text, images, time series, embeddings)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.812529Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:289-291:0036:a2aa4fe9", "doc_id": "DOC01", "version": "20251014", "chunk_index": 36, "block_start_index": 289, "block_end_index": 291, "text": "- CV (images) - o Learn: augmentation, CNNs, transfer learning, detection/segmentation metrics (mAP, IoU). - o Practice: fine-tune a pre-trained model on a small image dataset; evaluate confusion matrix &amp; IoU. - o Pitfalls: train/val leakage from near-duplicate images; class imbalance. - TS (time series) - o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits.\n\n- o Learn: vector representations; cosine similarity; vector DB basics.\n\n- o Practice: build a mini semantic search over documents/images.\n\n- o Artifacts: modality-specific evaluation scripts; embedding search demo.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Embeddings (all modalities)\n\n- CV (images) - o Learn: augmentation, CNNs, transfer learning, detection/segmentation metrics (mAP, IoU). - o Practice: fine-tune a pre-trained model on a small image dataset; evaluate confusion matrix &amp; IoU. - o Pitfalls: train/val leakage from near-duplicate images; class imbalance. - TS (time series) - o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits.\n\n- o Learn: vector representations; cosine similarity; vector DB basics.\n\n- o Practice: build a mini semantic search over documents/images.\n\n- o Artifacts: modality-specific evaluation scripts; embedding search demo.", "token_count": 171, "embedding_token_count": 192, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Embeddings (all modalities)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Embeddings (all modalities)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Embeddings (all modalities)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.812694Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:293-293:0037:27869bf2", "doc_id": "DOC01", "version": "20251014", "chunk_index": 37, "block_start_index": 293, "block_end_index": 293, "text": "&amp; IoU. - o Pitfalls: train/val leakage from near-duplicate images; class imbalance. - TS (time series) - o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits. - o Learn: vector representations; cosine similarity; vector DB basics. - o Practice: build a mini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo.\n\nResponsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Ethics &amp; Governance (fairness, privacy, security)\n\n&amp; IoU. - o Pitfalls: train/val leakage from near-duplicate images; class imbalance. - TS (time series) - o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits. - o Learn: vector representations; cosine similarity; vector DB basics. - o Practice: build a mini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo.\n\nResponsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later.", "token_count": 145, "embedding_token_count": 172, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Ethics &amp; Governance (fairness, privacy, security)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Ethics &amp; Governance (fairness, privacy, security)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Ethics &amp; Governance (fairness, privacy, security)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.812845Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:295-299:0038:ec38f143", "doc_id": "DOC01", "version": "20251014", "chunk_index": 38, "block_start_index": 295, "block_end_index": 299, "text": "o Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits. - o Learn: vector representations; cosine similarity; vector DB basics. - o Practice: build a mini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo. Responsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later.\n\n- o Privacy: PII/PHI inventory, minimization, consent, retention, access control.\n\n- o Fairness: subgroup metrics, disparate impact checks, remediation.\n\n- o Explainability: global/local (feature importance, SHAP); model cards.\n\n- o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks.\n\n- o Governance: approvals, audit trails, Model Risk Management (MRM).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\no Learn: trend/seasonality, exogenous vars, rolling CV, forecasting (ARIMA/Prophet/GBM). - o Practice: forecast demand with quantile loss; compare naive vs model WAPE. - o Pitfalls: using future info in features; not respecting temporal splits. - o Learn: vector representations; cosine similarity; vector DB basics. - o Practice: build a mini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo. Responsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later.\n\n- o Privacy: PII/PHI inventory, minimization, consent, retention, access control.\n\n- o Fairness: subgroup metrics, disparate impact checks, remediation.\n\n- o Explainability: global/local (feature importance, SHAP); model cards.\n\n- o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks.\n\n- o Governance: approvals, audit trails, Model Risk Management (MRM).", "token_count": 199, "embedding_token_count": 216, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813032Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:301-304:0039:bf0482d6", "doc_id": "DOC01", "version": "20251014", "chunk_index": 39, "block_start_index": 301, "block_end_index": 304, "text": "mini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo. Responsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later. - o Privacy: PII/PHI inventory, minimization, consent, retention, access control. - o Fairness: subgroup metrics, disparate impact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM).\n\n- o Add subgroup performance tables to every evaluation; write a model card.\n\n- o Run a privacy review: identify sensitive fields; propose minimization.\n\n- Pitfalls\n\n- o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Practice\n\nmini semantic search over documents/images. - o Artifacts: modality-specific evaluation scripts; embedding search demo. Responsible AI prevents harm and keeps you compliant. Bake it in from day one-cheaper than fixing later. - o Privacy: PII/PHI inventory, minimization, consent, retention, access control. - o Fairness: subgroup metrics, disparate impact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM).\n\n- o Add subgroup performance tables to every evaluation; write a model card.\n\n- o Run a privacy review: identify sensitive fields; propose minimization.\n\n- Pitfalls\n\n- o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups).", "token_count": 184, "embedding_token_count": 201, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Practice"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Practice", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Practice", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813201Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:306-306:0040:d7bc9800", "doc_id": "DOC01", "version": "20251014", "chunk_index": 40, "block_start_index": 306, "block_end_index": 306, "text": "consent, retention, access control. - o Fairness: subgroup metrics, disparate impact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups).\n\n- o Model card; risk assessment; access logs; DPA/consent documentation.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Artifacts\n\nconsent, retention, access control. - o Fairness: subgroup metrics, disparate impact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups).\n\n- o Model card; risk assessment; access logs; DPA/consent documentation.", "token_count": 146, "embedding_token_count": 163, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Artifacts"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Artifacts", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Artifacts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813340Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:308-308:0041:e5f24a65", "doc_id": "DOC01", "version": "20251014", "chunk_index": 41, "block_start_index": 308, "block_end_index": 308, "text": "impact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation.\n\nGreat work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Communication &amp; Product Thinking (stakeholders, decision impact)\n\nimpact checks, remediation. - o Explainability: global/local (feature importance, SHAP); model cards. - o Security: secrets management, encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation.\n\nGreat work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact.", "token_count": 152, "embedding_token_count": 179, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Communication &amp; Product Thinking (stakeholders, decision impact)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Communication &amp; Product Thinking (stakeholders, decision impact)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Communication &amp; Product Thinking (stakeholders, decision impact)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813485Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:310-313:0042:1950e766", "doc_id": "DOC01", "version": "20251014", "chunk_index": 42, "block_start_index": 310, "block_end_index": 313, "text": "encryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation. Great work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact.\n\n- o Problem framing: 'Who will do what differently because of this model?'\n\n- o Stakeholders: roles (owner, approver, user, maintainer); update cadence.\n\n- o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs.\n\n- o Storytelling: structure (context → insight → action → impact), visuals, FAQs.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\nencryption at rest/in transit, abuse/poisoning risks. - o Governance: approvals, audit trails, Model Risk Management (MRM). - o Add subgroup performance tables to every evaluation; write a model card. - o Run a privacy review: identify sensitive fields; propose minimization. - Pitfalls - o One-time 'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation. Great work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact.\n\n- o Problem framing: 'Who will do what differently because of this model?'\n\n- o Stakeholders: roles (owner, approver, user, maintainer); update cadence.\n\n- o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs.\n\n- o Storytelling: structure (context → insight → action → impact), visuals, FAQs.", "token_count": 193, "embedding_token_count": 210, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813684Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:315-316:0043:5105b3c1", "doc_id": "DOC01", "version": "20251014", "chunk_index": 43, "block_start_index": 315, "block_end_index": 316, "text": "'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation. Great work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs.\n\n- o Write a one-page problem charter and a post-pilot readout with KPIs.\n\n- o Define threshold rules tied to business costs; simulate outcomes.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Practice\n\n'ethics check'; ignoring high-risk slices (e.g., low-prevalence groups). - o Model card; risk assessment; access logs; DPA/consent documentation. Great work fails without adoption. Tie models to decisions, tell a clear story, and measure real impact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs.\n\n- o Write a one-page problem charter and a post-pilot readout with KPIs.\n\n- o Define threshold rules tied to business costs; simulate outcomes.", "token_count": 160, "embedding_token_count": 177, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Practice"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Practice", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Practice", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.813855Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:318-318:0044:945e2d2a", "doc_id": "DOC01", "version": "20251014", "chunk_index": 44, "block_start_index": 318, "block_end_index": 318, "text": "models to decisions, tell a clear story, and measure real impact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes.\n\n- o Optimizing vanity metrics; ignoring adoption &amp; change management.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Pitfalls\n\nmodels to decisions, tell a clear story, and measure real impact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes.\n\n- o Optimizing vanity metrics; ignoring adoption &amp; change management.", "token_count": 129, "embedding_token_count": 146, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Pitfalls"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Pitfalls", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Pitfalls", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.814012Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:320-320:0045:318ca5ec", "doc_id": "DOC01", "version": "20251014", "chunk_index": 45, "block_start_index": 320, "block_end_index": 320, "text": "impact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management.\n\n- o Problem charter; executive one-pager; dashboard link; rollout plan.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Artifacts\n\nimpact. - o Problem framing: 'Who will do what differently because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management.\n\n- o Problem charter; executive one-pager; dashboard link; rollout plan.", "token_count": 133, "embedding_token_count": 150, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Artifacts"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Artifacts", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Artifacts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.814158Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:322-322:0046:aa90d041", "doc_id": "DOC01", "version": "20251014", "chunk_index": 46, "block_start_index": 322, "block_end_index": 322, "text": "because of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management. - o Problem charter; executive one-pager; dashboard link; rollout plan.\n\nData Analysts turn raw tables into decisions people can act on daily. They specialize in cleaning data, building reports, and answering 'what happened/why' with clear visuals and narratives.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data Analyst - BI &amp; reporting\n\nbecause of this model?' - o Stakeholders: roles (owner, approver, user, maintainer); update cadence. - o Decision economics: cost-benefit, thresholds, capacity constraints, SLAs. - o Storytelling: structure (context → insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management. - o Problem charter; executive one-pager; dashboard link; rollout plan.\n\nData Analysts turn raw tables into decisions people can act on daily. They specialize in cleaning data, building reports, and answering 'what happened/why' with clear visuals and narratives.", "token_count": 156, "embedding_token_count": 179, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data Analyst - BI &amp; reporting"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data Analyst - BI &amp; reporting", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data Analyst - BI &amp; reporting", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.814324Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:324-336:0047:6f78c4a5", "doc_id": "DOC01", "version": "20251014", "chunk_index": 47, "block_start_index": 324, "block_end_index": 336, "text": "→ insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management. - o Problem charter; executive one-pager; dashboard link; rollout plan. Data Analysts turn raw tables into decisions people can act on daily. They specialize in cleaning data, building reports, and answering 'what happened/why' with clear visuals and narratives.\n\n- o Pull, clean, and join data from multiple sources.\n\n- o Build reusable dashboards and ad-hoc analyses.\n\n- o Define/maintain business definitions (metrics, dimensions).\n\n- o Partner with business owners to translate questions into queries.\n\n- Typical outputs\n\n- o KPI dashboards, cohort/funnel reports, weekly business reviews.\n\n- o Deep-dives: variance analysis, seasonality, anomaly investigations.\n\n- Tools &amp; skills\n\n- o SQL (joins, window functions), Excel/Sheets, BI tools (Power BI/Tableau/Looker).\n\n- o Basic stats (averages, CIs), data modeling (star/snowflake).\n\n- How success is measured\n\n- o Report accuracy and freshness, stakeholder adoption, reduced time-to-insight.\n\n- o Clear documentation and consistent metric definitions.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Core responsibilities\n\n→ insight → action → impact), visuals, FAQs. - o Write a one-page problem charter and a post-pilot readout with KPIs. - o Define threshold rules tied to business costs; simulate outcomes. - o Optimizing vanity metrics; ignoring adoption &amp; change management. - o Problem charter; executive one-pager; dashboard link; rollout plan. Data Analysts turn raw tables into decisions people can act on daily. They specialize in cleaning data, building reports, and answering 'what happened/why' with clear visuals and narratives.\n\n- o Pull, clean, and join data from multiple sources.\n\n- o Build reusable dashboards and ad-hoc analyses.\n\n- o Define/maintain business definitions (metrics, dimensions).\n\n- o Partner with business owners to translate questions into queries.\n\n- Typical outputs\n\n- o KPI dashboards, cohort/funnel reports, weekly business reviews.\n\n- o Deep-dives: variance analysis, seasonality, anomaly investigations.\n\n- Tools &amp; skills\n\n- o SQL (joins, window functions), Excel/Sheets, BI tools (Power BI/Tableau/Looker).\n\n- o Basic stats (averages, CIs), data modeling (star/snowflake).\n\n- How success is measured\n\n- o Report accuracy and freshness, stakeholder adoption, reduced time-to-insight.\n\n- o Clear documentation and consistent metric definitions.", "token_count": 269, "embedding_token_count": 287, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Core responsibilities"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Core responsibilities", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Core responsibilities", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.814601Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:338-349:0048:ea6ebddf", "doc_id": "DOC01", "version": "20251014", "chunk_index": 48, "block_start_index": 338, "block_end_index": 349, "text": "(metrics, dimensions). - o Partner with business owners to translate questions into queries. - Typical outputs - o KPI dashboards, cohort/funnel reports, weekly business reviews. - o Deep-dives: variance analysis, seasonality, anomaly investigations. - Tools &amp; skills - o SQL (joins, window functions), Excel/Sheets, BI tools (Power BI/Tableau/Looker). - o Basic stats (averages, CIs), data modeling (star/snowflake). - How success is measured - o Report accuracy and freshness, stakeholder adoption, reduced time-to-insight. - o Clear documentation and consistent metric definitions.\n\nData Scientists build and validate predictive/causal models, moving from EDA to prototypes that inform or automate decisions.\n\n- Core responsibilities\n\n- o Frame problems, run EDA, engineer features, train/evaluate models.\n\n- o Design/interpret experiments (A/B tests), communicate findings.\n\n- o Collaborate with engineers to hand off models or decision rules.\n\n- Typical outputs\n\n- o Notebooks, model artifacts/pipelines, experiment readouts, model cards.\n\n- Tools &amp; skills\n\n- o Python, pandas/NumPy, scikit-learn; stats (hypothesis testing), ML basics.\n\n- o Versioning/experimentation (Git, MLflow/W&amp;B).\n\n- How success is measured\n\n- o Offline/online metric lift (F1/AUC/MAE, business KPIs), clarity of insights, reproducibility.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data Scientist - modeling &amp; experimentation\n\n(metrics, dimensions). - o Partner with business owners to translate questions into queries. - Typical outputs - o KPI dashboards, cohort/funnel reports, weekly business reviews. - o Deep-dives: variance analysis, seasonality, anomaly investigations. - Tools &amp; skills - o SQL (joins, window functions), Excel/Sheets, BI tools (Power BI/Tableau/Looker). - o Basic stats (averages, CIs), data modeling (star/snowflake). - How success is measured - o Report accuracy and freshness, stakeholder adoption, reduced time-to-insight. - o Clear documentation and consistent metric definitions.\n\nData Scientists build and validate predictive/causal models, moving from EDA to prototypes that inform or automate decisions.\n\n- Core responsibilities\n\n- o Frame problems, run EDA, engineer features, train/evaluate models.\n\n- o Design/interpret experiments (A/B tests), communicate findings.\n\n- o Collaborate with engineers to hand off models or decision rules.\n\n- Typical outputs\n\n- o Notebooks, model artifacts/pipelines, experiment readouts, model cards.\n\n- Tools &amp; skills\n\n- o Python, pandas/NumPy, scikit-learn; stats (hypothesis testing), ML basics.\n\n- o Versioning/experimentation (Git, MLflow/W&amp;B).\n\n- How success is measured\n\n- o Offline/online metric lift (F1/AUC/MAE, business KPIs), clarity of insights, reproducibility.", "token_count": 289, "embedding_token_count": 312, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data Scientist - modeling &amp; experimentation"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data Scientist - modeling &amp; experimentation", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data Scientist - modeling &amp; experimentation", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.814840Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:351-362:0049:5b8298b2", "doc_id": "DOC01", "version": "20251014", "chunk_index": 49, "block_start_index": 351, "block_end_index": 362, "text": "Core responsibilities - o Frame problems, run EDA, engineer features, train/evaluate models. - o Design/interpret experiments (A/B tests), communicate findings. - o Collaborate with engineers to hand off models or decision rules. - Typical outputs - o Notebooks, model artifacts/pipelines, experiment readouts, model cards. - Tools &amp; skills - o Python, pandas/NumPy, scikit-learn; stats (hypothesis testing), ML basics. - o Versioning/experimentation (Git, MLflow/W&amp;B). - How success is measured - o Offline/online metric lift (F1/AUC/MAE, business KPIs), clarity of insights, reproducibility.\n\nML Engineers make models dependable in production. They package, serve, scale, and monitor ML systems.\n\n- Core responsibilities\n\n- o Convert research code to robust services/pipelines (batch/real-time).\n\n- o Optimize latency/cost; add monitoring, logging, and alerting.\n\n- o Manage model registries, CI/CD, and rollout/rollback strategies.\n\n- Typical outputs\n\n- o APIs (FastAPI/gRPC), batch jobs, feature and inference pipelines, infra-as-code.\n\n- Tools &amp; skills\n\n- o Python, Docker, CI/CD, cloud (AWS/GCP/Azure), orchestration (Airflow), feature stores.\n\n- o Observability (Prometheus/Grafana), testing (unit/integration).\n\n- How success is measured\n\n- o SLOs (p95 latency, uptime), data/metric drift detection, time-to-deploy, reliability.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > ML Engineer - productionizing models\n\nCore responsibilities - o Frame problems, run EDA, engineer features, train/evaluate models. - o Design/interpret experiments (A/B tests), communicate findings. - o Collaborate with engineers to hand off models or decision rules. - Typical outputs - o Notebooks, model artifacts/pipelines, experiment readouts, model cards. - Tools &amp; skills - o Python, pandas/NumPy, scikit-learn; stats (hypothesis testing), ML basics. - o Versioning/experimentation (Git, MLflow/W&amp;B). - How success is measured - o Offline/online metric lift (F1/AUC/MAE, business KPIs), clarity of insights, reproducibility.\n\nML Engineers make models dependable in production. They package, serve, scale, and monitor ML systems.\n\n- Core responsibilities\n\n- o Convert research code to robust services/pipelines (batch/real-time).\n\n- o Optimize latency/cost; add monitoring, logging, and alerting.\n\n- o Manage model registries, CI/CD, and rollout/rollback strategies.\n\n- Typical outputs\n\n- o APIs (FastAPI/gRPC), batch jobs, feature and inference pipelines, infra-as-code.\n\n- Tools &amp; skills\n\n- o Python, Docker, CI/CD, cloud (AWS/GCP/Azure), orchestration (Airflow), feature stores.\n\n- o Observability (Prometheus/Grafana), testing (unit/integration).\n\n- How success is measured\n\n- o SLOs (p95 latency, uptime), data/metric drift detection, time-to-deploy, reliability.", "token_count": 314, "embedding_token_count": 334, "section_path": ["Definition &amp; scope (what DS is / isn't)", "ML Engineer - productionizing models"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > ML Engineer - productionizing models", "section_group_id": "Definition &amp; scope (what DS is / isn't)|ML Engineer - productionizing models", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.815088Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:364-375:0050:ea593461", "doc_id": "DOC01", "version": "20251014", "chunk_index": 50, "block_start_index": 364, "block_end_index": 375, "text": "Core responsibilities - o Convert research code to robust services/pipelines (batch/real-time). - o Optimize latency/cost; add monitoring, logging, and alerting. - o Manage model registries, CI/CD, and rollout/rollback strategies. - Typical outputs - o APIs (FastAPI/gRPC), batch jobs, feature and inference pipelines, infra-as-code. - Tools &amp; skills - o Python, Docker, CI/CD, cloud (AWS/GCP/Azure), orchestration (Airflow), feature stores. - o Observability (Prometheus/Grafana), testing (unit/integration). - How success is measured - o SLOs (p95 latency, uptime), data/metric drift detection, time-to-deploy, reliability.\n\nData Engineers build the plumbing: reliable, scalable data flows and storage so others can analyze and model.\n\n- Core responsibilities\n\n- o Design schemas; build ETL/ELT pipelines (batch/streaming).\n\n- o Ensure quality (validation, SLAs) and lineage.\n\n- o Optimize performance and costs across lake/warehouse.\n\n- Typical outputs\n\n- o Curated tables/models, ingestion jobs, documentation, data contracts.\n\n- Tools &amp; skills\n\n- o SQL, PySpark/Spark, dbt, Kafka/PubSub, cloud data stacks (BigQuery/Redshift/Snowflake).\n\n- o Parquet/Delta, partitioning, orchestration (Airflow/Prefect).\n\n- How success is measured\n\n- o Data freshness/availability, query performance, reliability, cost efficiency.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data Engineer - data pipelines &amp; platforms\n\nCore responsibilities - o Convert research code to robust services/pipelines (batch/real-time). - o Optimize latency/cost; add monitoring, logging, and alerting. - o Manage model registries, CI/CD, and rollout/rollback strategies. - Typical outputs - o APIs (FastAPI/gRPC), batch jobs, feature and inference pipelines, infra-as-code. - Tools &amp; skills - o Python, Docker, CI/CD, cloud (AWS/GCP/Azure), orchestration (Airflow), feature stores. - o Observability (Prometheus/Grafana), testing (unit/integration). - How success is measured - o SLOs (p95 latency, uptime), data/metric drift detection, time-to-deploy, reliability.\n\nData Engineers build the plumbing: reliable, scalable data flows and storage so others can analyze and model.\n\n- Core responsibilities\n\n- o Design schemas; build ETL/ELT pipelines (batch/streaming).\n\n- o Ensure quality (validation, SLAs) and lineage.\n\n- o Optimize performance and costs across lake/warehouse.\n\n- Typical outputs\n\n- o Curated tables/models, ingestion jobs, documentation, data contracts.\n\n- Tools &amp; skills\n\n- o SQL, PySpark/Spark, dbt, Kafka/PubSub, cloud data stacks (BigQuery/Redshift/Snowflake).\n\n- o Parquet/Delta, partitioning, orchestration (Airflow/Prefect).\n\n- How success is measured\n\n- o Data freshness/availability, query performance, reliability, cost efficiency.", "token_count": 301, "embedding_token_count": 325, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data Engineer - data pipelines &amp; platforms"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data Engineer - data pipelines &amp; platforms", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data Engineer - data pipelines &amp; platforms", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.815343Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:377-387:0051:f57f3f18", "doc_id": "DOC01", "version": "20251014", "chunk_index": 51, "block_start_index": 377, "block_end_index": 387, "text": "storage so others can analyze and model. - Core responsibilities - o Design schemas; build ETL/ELT pipelines (batch/streaming). - o Ensure quality (validation, SLAs) and lineage. - o Optimize performance and costs across lake/warehouse. - Typical outputs - o Curated tables/models, ingestion jobs, documentation, data contracts. - Tools &amp; skills - o SQL, PySpark/Spark, dbt, Kafka/PubSub, cloud data stacks (BigQuery/Redshift/Snowflake). - o Parquet/Delta, partitioning, orchestration (Airflow/Prefect). - How success is measured - o Data freshness/availability, query performance, reliability, cost efficiency.\n\nAnalytics Engineers sit between DE and BI. They model business logic in SQL/dbt so metrics are consistent and dashboards stay fast.\n\n- Core responsibilities\n\n- o Translate business rules into tested SQL models.\n\n- o Maintain semantic layers/metric definitions; reduce duplication.\n\n- o Enable self-serve analytics via clean, documented datasets.\n\n- Typical outputs\n\n- o dbt projects, semantic layers/metrics, BI-ready tables and documentation.\n\n- Tools &amp; skills\n\n- o SQL, dbt, Git, BI tools (Looker/Power BI/Tableau), data testing (Great Expectations/dbt tests).\n\n- How success is measured\n\n- o Data model clarity, test coverage, dashboard performance, stakeholder trust.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Analytics Engineer - transforming for BI\n\nstorage so others can analyze and model. - Core responsibilities - o Design schemas; build ETL/ELT pipelines (batch/streaming). - o Ensure quality (validation, SLAs) and lineage. - o Optimize performance and costs across lake/warehouse. - Typical outputs - o Curated tables/models, ingestion jobs, documentation, data contracts. - Tools &amp; skills - o SQL, PySpark/Spark, dbt, Kafka/PubSub, cloud data stacks (BigQuery/Redshift/Snowflake). - o Parquet/Delta, partitioning, orchestration (Airflow/Prefect). - How success is measured - o Data freshness/availability, query performance, reliability, cost efficiency.\n\nAnalytics Engineers sit between DE and BI. They model business logic in SQL/dbt so metrics are consistent and dashboards stay fast.\n\n- Core responsibilities\n\n- o Translate business rules into tested SQL models.\n\n- o Maintain semantic layers/metric definitions; reduce duplication.\n\n- o Enable self-serve analytics via clean, documented datasets.\n\n- Typical outputs\n\n- o dbt projects, semantic layers/metrics, BI-ready tables and documentation.\n\n- Tools &amp; skills\n\n- o SQL, dbt, Git, BI tools (Looker/Power BI/Tableau), data testing (Great Expectations/dbt tests).\n\n- How success is measured\n\n- o Data model clarity, test coverage, dashboard performance, stakeholder trust.", "token_count": 272, "embedding_token_count": 293, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Analytics Engineer - transforming for BI"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Analytics Engineer - transforming for BI", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Analytics Engineer - transforming for BI", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.815568Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:389-399:0052:d00af3a7", "doc_id": "DOC01", "version": "20251014", "chunk_index": 52, "block_start_index": 389, "block_end_index": 399, "text": "and dashboards stay fast. - Core responsibilities - o Translate business rules into tested SQL models. - o Maintain semantic layers/metric definitions; reduce duplication. - o Enable self-serve analytics via clean, documented datasets. - Typical outputs - o dbt projects, semantic layers/metrics, BI-ready tables and documentation. - Tools &amp; skills - o SQL, dbt, Git, BI tools (Looker/Power BI/Tableau), data testing (Great Expectations/dbt tests). - How success is measured - o Data model clarity, test coverage, dashboard performance, stakeholder trust.\n\nMLOps Engineers create the tooling and processes that keep ML systems healthy from training to serving.\n\n- Core responsibilities\n\n- o Build/maintain model registries, pipelines, and automated retraining.\n\n- o Implement monitoring for data quality, performance, and drift.\n\n- o Govern versions, approvals, and compliance for model changes.\n\n- Typical outputs\n\n- o Training/serving pipelines, monitoring dashboards, incident runbooks.\n\n- Tools &amp; skills\n\n- o MLflow/SageMaker/Vertex, Docker/Kubernetes, CI/CD, feature stores, terraform/IaC.\n\n- How success is measured\n\n- o Deployment frequency/lead time, rollback safety, drift detection speed, audit readiness.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > MLOps Engineer - deployment &amp; monitoring at scale\n\nand dashboards stay fast. - Core responsibilities - o Translate business rules into tested SQL models. - o Maintain semantic layers/metric definitions; reduce duplication. - o Enable self-serve analytics via clean, documented datasets. - Typical outputs - o dbt projects, semantic layers/metrics, BI-ready tables and documentation. - Tools &amp; skills - o SQL, dbt, Git, BI tools (Looker/Power BI/Tableau), data testing (Great Expectations/dbt tests). - How success is measured - o Data model clarity, test coverage, dashboard performance, stakeholder trust.\n\nMLOps Engineers create the tooling and processes that keep ML systems healthy from training to serving.\n\n- Core responsibilities\n\n- o Build/maintain model registries, pipelines, and automated retraining.\n\n- o Implement monitoring for data quality, performance, and drift.\n\n- o Govern versions, approvals, and compliance for model changes.\n\n- Typical outputs\n\n- o Training/serving pipelines, monitoring dashboards, incident runbooks.\n\n- Tools &amp; skills\n\n- o MLflow/SageMaker/Vertex, Docker/Kubernetes, CI/CD, feature stores, terraform/IaC.\n\n- How success is measured\n\n- o Deployment frequency/lead time, rollback safety, drift detection speed, audit readiness.", "token_count": 246, "embedding_token_count": 271, "section_path": ["Definition &amp; scope (what DS is / isn't)", "MLOps Engineer - deployment &amp; monitoring at scale"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > MLOps Engineer - deployment &amp; monitoring at scale", "section_group_id": "Definition &amp; scope (what DS is / isn't)|MLOps Engineer - deployment &amp; monitoring at scale", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.815842Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:401-410:0053:42a1845e", "doc_id": "DOC01", "version": "20251014", "chunk_index": 53, "block_start_index": 401, "block_end_index": 410, "text": "keep ML systems healthy from training to serving. - Core responsibilities - o Build/maintain model registries, pipelines, and automated retraining. - o Implement monitoring for data quality, performance, and drift. - o Govern versions, approvals, and compliance for model changes. - Typical outputs - o Training/serving pipelines, monitoring dashboards, incident runbooks. - Tools &amp; skills - o MLflow/SageMaker/Vertex, Docker/Kubernetes, CI/CD, feature stores, terraform/IaC. - How success is measured - o Deployment frequency/lead time, rollback safety, drift detection speed, audit readiness.\n\nApplied/Research Scientists push capability frontiers (e.g., GenAI, CV/NLP), then adapt them to products.\n\n- Core responsibilities\n\n- o Explore new architectures, pretrain/fine-tune models, run benchmarks.\n\n- o Publish/internal tech notes; collaborate on productization with MLEs.\n\n- Typical outputs\n\n- o SOTA prototypes, evaluation suites, papers/tech reports, reusable model libs.\n\n- Tools &amp; skills\n\n- o PyTorch/JAX/TF, CUDA, distributed training, evaluation design, literature review.\n\n- How success is measured\n\n- o Research quality (benchmarks, novelty), transfer to product impact, reusable components.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Applied/Research Scientist - advanced methods/GenAI\n\nkeep ML systems healthy from training to serving. - Core responsibilities - o Build/maintain model registries, pipelines, and automated retraining. - o Implement monitoring for data quality, performance, and drift. - o Govern versions, approvals, and compliance for model changes. - Typical outputs - o Training/serving pipelines, monitoring dashboards, incident runbooks. - Tools &amp; skills - o MLflow/SageMaker/Vertex, Docker/Kubernetes, CI/CD, feature stores, terraform/IaC. - How success is measured - o Deployment frequency/lead time, rollback safety, drift detection speed, audit readiness.\n\nApplied/Research Scientists push capability frontiers (e.g., GenAI, CV/NLP), then adapt them to products.\n\n- Core responsibilities\n\n- o Explore new architectures, pretrain/fine-tune models, run benchmarks.\n\n- o Publish/internal tech notes; collaborate on productization with MLEs.\n\n- Typical outputs\n\n- o SOTA prototypes, evaluation suites, papers/tech reports, reusable model libs.\n\n- Tools &amp; skills\n\n- o PyTorch/JAX/TF, CUDA, distributed training, evaluation design, literature review.\n\n- How success is measured\n\n- o Research quality (benchmarks, novelty), transfer to product impact, reusable components.", "token_count": 247, "embedding_token_count": 271, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Applied/Research Scientist - advanced methods/GenAI"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Applied/Research Scientist - advanced methods/GenAI", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Applied/Research Scientist - advanced methods/GenAI", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.816177Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:412-423:0054:4d17ff46", "doc_id": "DOC01", "version": "20251014", "chunk_index": 54, "block_start_index": 412, "block_end_index": 423, "text": "Applied/Research Scientists push capability frontiers (e.g., GenAI, CV/NLP), then adapt them to products. - Core responsibilities - o Explore new architectures, pretrain/fine-tune models, run benchmarks. - o Publish/internal tech notes; collaborate on productization with MLEs. - Typical outputs - o SOTA prototypes, evaluation suites, papers/tech reports, reusable model libs. - Tools &amp; skills - o PyTorch/JAX/TF, CUDA, distributed training, evaluation design, literature review. - How success is measured - o Research quality (benchmarks, novelty), transfer to product impact, reusable components.\n\nGreat DS starts with a clear decision to improve. You translate vague asks ('reduce churn') into a concrete objective, target user, success metric, constraints, and a plan that can be tested and owned after launch.\n\n- Learn\n\n- o Define the decision, the user, and the action that will change because of your model.\n\n- o Specify target variable, unit of analysis, population, horizon, and constraints (latency, budget, privacy).\n\n- o Map success: business KPI ↔ ML metric; cost of errors; capacity limits (e.g., top-k outreach).\n\n- Practice\n\n- o Rewrite ambiguous prompts into a one-page problem charter with SMART goals and acceptance criteria.\n\n- o Build a cost matrix for false positives/negatives and a rule of engagement (who acts, when).\n\n- Pitfalls\n\n- o Optimizing the wrong metric; skipping baselines; 'boil-the-ocean' scope; starting with data before the decision.\n\n- Artifacts\n\n- o Problem charter, metric/guardrail table, baseline definition, decision playbook.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Problem framing &amp; business sense (turn questions into measurable objectives)\n\nApplied/Research Scientists push capability frontiers (e.g., GenAI, CV/NLP), then adapt them to products. - Core responsibilities - o Explore new architectures, pretrain/fine-tune models, run benchmarks. - o Publish/internal tech notes; collaborate on productization with MLEs. - Typical outputs - o SOTA prototypes, evaluation suites, papers/tech reports, reusable model libs. - Tools &amp; skills - o PyTorch/JAX/TF, CUDA, distributed training, evaluation design, literature review. - How success is measured - o Research quality (benchmarks, novelty), transfer to product impact, reusable components.\n\nGreat DS starts with a clear decision to improve. You translate vague asks ('reduce churn') into a concrete objective, target user, success metric, constraints, and a plan that can be tested and owned after launch.\n\n- Learn\n\n- o Define the decision, the user, and the action that will change because of your model.\n\n- o Specify target variable, unit of analysis, population, horizon, and constraints (latency, budget, privacy).\n\n- o Map success: business KPI ↔ ML metric; cost of errors; capacity limits (e.g., top-k outreach).\n\n- Practice\n\n- o Rewrite ambiguous prompts into a one-page problem charter with SMART goals and acceptance criteria.\n\n- o Build a cost matrix for false positives/negatives and a rule of engagement (who acts, when).\n\n- Pitfalls\n\n- o Optimizing the wrong metric; skipping baselines; 'boil-the-ocean' scope; starting with data before the decision.\n\n- Artifacts\n\n- o Problem charter, metric/guardrail table, baseline definition, decision playbook.", "token_count": 337, "embedding_token_count": 366, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Problem framing &amp; business sense (turn questions into measurable objectives)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Problem framing &amp; business sense (turn questions into measurable objectives)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Problem framing &amp; business sense (turn questions into measurable objectives)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.816532Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:425-436:0055:f0a3b2d6", "doc_id": "DOC01", "version": "20251014", "chunk_index": 55, "block_start_index": 425, "block_end_index": 436, "text": "Map success: business KPI ↔ ML metric; cost of errors; capacity limits (e.g., top-k outreach). - Practice - o Rewrite ambiguous prompts into a one-page problem charter with SMART goals and acceptance criteria. - o Build a cost matrix for false positives/negatives and a rule of engagement (who acts, when). - Pitfalls - o Optimizing the wrong metric; skipping baselines; 'boil-the-ocean' scope; starting with data before the decision. - Artifacts - o Problem charter, metric/guardrail table, baseline definition, decision playbook.\n\nPython is how you transform data and build models; SQL is how you fetch and shape data at the source. Clean code and efficient queries save hours weekly and reduce bugs.\n\n- Learn\n\n- o Python: functions, modules, exceptions, typing, pathlib, packaging, virtual envs; pandas/NumPy idioms.\n\n- o SQL: JOINs, GROUP BY/HAVING, subqueries/CTEs, window functions (LAG/LEAD/RANK), indexing basics.\n\n- o Style/discipline: PEP8, docstrings, small pure functions, profiling.\n\n- Practice\n\n- o Build an idempotent Python script: read → clean → join → validate → write Parquet.\n\n- o Solve 5 JOIN problems (one-to-many, many-to-many) and 5 window-function tasks (rolling sums, de-dupe).\n\n- Pitfalls\n\n- o pandas SettingWithCopy, accidental cartesian joins, SELECT * in prod queries, unbounded window specs.\n\n- Artifacts\n\n- o utils.py helpers, requirements.txt, SQL 'cookbook' of reusable snippets.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Python &amp; SQL fundamentals (clean, readable code; efficient queries)\n\nMap success: business KPI ↔ ML metric; cost of errors; capacity limits (e.g., top-k outreach). - Practice - o Rewrite ambiguous prompts into a one-page problem charter with SMART goals and acceptance criteria. - o Build a cost matrix for false positives/negatives and a rule of engagement (who acts, when). - Pitfalls - o Optimizing the wrong metric; skipping baselines; 'boil-the-ocean' scope; starting with data before the decision. - Artifacts - o Problem charter, metric/guardrail table, baseline definition, decision playbook.\n\nPython is how you transform data and build models; SQL is how you fetch and shape data at the source. Clean code and efficient queries save hours weekly and reduce bugs.\n\n- Learn\n\n- o Python: functions, modules, exceptions, typing, pathlib, packaging, virtual envs; pandas/NumPy idioms.\n\n- o SQL: JOINs, GROUP BY/HAVING, subqueries/CTEs, window functions (LAG/LEAD/RANK), indexing basics.\n\n- o Style/discipline: PEP8, docstrings, small pure functions, profiling.\n\n- Practice\n\n- o Build an idempotent Python script: read → clean → join → validate → write Parquet.\n\n- o Solve 5 JOIN problems (one-to-many, many-to-many) and 5 window-function tasks (rolling sums, de-dupe).\n\n- Pitfalls\n\n- o pandas SettingWithCopy, accidental cartesian joins, SELECT * in prod queries, unbounded window specs.\n\n- Artifacts\n\n- o utils.py helpers, requirements.txt, SQL 'cookbook' of reusable snippets.", "token_count": 318, "embedding_token_count": 348, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Python &amp; SQL fundamentals (clean, readable code; efficient queries)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Python &amp; SQL fundamentals (clean, readable code; efficient queries)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Python &amp; SQL fundamentals (clean, readable code; efficient queries)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.816837Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:438-450:0056:c8db2006", "doc_id": "DOC01", "version": "20251014", "chunk_index": 56, "block_start_index": 438, "block_end_index": 450, "text": "GROUP BY/HAVING, subqueries/CTEs, window functions (LAG/LEAD/RANK), indexing basics. - o Style/discipline: PEP8, docstrings, small pure functions, profiling. - Practice - o Build an idempotent Python script: read → clean → join → validate → write Parquet. - o Solve 5 JOIN problems (one-to-many, many-to-many) and 5 window-function tasks (rolling sums, de-dupe). - Pitfalls - o pandas SettingWithCopy, accidental cartesian joins, SELECT * in prod queries, unbounded window specs. - Artifacts - o utils.py helpers, requirements.txt, SQL 'cookbook' of reusable snippets.\n\nStats is how you reason under uncertainty: what's signal vs noise, and how confident you are. It powers A/B tests, intervals, and understanding model limits.\n\n- Learn\n\n- o Distributions (Bernoulli/Binomial/Normal/Poisson), CLT, sampling distributions.\n\n- o Estimation: confidence intervals, effect size, power &amp; sample size.\n\n- o Hypothesis testing: null/alternative, p-values, Type I/II errors; multiple-testing corrections.\n\n- o Bias vs variance; correlation vs causation; confounding and Simpson's paradox.\n\n- Practice\n\n- o Write an A/B readout: point estimate + 95% CI + p-value + plain-English implication.\n\n- o Bootstrap a CI for a metric; simulate CLT to see means stabilize.\n\n- Pitfalls\n\n- o 'p &lt; 0.05 ⇒ truth' fallacy; p-hacking; ignoring confounders; reporting only averages.\n\n- Artifacts\n\n- o Stats cheat sheet, experiment template, quick sample-size calculator.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Statistics intuition (uncertainty, tests, bias/variance)\n\nGROUP BY/HAVING, subqueries/CTEs, window functions (LAG/LEAD/RANK), indexing basics. - o Style/discipline: PEP8, docstrings, small pure functions, profiling. - Practice - o Build an idempotent Python script: read → clean → join → validate → write Parquet. - o Solve 5 JOIN problems (one-to-many, many-to-many) and 5 window-function tasks (rolling sums, de-dupe). - Pitfalls - o pandas SettingWithCopy, accidental cartesian joins, SELECT * in prod queries, unbounded window specs. - Artifacts - o utils.py helpers, requirements.txt, SQL 'cookbook' of reusable snippets.\n\nStats is how you reason under uncertainty: what's signal vs noise, and how confident you are. It powers A/B tests, intervals, and understanding model limits.\n\n- Learn\n\n- o Distributions (Bernoulli/Binomial/Normal/Poisson), CLT, sampling distributions.\n\n- o Estimation: confidence intervals, effect size, power &amp; sample size.\n\n- o Hypothesis testing: null/alternative, p-values, Type I/II errors; multiple-testing corrections.\n\n- o Bias vs variance; correlation vs causation; confounding and Simpson's paradox.\n\n- Practice\n\n- o Write an A/B readout: point estimate + 95% CI + p-value + plain-English implication.\n\n- o Bootstrap a CI for a metric; simulate CLT to see means stabilize.\n\n- Pitfalls\n\n- o 'p &lt; 0.05 ⇒ truth' fallacy; p-hacking; ignoring confounders; reporting only averages.\n\n- Artifacts\n\n- o Stats cheat sheet, experiment template, quick sample-size calculator.", "token_count": 336, "embedding_token_count": 362, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Statistics intuition (uncertainty, tests, bias/variance)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Statistics intuition (uncertainty, tests, bias/variance)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Statistics intuition (uncertainty, tests, bias/variance)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.817168Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:452-463:0057:a68b03a4", "doc_id": "DOC01", "version": "20251014", "chunk_index": 57, "block_start_index": 452, "block_end_index": 463, "text": "testing: null/alternative, p-values, Type I/II errors; multiple-testing corrections. - o Bias vs variance; correlation vs causation; confounding and Simpson's paradox. - Practice - o Write an A/B readout: point estimate + 95% CI + p-value + plain-English implication. - o Bootstrap a CI for a metric; simulate CLT to see means stabilize. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; p-hacking; ignoring confounders; reporting only averages. - Artifacts - o Stats cheat sheet, experiment template, quick sample-size calculator.\n\nYou'll spend most time here. Wrangling and EDA expose quality problems, suggest features, and prevent bad surprises later.\n\n- Learn\n\n- o Cleaning: types/units, parsing dates, deduping, missingness patterns, outlier handling (winsorize/clip).\n\n- o Joining: keys/grain, surrogate keys, time-aware joins, anti/semijoins for QA.\n\n- o EDA: distributions, pairplots, imbalance checks, leakage/drift detection, target-feature relationships.\n\n- Practice\n\n- o Produce a one-page EDA: 5 key charts + 5 'so-what' insights + a data dictionary.\n\n- o Add assertive checks (row counts, null rates, unique keys) that fail fast.\n\n- Pitfalls\n\n- o Joining at the wrong grain, silent parse errors, using future information, ignoring drift over time.\n\n- Artifacts\n\n- o EDA notebook &amp; images, data-quality report, feature/column dictionary.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Data wrangling &amp; EDA (cleaning, joining, spotting issues)\n\ntesting: null/alternative, p-values, Type I/II errors; multiple-testing corrections. - o Bias vs variance; correlation vs causation; confounding and Simpson's paradox. - Practice - o Write an A/B readout: point estimate + 95% CI + p-value + plain-English implication. - o Bootstrap a CI for a metric; simulate CLT to see means stabilize. - Pitfalls - o 'p &lt; 0.05 ⇒ truth' fallacy; p-hacking; ignoring confounders; reporting only averages. - Artifacts - o Stats cheat sheet, experiment template, quick sample-size calculator.\n\nYou'll spend most time here. Wrangling and EDA expose quality problems, suggest features, and prevent bad surprises later.\n\n- Learn\n\n- o Cleaning: types/units, parsing dates, deduping, missingness patterns, outlier handling (winsorize/clip).\n\n- o Joining: keys/grain, surrogate keys, time-aware joins, anti/semijoins for QA.\n\n- o EDA: distributions, pairplots, imbalance checks, leakage/drift detection, target-feature relationships.\n\n- Practice\n\n- o Produce a one-page EDA: 5 key charts + 5 'so-what' insights + a data dictionary.\n\n- o Add assertive checks (row counts, null rates, unique keys) that fail fast.\n\n- Pitfalls\n\n- o Joining at the wrong grain, silent parse errors, using future information, ignoring drift over time.\n\n- Artifacts\n\n- o EDA notebook &amp; images, data-quality report, feature/column dictionary.", "token_count": 309, "embedding_token_count": 338, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Data wrangling &amp; EDA (cleaning, joining, spotting issues)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Data wrangling &amp; EDA (cleaning, joining, spotting issues)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Data wrangling &amp; EDA (cleaning, joining, spotting issues)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.817469Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:465-476:0058:985a7a61", "doc_id": "DOC01", "version": "20251014", "chunk_index": 58, "block_start_index": 465, "block_end_index": 476, "text": "time-aware joins, anti/semijoins for QA. - o EDA: distributions, pairplots, imbalance checks, leakage/drift detection, target-feature relationships. - Practice - o Produce a one-page EDA: 5 key charts + 5 'so-what' insights + a data dictionary. - o Add assertive checks (row counts, null rates, unique keys) that fail fast. - Pitfalls - o Joining at the wrong grain, silent parse errors, using future information, ignoring drift over time. - Artifacts - o EDA notebook &amp; images, data-quality report, feature/column dictionary.\n\nPick the simplest model that solves the problem, beat a baseline, validate correctly, and document trade-offs.\n\n- Learn\n\n- o Task ↔ method: linear/logistic for simple &amp; interpretable; trees/GBM for nonlinear tabular; kmeans/PCA for structure.\n\n- o Workflow: split → baseline → features → model → tune → evaluate → calibrate (if probabilities matter).\n\n- o Key knobs: regularization, class weights, early stopping; calibration curves.\n\n- Practice\n\n- o Build a naive baseline then beat it with 2 models; log metrics and rationale.\n\n- o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves.\n\n- Pitfalls\n\n- o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains.\n\n- Artifacts\n\n- o Experiment log, saved pipeline, model card (purpose, data, metrics, limits).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Model basics (when to use what; evaluate properly)\n\ntime-aware joins, anti/semijoins for QA. - o EDA: distributions, pairplots, imbalance checks, leakage/drift detection, target-feature relationships. - Practice - o Produce a one-page EDA: 5 key charts + 5 'so-what' insights + a data dictionary. - o Add assertive checks (row counts, null rates, unique keys) that fail fast. - Pitfalls - o Joining at the wrong grain, silent parse errors, using future information, ignoring drift over time. - Artifacts - o EDA notebook &amp; images, data-quality report, feature/column dictionary.\n\nPick the simplest model that solves the problem, beat a baseline, validate correctly, and document trade-offs.\n\n- Learn\n\n- o Task ↔ method: linear/logistic for simple &amp; interpretable; trees/GBM for nonlinear tabular; kmeans/PCA for structure.\n\n- o Workflow: split → baseline → features → model → tune → evaluate → calibrate (if probabilities matter).\n\n- o Key knobs: regularization, class weights, early stopping; calibration curves.\n\n- Practice\n\n- o Build a naive baseline then beat it with 2 models; log metrics and rationale.\n\n- o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves.\n\n- Pitfalls\n\n- o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains.\n\n- Artifacts\n\n- o Experiment log, saved pipeline, model card (purpose, data, metrics, limits).", "token_count": 303, "embedding_token_count": 329, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Model basics (when to use what; evaluate properly)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Model basics (when to use what; evaluate properly)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Model basics (when to use what; evaluate properly)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.817920Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:478-478:0059:fd1b9e1c", "doc_id": "DOC01", "version": "20251014", "chunk_index": 59, "block_start_index": 478, "block_end_index": 478, "text": "→ evaluate → calibrate (if probabilities matter). - o Key knobs: regularization, class weights, early stopping; calibration curves. - Practice - o Build a naive baseline then beat it with 2 models; log metrics and rationale. - o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves. - Pitfalls - o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains. - Artifacts - o Experiment log, saved pipeline, model card (purpose, data, metrics, limits).\n\nIf others can't understand or act on your work, it doesn't matter. Communicate decisions, not just numbers.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Communication (clear narratives, simple visuals, stakeholder updates)\n\n→ evaluate → calibrate (if probabilities matter). - o Key knobs: regularization, class weights, early stopping; calibration curves. - Practice - o Build a naive baseline then beat it with 2 models; log metrics and rationale. - o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves. - Pitfalls - o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains. - Artifacts - o Experiment log, saved pipeline, model card (purpose, data, metrics, limits).\n\nIf others can't understand or act on your work, it doesn't matter. Communicate decisions, not just numbers.", "token_count": 140, "embedding_token_count": 166, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Communication (clear narratives, simple visuals, stakeholder updates)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Communication (clear narratives, simple visuals, stakeholder updates)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Communication (clear narratives, simple visuals, stakeholder updates)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.818119Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:480-487:0060:964abc11", "doc_id": "DOC01", "version": "20251014", "chunk_index": 60, "block_start_index": 480, "block_end_index": 487, "text": "curves. - Practice - o Build a naive baseline then beat it with 2 models; log metrics and rationale. - o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves. - Pitfalls - o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains. - Artifacts - o Experiment log, saved pipeline, model card (purpose, data, metrics, limits). If others can't understand or act on your work, it doesn't matter. Communicate decisions, not just numbers.\n\n- o Story shape: context → insight → action → impact , with an executive summary up top.\n\n- o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes.\n\n- o Updates: regular cadence, risks/assumptions, asks/decisions needed.\n\n- Practice\n\n- o Write a 1-page executive readout and a 3-slide deck for the same analysis.\n\n- o Turn a complex chart into a simple one with annotations and a headline.\n\n- Pitfalls\n\n- o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\ncurves. - Practice - o Build a naive baseline then beat it with 2 models; log metrics and rationale. - o Tune GBM (learning\\_rate, max\\_depth, n\\_estimators) and plot validation curves. - Pitfalls - o Leakage, time-series evaluated with random splits, using accuracy on imbalanced data, overfitting for tiny gains. - Artifacts - o Experiment log, saved pipeline, model card (purpose, data, metrics, limits). If others can't understand or act on your work, it doesn't matter. Communicate decisions, not just numbers.\n\n- o Story shape: context → insight → action → impact , with an executive summary up top.\n\n- o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes.\n\n- o Updates: regular cadence, risks/assumptions, asks/decisions needed.\n\n- Practice\n\n- o Write a 1-page executive readout and a 3-slide deck for the same analysis.\n\n- o Turn a complex chart into a simple one with annotations and a headline.\n\n- Pitfalls\n\n- o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats.", "token_count": 230, "embedding_token_count": 247, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.818363Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:489-489:0061:45ecd365", "doc_id": "DOC01", "version": "20251014", "chunk_index": 61, "block_start_index": 489, "block_end_index": 489, "text": "→ impact , with an executive summary up top. - o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes. - o Updates: regular cadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats.\n\n- o Readout template, chart style guide, stakeholder FAQ.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Artifacts\n\n→ impact , with an executive summary up top. - o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes. - o Updates: regular cadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats.\n\n- o Readout template, chart style guide, stakeholder FAQ.", "token_count": 116, "embedding_token_count": 133, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Artifacts"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Artifacts", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Artifacts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.818504Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:491-491:0062:0ac6f76c", "doc_id": "DOC01", "version": "20251014", "chunk_index": 62, "block_start_index": 491, "block_end_index": 491, "text": "- o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes. - o Updates: regular cadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats. - o Readout template, chart style guide, stakeholder FAQ.\n\nReproducibility is the 'science' in data science. Anyone should be able to rerun your work and get the same result.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Reproducibility (Git, environments, notebooks vs scripts discipline)\n\n- o Visuals: pick the right chart, label clearly, show uncertainty, avoid clutter and dual axes. - o Updates: regular cadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats. - o Readout template, chart style guide, stakeholder FAQ.\n\nReproducibility is the 'science' in data science. Anyone should be able to rerun your work and get the same result.", "token_count": 130, "embedding_token_count": 156, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Reproducibility (Git, environments, notebooks vs scripts discipline)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Reproducibility (Git, environments, notebooks vs scripts discipline)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Reproducibility (Git, environments, notebooks vs scripts discipline)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.818641Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:493-498:0063:5e7be9d3", "doc_id": "DOC01", "version": "20251014", "chunk_index": 63, "block_start_index": 493, "block_end_index": 498, "text": "cadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats. - o Readout template, chart style guide, stakeholder FAQ. Reproducibility is the 'science' in data science. Anyone should be able to rerun your work and get the same result.\n\n- o Git flow (branches, PRs, reviews), semantic commits, tagging releases.\n\n- o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow).\n\n- o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics.\n\n- Practice\n\n- o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch.\n\n- o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Learn\n\ncadence, risks/assumptions, asks/decisions needed. - Practice - o Write a 1-page executive readout and a 3-slide deck for the same analysis. - o Turn a complex chart into a simple one with annotations and a headline. - Pitfalls - o Jargon, burying the lead, showing metrics without a recommendation, hiding caveats. - o Readout template, chart style guide, stakeholder FAQ. Reproducibility is the 'science' in data science. Anyone should be able to rerun your work and get the same result.\n\n- o Git flow (branches, PRs, reviews), semantic commits, tagging releases.\n\n- o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow).\n\n- o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics.\n\n- Practice\n\n- o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch.\n\n- o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B.", "token_count": 206, "embedding_token_count": 223, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Learn"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Learn", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Learn", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.818833Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:500-500:0064:58ab9b8e", "doc_id": "DOC01", "version": "20251014", "chunk_index": 64, "block_start_index": 500, "block_end_index": 500, "text": "is the 'science' in data science. Anyone should be able to rerun your work and get the same result. - o Git flow (branches, PRs, reviews), semantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B.\n\n- o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Pitfalls\n\nis the 'science' in data science. Anyone should be able to rerun your work and get the same result. - o Git flow (branches, PRs, reviews), semantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B.\n\n- o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes.", "token_count": 145, "embedding_token_count": 162, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Pitfalls"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Pitfalls", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Pitfalls", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.819006Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:502-502:0065:56a8912d", "doc_id": "DOC01", "version": "20251014", "chunk_index": 65, "block_start_index": 502, "block_end_index": 502, "text": "and get the same result. - o Git flow (branches, PRs, reviews), semantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B. - o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes.\n\n- o Repo skeleton, CI pipeline, model registry record, runbook for reruns.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Artifacts\n\nand get the same result. - o Git flow (branches, PRs, reviews), semantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B. - o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes.\n\n- o Repo skeleton, CI pipeline, model registry record, runbook for reruns.", "token_count": 144, "embedding_token_count": 161, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Artifacts"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Artifacts", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Artifacts", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.819149Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:504-509:0066:db5d3fa6", "doc_id": "DOC01", "version": "20251014", "chunk_index": 66, "block_start_index": 504, "block_end_index": 509, "text": "semantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B. - o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes. - o Repo skeleton, CI pipeline, model registry record, runbook for reruns.\n\nFoundation models shift DS from 'predict' to 'generate &amp; reason,' enabling assistants that read, write, summarize, plan, and create images/audio/video. Multimodal models combine text, images, tables, and sensor data so one system can parse documents, inspect photos, and chat about dashboards.\n\n- Where it helps: document processing (RAG), code/data copilots, customer support, creative assets, analytics Q&amp;A.\n\n- Key enablers: prompt engineering, retrieval-augmented generation, fine-tuning/LoRA, vector databases, guardrails.\n\n- KPIs to track: task success rate, time-to-answer, hallucination rate, top-k retrieval precision/recall, cost per request.\n\n- Risks: hallucinations, data leakage via prompts, IP/privacy exposure, unpredictable latency/cost spikes.\n\n- Practical actions: start with RAG (not full fine-tune), add deterministic fallbacks, log prompts/answers, redteam prompts, maintain a prompt/model registry.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Generative AI &amp; foundation models (text, image, multimodal assistants)\n\nsemantic commits, tagging releases. - o Environments: pinned deps, lockfiles, seeds; data &amp; model versioning (DVC/MLflow). - o Structure: src/ packages, notebooks/ for exploration, config/logging/testing basics. - Practice - o Convert a notebook into a Python package + CLI entrypoint; reproduce a run from scratch. - o Track experiments (params/metrics/artifacts) with MLflow or W&amp;B. - o Hidden notebook state, 'works on my machine', magic numbers, untracked data changes. - o Repo skeleton, CI pipeline, model registry record, runbook for reruns.\n\nFoundation models shift DS from 'predict' to 'generate &amp; reason,' enabling assistants that read, write, summarize, plan, and create images/audio/video. Multimodal models combine text, images, tables, and sensor data so one system can parse documents, inspect photos, and chat about dashboards.\n\n- Where it helps: document processing (RAG), code/data copilots, customer support, creative assets, analytics Q&amp;A.\n\n- Key enablers: prompt engineering, retrieval-augmented generation, fine-tuning/LoRA, vector databases, guardrails.\n\n- KPIs to track: task success rate, time-to-answer, hallucination rate, top-k retrieval precision/recall, cost per request.\n\n- Risks: hallucinations, data leakage via prompts, IP/privacy exposure, unpredictable latency/cost spikes.\n\n- Practical actions: start with RAG (not full fine-tune), add deterministic fallbacks, log prompts/answers, redteam prompts, maintain a prompt/model registry.", "token_count": 326, "embedding_token_count": 356, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Generative AI &amp; foundation models (text, image, multimodal assistants)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Generative AI &amp; foundation models (text, image, multimodal assistants)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Generative AI &amp; foundation models (text, image, multimodal assistants)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.819465Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:511-516:0067:f122a2d7", "doc_id": "DOC01", "version": "20251014", "chunk_index": 67, "block_start_index": 511, "block_end_index": 516, "text": "inspect photos, and chat about dashboards. - Where it helps: document processing (RAG), code/data copilots, customer support, creative assets, analytics Q&amp;A. - Key enablers: prompt engineering, retrieval-augmented generation, fine-tuning/LoRA, vector databases, guardrails. - KPIs to track: task success rate, time-to-answer, hallucination rate, top-k retrieval precision/recall, cost per request. - Risks: hallucinations, data leakage via prompts, IP/privacy exposure, unpredictable latency/cost spikes. - Practical actions: start with RAG (not full fine-tune), add deterministic fallbacks, log prompts/answers, redteam prompts, maintain a prompt/model registry.\n\nBusinesses want decisions 'in the moment'-fraud checks during checkout, recommendations as a user scrolls, safety checks on a factory line. Edge deployment keeps inference close to the event for reliability and privacy.\n\n- Use cases: fraud/abuse detection, dynamic pricing, on-device quality inspection, driver/robot assistance, IoT anomaly alerts.\n\n- Tech patterns: streaming features (Kafka/PubSub), low-latency feature stores, on-device/edge runtimes (ONNX, TensorRT), async queues, canary rollouts.\n\n- SLOs/KPIs: p95/p99 latency, throughput (req/s), freshness (feature age), offline → online feature parity, alert precision@K.\n\n- Risks: feature skew vs training data, cold-start, backpressure under spikes, on-device model drift.\n\n- Practical actions: define online/offline contract for features, add shadow mode before going live, measure end-to-end latency (network+model), budget for circuit breakers &amp; fallbacks.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Real-time &amp; edge inference (low-latency decisions, streaming features)\n\ninspect photos, and chat about dashboards. - Where it helps: document processing (RAG), code/data copilots, customer support, creative assets, analytics Q&amp;A. - Key enablers: prompt engineering, retrieval-augmented generation, fine-tuning/LoRA, vector databases, guardrails. - KPIs to track: task success rate, time-to-answer, hallucination rate, top-k retrieval precision/recall, cost per request. - Risks: hallucinations, data leakage via prompts, IP/privacy exposure, unpredictable latency/cost spikes. - Practical actions: start with RAG (not full fine-tune), add deterministic fallbacks, log prompts/answers, redteam prompts, maintain a prompt/model registry.\n\nBusinesses want decisions 'in the moment'-fraud checks during checkout, recommendations as a user scrolls, safety checks on a factory line. Edge deployment keeps inference close to the event for reliability and privacy.\n\n- Use cases: fraud/abuse detection, dynamic pricing, on-device quality inspection, driver/robot assistance, IoT anomaly alerts.\n\n- Tech patterns: streaming features (Kafka/PubSub), low-latency feature stores, on-device/edge runtimes (ONNX, TensorRT), async queues, canary rollouts.\n\n- SLOs/KPIs: p95/p99 latency, throughput (req/s), freshness (feature age), offline → online feature parity, alert precision@K.\n\n- Risks: feature skew vs training data, cold-start, backpressure under spikes, on-device model drift.\n\n- Practical actions: define online/offline contract for features, add shadow mode before going live, measure end-to-end latency (network+model), budget for circuit breakers &amp; fallbacks.", "token_count": 346, "embedding_token_count": 378, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Real-time &amp; edge inference (low-latency decisions, streaming features)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Real-time &amp; edge inference (low-latency decisions, streaming features)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Real-time &amp; edge inference (low-latency decisions, streaming features)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.819794Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:518-523:0068:3e575a8c", "doc_id": "DOC01", "version": "20251014", "chunk_index": 68, "block_start_index": 518, "block_end_index": 523, "text": "pricing, on-device quality inspection, driver/robot assistance, IoT anomaly alerts. - Tech patterns: streaming features (Kafka/PubSub), low-latency feature stores, on-device/edge runtimes (ONNX, TensorRT), async queues, canary rollouts. - SLOs/KPIs: p95/p99 latency, throughput (req/s), freshness (feature age), offline → online feature parity, alert precision@K. - Risks: feature skew vs training data, cold-start, backpressure under spikes, on-device model drift. - Practical actions: define online/offline contract for features, add shadow mode before going live, measure end-to-end latency (network+model), budget for circuit breakers &amp; fallbacks.\n\nPETs let you learn from sensitive data without centralizing raw records or exposing individuals. They're becoming table stakes where regulation or trust is critical.\n\n- Approaches: federated learning (train where data lives), differential privacy (noise to protect individuals), secure enclaves/TEEs, secure aggregation, synthetic data.\n\n- When to use: healthcare/finance/education, cross-org collaborations, mobile/edge training, jurisdictions with strict residency.\n\n- KPIs: privacy budget ( ε ), utility loss vs baseline, participation rate of clients, secure aggregation coverage, audit findings.\n\n- Risks: degraded accuracy if privacy budget is too tight, complex ops, false sense of security if governance is weak.\n\n- Practical actions: classify data/PII, select PET per risk, start with DP on analytics queries, pilot FL with a narrow model, document ε and trade-offs in model cards.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Privacy-enhancing tech (federated learning, differential privacy)\n\npricing, on-device quality inspection, driver/robot assistance, IoT anomaly alerts. - Tech patterns: streaming features (Kafka/PubSub), low-latency feature stores, on-device/edge runtimes (ONNX, TensorRT), async queues, canary rollouts. - SLOs/KPIs: p95/p99 latency, throughput (req/s), freshness (feature age), offline → online feature parity, alert precision@K. - Risks: feature skew vs training data, cold-start, backpressure under spikes, on-device model drift. - Practical actions: define online/offline contract for features, add shadow mode before going live, measure end-to-end latency (network+model), budget for circuit breakers &amp; fallbacks.\n\nPETs let you learn from sensitive data without centralizing raw records or exposing individuals. They're becoming table stakes where regulation or trust is critical.\n\n- Approaches: federated learning (train where data lives), differential privacy (noise to protect individuals), secure enclaves/TEEs, secure aggregation, synthetic data.\n\n- When to use: healthcare/finance/education, cross-org collaborations, mobile/edge training, jurisdictions with strict residency.\n\n- KPIs: privacy budget ( ε ), utility loss vs baseline, participation rate of clients, secure aggregation coverage, audit findings.\n\n- Risks: degraded accuracy if privacy budget is too tight, complex ops, false sense of security if governance is weak.\n\n- Practical actions: classify data/PII, select PET per risk, start with DP on analytics queries, pilot FL with a narrow model, document ε and trade-offs in model cards.", "token_count": 325, "embedding_token_count": 351, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Privacy-enhancing tech (federated learning, differential privacy)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Privacy-enhancing tech (federated learning, differential privacy)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Privacy-enhancing tech (federated learning, differential privacy)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.820131Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:525-530:0069:c79df023", "doc_id": "DOC01", "version": "20251014", "chunk_index": 69, "block_start_index": 525, "block_end_index": 530, "text": "When to use: healthcare/finance/education, cross-org collaborations, mobile/edge training, jurisdictions with strict residency. - KPIs: privacy budget ( ε ), utility loss vs baseline, participation rate of clients, secure aggregation coverage, audit findings. - Risks: degraded accuracy if privacy budget is too tight, complex ops, false sense of security if governance is weak. - Practical actions: classify data/PII, select PET per risk, start with DP on analytics queries, pilot FL with a narrow model, document ε and trade-offs in model cards.\n\nCopilots lift productivity: they scaffold pipelines, generate SQL, write tests, and review notebooks. AutoML covers baseline modeling/tuning so scientists focus on framing, evaluation, and deployment.\n\n- Targets: SQL generation/validation, data quality rule suggestions, feature discovery, pipeline boilerplate, docstrings/tests, unit data checks.\n\n- Benefits: faster iteration, fewer trivial bugs, better onboarding, standardized scaffolds.\n\n- KPIs: cycle time (idea → first result), review defects caught, % code/tests suggested by copilot adopted, analyst hours saved.\n\n- Risks: cargo-culted code, hidden data assumptions, dependency on vendor models.\n\n- Practical actions: define 'copilot-ready' repos with templates, require human review, keep a pattern library of approved prompts, restrict copilot access to non-sensitive code/data.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Automation &amp; copilots (AutoML, code/data assistants)\n\nWhen to use: healthcare/finance/education, cross-org collaborations, mobile/edge training, jurisdictions with strict residency. - KPIs: privacy budget ( ε ), utility loss vs baseline, participation rate of clients, secure aggregation coverage, audit findings. - Risks: degraded accuracy if privacy budget is too tight, complex ops, false sense of security if governance is weak. - Practical actions: classify data/PII, select PET per risk, start with DP on analytics queries, pilot FL with a narrow model, document ε and trade-offs in model cards.\n\nCopilots lift productivity: they scaffold pipelines, generate SQL, write tests, and review notebooks. AutoML covers baseline modeling/tuning so scientists focus on framing, evaluation, and deployment.\n\n- Targets: SQL generation/validation, data quality rule suggestions, feature discovery, pipeline boilerplate, docstrings/tests, unit data checks.\n\n- Benefits: faster iteration, fewer trivial bugs, better onboarding, standardized scaffolds.\n\n- KPIs: cycle time (idea → first result), review defects caught, % code/tests suggested by copilot adopted, analyst hours saved.\n\n- Risks: cargo-culted code, hidden data assumptions, dependency on vendor models.\n\n- Practical actions: define 'copilot-ready' repos with templates, require human review, keep a pattern library of approved prompts, restrict copilot access to non-sensitive code/data.", "token_count": 274, "embedding_token_count": 302, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Automation &amp; copilots (AutoML, code/data assistants)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Automation &amp; copilots (AutoML, code/data assistants)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Automation &amp; copilots (AutoML, code/data assistants)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.820401Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:532-537:0070:70e39764", "doc_id": "DOC01", "version": "20251014", "chunk_index": 70, "block_start_index": 532, "block_end_index": 537, "text": "SQL generation/validation, data quality rule suggestions, feature discovery, pipeline boilerplate, docstrings/tests, unit data checks. - Benefits: faster iteration, fewer trivial bugs, better onboarding, standardized scaffolds. - KPIs: cycle time (idea → first result), review defects caught, % code/tests suggested by copilot adopted, analyst hours saved. - Risks: cargo-culted code, hidden data assumptions, dependency on vendor models. - Practical actions: define 'copilot-ready' repos with templates, require human review, keep a pattern library of approved prompts, restrict copilot access to non-sensitive code/data.\n\nScrutiny is rising: organizations must prove models are safe, fair, documented, and controllable. Governance turns ad-hoc ML into audited, repeatable practice.\n\n- Core components: risk tiering (use-case risk levels), approvals, model cards &amp; data sheets, change management, incident playbooks, independent validation, lineage &amp; access logs.\n\n- What to document: purpose, data sources, consent, metrics (overall + subgroups), limitations, human oversight, rollback criteria, retraining cadence.\n\n- KPIs: audit pass rate, time to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds.\n\n- Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership.\n\n- Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Regulation &amp; model governance (AI acts, audit trails, documentation)\n\nSQL generation/validation, data quality rule suggestions, feature discovery, pipeline boilerplate, docstrings/tests, unit data checks. - Benefits: faster iteration, fewer trivial bugs, better onboarding, standardized scaffolds. - KPIs: cycle time (idea → first result), review defects caught, % code/tests suggested by copilot adopted, analyst hours saved. - Risks: cargo-culted code, hidden data assumptions, dependency on vendor models. - Practical actions: define 'copilot-ready' repos with templates, require human review, keep a pattern library of approved prompts, restrict copilot access to non-sensitive code/data.\n\nScrutiny is rising: organizations must prove models are safe, fair, documented, and controllable. Governance turns ad-hoc ML into audited, repeatable practice.\n\n- Core components: risk tiering (use-case risk levels), approvals, model cards &amp; data sheets, change management, incident playbooks, independent validation, lineage &amp; access logs.\n\n- What to document: purpose, data sources, consent, metrics (overall + subgroups), limitations, human oversight, rollback criteria, retraining cadence.\n\n- KPIs: audit pass rate, time to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds.\n\n- Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership.\n\n- Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews.", "token_count": 298, "embedding_token_count": 328, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Regulation &amp; model governance (AI acts, audit trails, documentation)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Regulation &amp; model governance (AI acts, audit trails, documentation)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Regulation &amp; model governance (AI acts, audit trails, documentation)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.820704Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:539-539:0071:facb4395", "doc_id": "DOC01", "version": "20251014", "chunk_index": 71, "block_start_index": 539, "block_end_index": 539, "text": "sheets, change management, incident playbooks, independent validation, lineage &amp; access logs. - What to document: purpose, data sources, consent, metrics (overall + subgroups), limitations, human oversight, rollback criteria, retraining cadence. - KPIs: audit pass rate, time to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds. - Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership. - Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews.\n\nOrganizations fund DS because digital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Demand drivers (digital transformation, data abundance, automation ROI)\n\nsheets, change management, incident playbooks, independent validation, lineage &amp; access logs. - What to document: purpose, data sources, consent, metrics (overall + subgroups), limitations, human oversight, rollback criteria, retraining cadence. - KPIs: audit pass rate, time to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds. - Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership. - Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews.\n\nOrganizations fund DS because digital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction.", "token_count": 164, "embedding_token_count": 191, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Demand drivers (digital transformation, data abundance, automation ROI)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Demand drivers (digital transformation, data abundance, automation ROI)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Demand drivers (digital transformation, data abundance, automation ROI)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.820864Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:541-544:0072:d2e6d567", "doc_id": "DOC01", "version": "20251014", "chunk_index": 72, "block_start_index": 541, "block_end_index": 544, "text": "time to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds. - Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership. - Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews. Organizations fund DS because digital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction.\n\n- o Cloud + SaaS adoption → easier data access and experimentation.\n\n- o Data exhaust from apps, sensors, and logs → new signals to exploit.\n\n- o Labor scarcity &amp; cost pressure → automation and decision support.\n\n- o Competitive dynamics → personalization, faster cycle times, better UX.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · What's pushing demand\n\ntime to approve changes, % models with complete documentation, incidents MTTR, subgroup gap thresholds. - Risks: 'paper compliance' without monitoring, untracked prompt/model drift, unclear ownership. - Practical actions: set model inventory &amp; registry, standardize review checklists, automate lineage/metrics capture, schedule periodic fairness &amp; drift reviews. Organizations fund DS because digital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction.\n\n- o Cloud + SaaS adoption → easier data access and experimentation.\n\n- o Data exhaust from apps, sensors, and logs → new signals to exploit.\n\n- o Labor scarcity &amp; cost pressure → automation and decision support.\n\n- o Competitive dynamics → personalization, faster cycle times, better UX.", "token_count": 168, "embedding_token_count": 189, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· What's pushing demand"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · What's pushing demand", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· What's pushing demand", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821045Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:546-549:0073:b91fbe88", "doc_id": "DOC01", "version": "20251014", "chunk_index": 73, "block_start_index": 546, "block_end_index": 549, "text": "digital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction. - o Cloud + SaaS adoption → easier data access and experimentation. - o Data exhaust from apps, sensors, and logs → new signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX.\n\n- o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency.\n\n- o SLA/latency improvements, 'do more with less,' regulatory assurance.\n\n- Buying signals\n\n- o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Typical executive goals\n\ndigital processes throw off data, AI can turn that data into decisions, and automation improves margins at scale. Boards see DS as a lever for growth, risk control, and cost reduction. - o Cloud + SaaS adoption → easier data access and experimentation. - o Data exhaust from apps, sensors, and logs → new signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX.\n\n- o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency.\n\n- o SLA/latency improvements, 'do more with less,' regulatory assurance.\n\n- Buying signals\n\n- o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs.", "token_count": 155, "embedding_token_count": 174, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Typical executive goals"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Typical executive goals", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Typical executive goals", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821218Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:551-551:0074:35e4d7a8", "doc_id": "DOC01", "version": "20251014", "chunk_index": 74, "block_start_index": 551, "block_end_index": 551, "text": "easier data access and experimentation. - o Data exhaust from apps, sensors, and logs → new signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs.\n\n- o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · KPIs sponsors care about\n\neasier data access and experimentation. - o Data exhaust from apps, sensors, and logs → new signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs.\n\n- o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate.", "token_count": 135, "embedding_token_count": 155, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· KPIs sponsors care about"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · KPIs sponsors care about", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· KPIs sponsors care about", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821369Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:553-553:0075:9a918f67", "doc_id": "DOC01", "version": "20251014", "chunk_index": 75, "block_start_index": 553, "block_end_index": 553, "text": "new signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate.\n\nSpend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > High-investment sectors (finance, healthcare, e-commerce, logistics, SaaS)\n\nnew signals to exploit. - o Labor scarcity &amp; cost pressure → automation and decision support. - o Competitive dynamics → personalization, faster cycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate.\n\nSpend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases.", "token_count": 146, "embedding_token_count": 178, "section_path": ["Definition &amp; scope (what DS is / isn't)", "High-investment sectors (finance, healthcare, e-commerce, logistics, SaaS)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > High-investment sectors (finance, healthcare, e-commerce, logistics, SaaS)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|High-investment sectors (finance, healthcare, e-commerce, logistics, SaaS)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821534Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:555-556:0076:fe9c94d5", "doc_id": "DOC01", "version": "20251014", "chunk_index": 76, "block_start_index": 555, "block_end_index": 556, "text": "cycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases.\n\n- o Use cases: credit scoring, fraud/AML, next-best-action.\n\n- o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Finance\n\ncycle times, better UX. - o Revenue lift, churn reduction, fraud/risk control, working-capital efficiency. - o SLA/latency improvements, 'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases.\n\n- o Use cases: credit scoring, fraud/AML, next-best-action.\n\n- o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency.", "token_count": 156, "embedding_token_count": 173, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Finance"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Finance", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Finance", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821690Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:558-559:0077:8a251f02", "doc_id": "DOC01", "version": "20251014", "chunk_index": 77, "block_start_index": 558, "block_end_index": 559, "text": "'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency.\n\n- o Use cases: readmission risk, imaging triage, capacity planning.\n\n- o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Healthcare\n\n'do more with less,' regulatory assurance. - Buying signals - o Data lake/warehouse already in place, analytics backlog, manual repetitive decisions, rising service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency.\n\n- o Use cases: readmission risk, imaging triage, capacity planning.\n\n- o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation.", "token_count": 158, "embedding_token_count": 175, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Healthcare"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Healthcare", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Healthcare", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.821847Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:561-562:0078:3defd2a1", "doc_id": "DOC01", "version": "20251014", "chunk_index": 78, "block_start_index": 561, "block_end_index": 562, "text": "service costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation.\n\n- o Use cases: recommendations, pricing, demand forecasting.\n\n- o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · E-commerce/Retail\n\nservice costs. - o $/savings or lift per model, payback period, adoption/coverage, p95 latency, compliance pass rate. Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation.\n\n- o Use cases: recommendations, pricing, demand forecasting.\n\n- o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn.", "token_count": 157, "embedding_token_count": 178, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· E-commerce/Retail"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · E-commerce/Retail", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· E-commerce/Retail", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.822013Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:564-565:0079:1e8300f8", "doc_id": "DOC01", "version": "20251014", "chunk_index": 79, "block_start_index": 564, "block_end_index": 565, "text": "Spend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn.\n\n- o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection.\n\n- o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · Logistics/Manufacturing\n\nSpend concentrates where data is rich, decisions repeat, and regulation or competition is intense. Each sector has a typical 'first 3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn.\n\n- o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection.\n\n- o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost.", "token_count": 165, "embedding_token_count": 184, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· Logistics/Manufacturing"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · Logistics/Manufacturing", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· Logistics/Manufacturing", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.822170Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:567-568:0080:51c383ae", "doc_id": "DOC01", "version": "20251014", "chunk_index": 80, "block_start_index": 567, "block_end_index": 568, "text": "3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn. - o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection. - o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost.\n\n- o Use cases: abuse detection, search/ranking, product analytics.\n\n- o Buyer: Product/Platform. Constraints: scale, cost per inference.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > · SaaS/Tech\n\n3' use cases. - o Use cases: credit scoring, fraud/AML, next-best-action. - o Buyer: Risk/Collections/Head of Analytics. Constraints: explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn. - o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection. - o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost.\n\n- o Use cases: abuse detection, search/ranking, product analytics.\n\n- o Buyer: Product/Platform. Constraints: scale, cost per inference.", "token_count": 172, "embedding_token_count": 191, "section_path": ["Definition &amp; scope (what DS is / isn't)", "· SaaS/Tech"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > · SaaS/Tech", "section_group_id": "Definition &amp; scope (what DS is / isn't)|· SaaS/Tech", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.822328Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:570-581:0081:e864a643", "doc_id": "DOC01", "version": "20251014", "chunk_index": 81, "block_start_index": 570, "block_end_index": 581, "text": "explainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn. - o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection. - o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost. - o Use cases: abuse detection, search/ranking, product analytics. - o Buyer: Product/Platform. Constraints: scale, cost per inference.\n\nHiring patterns show where opportunity is moving: from one-off analyses → reliable ML systems → GenAI-enabled apps.\n\n- Growing roles\n\n- o ML/GenAI Engineer: ship LLM/RAG features, latency/cost tuning, safety/guardrails.\n\n- o Analytics Engineer: dbt/semantic layers so metrics are consistent and BI scales.\n\n- o MLOps Engineer: monitoring, registries, CI/CD, retraining automation.\n\n- Stable roles\n\n- o Data Scientist / Product DS: experiment design, model development, KPI strategy.\n\n- o Data Engineer: ingestion, quality, lineage, batch/streaming platforms.\n\n- Signals of demand\n\n- o Job posts asking for vector DBs, prompt/retrieval pipelines, dbt + BI ownership, model governance experience.\n\n- Implication\n\n- o Opportunities favor people who bridge models ↔ production and data modeling ↔ business metrics .", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Role trends (ML/GenAI engineers, analytics engineers, MLOps)\n\nexplainability, audit, latency. - o Use cases: readmission risk, imaging triage, capacity planning. - o Buyer: Hospital ops, payers, digital health. Constraints: PHI, clinical validation. - o Use cases: recommendations, pricing, demand forecasting. - o Buyer: Growth/Category/Supply chain. Constraints: seasonality, catalog churn. - o Use cases: ETA/pick-path optimization, predictive maintenance, quality inspection. - o Buyer: Ops/Plant managers. Constraints: edge latency, downtime cost. - o Use cases: abuse detection, search/ranking, product analytics. - o Buyer: Product/Platform. Constraints: scale, cost per inference.\n\nHiring patterns show where opportunity is moving: from one-off analyses → reliable ML systems → GenAI-enabled apps.\n\n- Growing roles\n\n- o ML/GenAI Engineer: ship LLM/RAG features, latency/cost tuning, safety/guardrails.\n\n- o Analytics Engineer: dbt/semantic layers so metrics are consistent and BI scales.\n\n- o MLOps Engineer: monitoring, registries, CI/CD, retraining automation.\n\n- Stable roles\n\n- o Data Scientist / Product DS: experiment design, model development, KPI strategy.\n\n- o Data Engineer: ingestion, quality, lineage, batch/streaming platforms.\n\n- Signals of demand\n\n- o Job posts asking for vector DBs, prompt/retrieval pipelines, dbt + BI ownership, model governance experience.\n\n- Implication\n\n- o Opportunities favor people who bridge models ↔ production and data modeling ↔ business metrics .", "token_count": 298, "embedding_token_count": 326, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Role trends (ML/GenAI engineers, analytics engineers, MLOps)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Role trends (ML/GenAI engineers, analytics engineers, MLOps)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Role trends (ML/GenAI engineers, analytics engineers, MLOps)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.822638Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:583-598:0082:ccc39f74", "doc_id": "DOC01", "version": "20251014", "chunk_index": 82, "block_start_index": 583, "block_end_index": 598, "text": "metrics are consistent and BI scales. - o MLOps Engineer: monitoring, registries, CI/CD, retraining automation. - Stable roles - o Data Scientist / Product DS: experiment design, model development, KPI strategy. - o Data Engineer: ingestion, quality, lineage, batch/streaming platforms. - Signals of demand - o Job posts asking for vector DBs, prompt/retrieval pipelines, dbt + BI ownership, model governance experience. - Implication - o Opportunities favor people who bridge models ↔ production and data modeling ↔ business metrics .\n\nThese are repeatable problem types with clear ROI and maturing patterns. They're good bets for products, services, or internal platforms.\n\n- Recommendation/Ranking\n\n- o Value: lift conversion and AOV with personalization.\n\n- o Moat: first-party interaction data, inference latency. Metrics: CTR, revenue/session, diversity.\n\n- Forecasting &amp; Optimization\n\n- o Value: reduce stockouts/overstocks, improve staffing and cash cycles.\n\n- o Moat: high-quality exogenous signals. Metrics: WAPE/MAPE, service level, markdowns.\n\n- Fraud/Risk/Trust &amp; Safety\n\n- o Value: prevent loss with minimal friction.\n\n- o Moat: labels + real-time graph features. Metrics: precision@K, false-positive rate, manual review load.\n\n- Copilots/Assistants (GenAI)\n\n- o Value: compress analyst/agent time; self-serve analytics/search.\n\n- o Moat: retrieval quality + guardrails + domain prompts. Metrics: task success, time-to-answer, hallucination rate.\n\n- Vector Search &amp; RAG platforms\n\n- o Value: find semantically similar content across text/images/code.\n\n- o Moat: embeddings quality + chunking + metadata pipeline. Metrics: recall@K, MRR/NDCG, cost/query.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Product spaces (recommendation, forecasting, fraud/risk, copilots, vector search)\n\nmetrics are consistent and BI scales. - o MLOps Engineer: monitoring, registries, CI/CD, retraining automation. - Stable roles - o Data Scientist / Product DS: experiment design, model development, KPI strategy. - o Data Engineer: ingestion, quality, lineage, batch/streaming platforms. - Signals of demand - o Job posts asking for vector DBs, prompt/retrieval pipelines, dbt + BI ownership, model governance experience. - Implication - o Opportunities favor people who bridge models ↔ production and data modeling ↔ business metrics .\n\nThese are repeatable problem types with clear ROI and maturing patterns. They're good bets for products, services, or internal platforms.\n\n- Recommendation/Ranking\n\n- o Value: lift conversion and AOV with personalization.\n\n- o Moat: first-party interaction data, inference latency. Metrics: CTR, revenue/session, diversity.\n\n- Forecasting &amp; Optimization\n\n- o Value: reduce stockouts/overstocks, improve staffing and cash cycles.\n\n- o Moat: high-quality exogenous signals. Metrics: WAPE/MAPE, service level, markdowns.\n\n- Fraud/Risk/Trust &amp; Safety\n\n- o Value: prevent loss with minimal friction.\n\n- o Moat: labels + real-time graph features. Metrics: precision@K, false-positive rate, manual review load.\n\n- Copilots/Assistants (GenAI)\n\n- o Value: compress analyst/agent time; self-serve analytics/search.\n\n- o Moat: retrieval quality + guardrails + domain prompts. Metrics: task success, time-to-answer, hallucination rate.\n\n- Vector Search &amp; RAG platforms\n\n- o Value: find semantically similar content across text/images/code.\n\n- o Moat: embeddings quality + chunking + metadata pipeline. Metrics: recall@K, MRR/NDCG, cost/query.", "token_count": 357, "embedding_token_count": 388, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Product spaces (recommendation, forecasting, fraud/risk, copilots, vector search)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Product spaces (recommendation, forecasting, fraud/risk, copilots, vector search)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Product spaces (recommendation, forecasting, fraud/risk, copilots, vector search)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.822969Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:600-610:0083:cb65fc74", "doc_id": "DOC01", "version": "20251014", "chunk_index": 83, "block_start_index": 600, "block_end_index": 610, "text": "Safety - o Value: prevent loss with minimal friction. - o Moat: labels + real-time graph features. Metrics: precision@K, false-positive rate, manual review load. - Copilots/Assistants (GenAI) - o Value: compress analyst/agent time; self-serve analytics/search. - o Moat: retrieval quality + guardrails + domain prompts. Metrics: task success, time-to-answer, hallucination rate. - Vector Search &amp; RAG platforms - o Value: find semantically similar content across text/images/code. - o Moat: embeddings quality + chunking + metadata pipeline. Metrics: recall@K, MRR/NDCG, cost/query.\n\nChoosing to build or buy hinges on total cost, speed, and how 'differentiating' the problem is for your org.\n\n- Cost components (TCO)\n\n- o Data work (ingestion/cleaning/labels), infra (compute/storage/egress), platform tooling, talent, security/compliance, ongoing monitoring.\n\n- When to buy\n\n- o Commodity capability (OCR, transcription, generic chat), need speed to market, limited DS/MLE bandwidth, strong vendor benchmarks &amp; SLAs.\n\n- When to build\n\n- o Proprietary data is your moat, tight integration/latency constraints, scale makes vendor usage expensive, IP/regulatory needs.\n\n- Lock-in checks\n\n- o Export paths for data/models, BYO-key encryption, transparent pricing, offline/batch fallbacks.\n\n- Simple rule\n\n- o Pilot with buy; if usage &gt; threshold and data advantage emerges, re-platform to build.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Build vs buy economics (TCO, lock-in, talent availability)\n\nSafety - o Value: prevent loss with minimal friction. - o Moat: labels + real-time graph features. Metrics: precision@K, false-positive rate, manual review load. - Copilots/Assistants (GenAI) - o Value: compress analyst/agent time; self-serve analytics/search. - o Moat: retrieval quality + guardrails + domain prompts. Metrics: task success, time-to-answer, hallucination rate. - Vector Search &amp; RAG platforms - o Value: find semantically similar content across text/images/code. - o Moat: embeddings quality + chunking + metadata pipeline. Metrics: recall@K, MRR/NDCG, cost/query.\n\nChoosing to build or buy hinges on total cost, speed, and how 'differentiating' the problem is for your org.\n\n- Cost components (TCO)\n\n- o Data work (ingestion/cleaning/labels), infra (compute/storage/egress), platform tooling, talent, security/compliance, ongoing monitoring.\n\n- When to buy\n\n- o Commodity capability (OCR, transcription, generic chat), need speed to market, limited DS/MLE bandwidth, strong vendor benchmarks &amp; SLAs.\n\n- When to build\n\n- o Proprietary data is your moat, tight integration/latency constraints, scale makes vendor usage expensive, IP/regulatory needs.\n\n- Lock-in checks\n\n- o Export paths for data/models, BYO-key encryption, transparent pricing, offline/batch fallbacks.\n\n- Simple rule\n\n- o Pilot with buy; if usage &gt; threshold and data advantage emerges, re-platform to build.", "token_count": 318, "embedding_token_count": 347, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Build vs buy economics (TCO, lock-in, talent availability)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Build vs buy economics (TCO, lock-in, talent availability)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Build vs buy economics (TCO, lock-in, talent availability)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.823234Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:612-621:0084:5ff8a776", "doc_id": "DOC01", "version": "20251014", "chunk_index": 84, "block_start_index": 612, "block_end_index": 621, "text": "monitoring. - When to buy - o Commodity capability (OCR, transcription, generic chat), need speed to market, limited DS/MLE bandwidth, strong vendor benchmarks &amp; SLAs. - When to build - o Proprietary data is your moat, tight integration/latency constraints, scale makes vendor usage expensive, IP/regulatory needs. - Lock-in checks - o Export paths for data/models, BYO-key encryption, transparent pricing, offline/batch fallbacks. - Simple rule - o Pilot with buy; if usage &gt; threshold and data advantage emerges, re-platform to build.\n\nTalent and compliance shape where and how you execute. Remote expands the pool; regulations constrain data flow.\n\n- Talent hubs\n\n- o Mature DS/MLE communities (US, EU, India, SEA) + emerging hubs near universities.\n\n- o Nearshore/offshore models for 24/7 ops and cost leverage.\n\n- Compliance &amp; data residency\n\n- o Sector rules (banking/health), cross-border transfer limits, localization laws.\n\n- Operating model\n\n- o Hybrid teams: product &amp; data owners near users; platform &amp; model ops distributed.\n\n- Practical tips\n\n- o Keep sensitive training data region-local; move models not data; document data contracts.", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Geographic considerations (remote work, regional hubs, compliance regimes)\n\nmonitoring. - When to buy - o Commodity capability (OCR, transcription, generic chat), need speed to market, limited DS/MLE bandwidth, strong vendor benchmarks &amp; SLAs. - When to build - o Proprietary data is your moat, tight integration/latency constraints, scale makes vendor usage expensive, IP/regulatory needs. - Lock-in checks - o Export paths for data/models, BYO-key encryption, transparent pricing, offline/batch fallbacks. - Simple rule - o Pilot with buy; if usage &gt; threshold and data advantage emerges, re-platform to build.\n\nTalent and compliance shape where and how you execute. Remote expands the pool; regulations constrain data flow.\n\n- Talent hubs\n\n- o Mature DS/MLE communities (US, EU, India, SEA) + emerging hubs near universities.\n\n- o Nearshore/offshore models for 24/7 ops and cost leverage.\n\n- Compliance &amp; data residency\n\n- o Sector rules (banking/health), cross-border transfer limits, localization laws.\n\n- Operating model\n\n- o Hybrid teams: product &amp; data owners near users; platform &amp; model ops distributed.\n\n- Practical tips\n\n- o Keep sensitive training data region-local; move models not data; document data contracts.", "token_count": 253, "embedding_token_count": 280, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Geographic considerations (remote work, regional hubs, compliance regimes)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Geographic considerations (remote work, regional hubs, compliance regimes)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Geographic considerations (remote work, regional hubs, compliance regimes)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.823451Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
{"chunk_id": "DOC01:20251014:623-630:0085:f64c4dff", "doc_id": "DOC01", "version": "20251014", "chunk_index": 85, "block_start_index": 623, "block_end_index": 630, "text": "constrain data flow. - Talent hubs - o Mature DS/MLE communities (US, EU, India, SEA) + emerging hubs near universities. - o Nearshore/offshore models for 24/7 ops and cost leverage. - Compliance &amp; data residency - o Sector rules (banking/health), cross-border transfer limits, localization laws. - Operating model - o Hybrid teams: product &amp; data owners near users; platform &amp; model ops distributed. - Practical tips - o Keep sensitive training data region-local; move models not data; document data contracts.\n\nMost failed DS programs fall on organizational-not algorithmic-issues. Plan for them early.\n\n- Common blockers\n\n- o Unclear problem owners; missing labels or data contracts; privacy/security reviews slow access.\n\n- o Shadow metrics (different teams compute KPIs differently); low adoption of outputs.\n\n- Risk patterns\n\n- o Model built without deployment path; drift/monitoring missing; ROI not measured.\n\n- Mitigations\n\n- o Problem charters with owners/KPIs, metric definitions in a semantic layer, data access playbooks, model registry + monitoring SLOs, change-management plan (training, incentives).", "embedding_text": "Definition &amp; scope (what DS is / isn't) > Barriers &amp; risks (talent gap, data access, change management)\n\nconstrain data flow. - Talent hubs - o Mature DS/MLE communities (US, EU, India, SEA) + emerging hubs near universities. - o Nearshore/offshore models for 24/7 ops and cost leverage. - Compliance &amp; data residency - o Sector rules (banking/health), cross-border transfer limits, localization laws. - Operating model - o Hybrid teams: product &amp; data owners near users; platform &amp; model ops distributed. - Practical tips - o Keep sensitive training data region-local; move models not data; document data contracts.\n\nMost failed DS programs fall on organizational-not algorithmic-issues. Plan for them early.\n\n- Common blockers\n\n- o Unclear problem owners; missing labels or data contracts; privacy/security reviews slow access.\n\n- o Shadow metrics (different teams compute KPIs differently); low adoption of outputs.\n\n- Risk patterns\n\n- o Model built without deployment path; drift/monitoring missing; ROI not measured.\n\n- Mitigations\n\n- o Problem charters with owners/KPIs, metric definitions in a semantic layer, data access playbooks, model registry + monitoring SLOs, change-management plan (training, incentives).", "token_count": 232, "embedding_token_count": 262, "section_path": ["Definition &amp; scope (what DS is / isn't)", "Barriers &amp; risks (talent gap, data access, change management)"], "breadcrumb": "Definition &amp; scope (what DS is / isn't) > Barriers &amp; risks (talent gap, data access, change management)", "section_group_id": "Definition &amp; scope (what DS is / isn't)|Barriers &amp; risks (talent gap, data access, change management)", "chunk_type": "text", "contains_table": false, "contains_code": false, "created_at": "2025-10-16T12:45:48.823656Z", "config": {"target_tokens": 800, "max_tokens": 1100, "min_tokens": 150, "overlap_tokens": 80, "include_headings_in_text": false, "embed_with_breadcrumbs": true, "breadcrumb_depth": 2, "breadcrumb_joiner": " > ", "break_on_heading_level": 2}, "overlap_with_prev": 80}
